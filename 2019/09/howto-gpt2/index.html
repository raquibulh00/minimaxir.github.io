<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.3.1"><meta name=author content="Max Woolf"><meta name=description content="Thanks to gpt-2-simple and this Colaboratory Notebook, you can easily finetune GPT-2 on your own dataset!"><link rel=alternate hreflang=en-us href=https://minimaxir.com/2019/09/howto-gpt2/><meta name=theme-color content=#2962ff><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400italic,700|Source+Code+Pro:400,400italic,700&display=swap"><link rel=stylesheet href=/css/academic.min.4cdedb6ca5fc8a13caf3e26423bf7037.css><link rel=stylesheet href=/css/academic.59da4f61e2de6d8a5935b902fe667ab3.css><link rel=manifest href=/site.webmanifest><link rel=icon type=image/png href=/img/icon.png><link rel=apple-touch-icon type=image/png href=/img/icon-192.png><link rel=canonical href=https://minimaxir.com/2019/09/howto-gpt2/><meta property=twitter:card content=summary_large_image><meta property=twitter:site content=@minimaxir><meta property=twitter:creator content=@minimaxir><meta property=article:author content=https://www.facebook.com/max.woolf><meta property=og:site_name content="Max Woolf's Blog"><meta property=og:url content=https://minimaxir.com/2019/09/howto-gpt2/><meta property=og:type content=article><meta property=og:title content="How To Make Custom AI-Generated Text With GPT-2"><meta property=og:description content="Thanks to gpt-2-simple and this Colaboratory Notebook, you can easily finetune GPT-2 on your own dataset!"><meta property=og:image content=https://minimaxir.com/2019/09/howto-gpt2/featured.png><meta property=twitter:image content=https://minimaxir.com/2019/09/howto-gpt2/featured.png><meta property=og:locale content=en-us><meta property=article:published_time content=2019-09-04T08:00:00-07:00><meta property=article:modified_time content=2019-09-04T08:00:00-07:00><title>How To Make Custom AI-Generated Text With GPT-2 | Max Woolf&#39;s Blog</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id=navbar-main><div class=container><a class=navbar-brand href=/>Max Woolf&#39;s Blog</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/about/><span>About</span></a></li><li class=nav-item><a class=nav-link href=/post/><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/portfolio/><span>Portfolio</span></a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=https://www.patreon.com/minimaxir target=_blank rel=noopener><span><i class="fab fa-patreon mr-1"></i>Patreon</span></a></li><li class=nav-item><a class=nav-link href=https://github.com/minimaxir target=_blank rel=noopener><span><i class="fab fa-github-alt mr-1"></i>GitHub</span></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><article class=article itemscope itemtype=http://schema.org/Article><div class="article-container pt-3"><h1 itemprop=name>How To Make Custom AI-Generated Text With GPT-2</h1><meta content="2019-09-04 08:00:00 -0700 PDT" itemprop=datePublished><meta content="2019-09-04 08:00:00 -0700 PDT" itemprop=dateModified><div class=article-metadata><span class=article-date><time>September 4, 2019</time></span>
<span class=middot-divider></span><span class=article-reading-time>10 min read</span>
<span class=middot-divider></span><span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/categories/ai/>AI</a>, <a href=/categories/text-generation/>Text Generation</a></span><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://minimaxir.com/2019/09/howto-gpt2/&amp;text=How%20To%20Make%20Custom%20AI-Generated%20Text%20With%20GPT-2" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://minimaxir.com/2019/09/howto-gpt2/&amp;t=How%20To%20Make%20Custom%20AI-Generated%20Text%20With%20GPT-2" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook-f"></i></a></li><li><a href="mailto:?subject=How%20To%20Make%20Custom%20AI-Generated%20Text%20With%20GPT-2&amp;body=https://minimaxir.com/2019/09/howto-gpt2/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://minimaxir.com/2019/09/howto-gpt2/&amp;title=How%20To%20Make%20Custom%20AI-Generated%20Text%20With%20GPT-2" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://reddit.com/submit?url=https://minimaxir.com/2019/09/howto-gpt2/&amp;title=How%20To%20Make%20Custom%20AI-Generated%20Text%20With%20GPT-2" target=_blank rel=noopener class=share-btn-reddit><i class="fab fa-reddit-alien"></i></a></li></ul></div></div></div><div class=article-container><div class=article-style itemprop=articleBody><p>In February 2019, <a href=https://openai.com target=_blank>OpenAI</a> released <a href=https://openai.com/blog/better-language-models/ target=_blank>a paper</a> describing GPT-2, a AI-based text-generation model based on the <a href=https://arxiv.org/abs/1706.03762 target=_blank>Transformer architecture</a> and trained on massive amounts of text all around the internet. From a text-generation perspective, the included demos were very impressive: the text is coherent over a long horizon, and grammatical syntax and punctuation are near-perfect.</p><p><img src=/img/howto-gpt2/openai-demo.png alt></p><p>At the same time, the Python code which allowed anyone to download the model (albeit smaller versions out of concern the full model can be abused to mass-generate fake news) and the <a href=https://www.tensorflow.org target=_blank>TensorFlow</a> code to load the downloaded model and generate predictions was <a href=https://github.com/openai/gpt-2 target=_blank>open-sourced on GitHub</a>.</p><p>Neil Shepperd created <a href=https://github.com/nshepperd/gpt-2 target=_blank>a fork</a> of OpenAI&rsquo;s repo which contains additional code to allow <em>finetuning</em> the existing OpenAI model on custom datasets. A <a href=https://github.com/ak9250/gpt-2-colab target=_blank>notebook</a> was created soon after, which can be copied into <a href=https://colab.research.google.com target=_blank>Google Colaboratory</a> and clones Shepperd&rsquo;s repo to finetune GPT-2 backed by a free GPU. From there, the proliferation of GPT-2 generated text took off: researchers such as Gwern Branwen made <a href=https://www.gwern.net/GPT-2 target=_blank>GPT-2 Poetry</a> and Janelle Shane made <a href=https://aiweirdness.com/post/183471928977/dd-character-bios-now-making-slightly-more target=_blank>GPT-2 Dungeons and Dragons character bios</a>.</p><p>I waited to see if anyone would make a tool to help streamline this finetuning and text generation workflow, a la <a href=https://github.com/minimaxir/textgenrnn target=_blank>textgenrnn</a> which I had made for recurrent neural network-based text generation. Months later, no one did. So I did it myself. Enter <a href=https://github.com/minimaxir/gpt-2-simple target=_blank>gpt-2-simple</a>, a Python package which wraps Shepperd&rsquo;s finetuning code in a functional interface and adds <em>many</em> utilities for model management and generation control.</p><p>Thanks to gpt-2-simple and <a href=https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce target=_blank>this Colaboratory Notebook</a>, you can easily finetune GPT-2 on your own dataset with a simple function, and generate text to your own specifications!</p><h2 id=how-gpt-2-works>How GPT-2 Works</h2><p>OpenAI has released three flavors of GPT-2 models to date: the &ldquo;small&rdquo; 124M parameter model (500MB on disk), the &ldquo;medium&rdquo; 355M model (1.5GB on disk), and recently the 774M model (3GB on disk). These models are <em>much</em> larger than what you see in typical AI tutorials and are harder to wield: the &ldquo;small&rdquo; model hits GPU memory limits while finetuning with consumer GPUs, the &ldquo;medium&rdquo; model requires additional training techniques before it could be finetuned on server GPUs without going out-of-memory, and the &ldquo;large&rdquo; model <em>cannot be finetuned at all</em> with current server GPUs before going OOM, even with those techniques.</p><p>The actual Transformer architecture GPT-2 uses is very complicated to explain (here&rsquo;s a <a href=http://www.peterbloem.nl/blog/transformers target=_blank>great lecture</a>). For the purposes of finetuning, since we can&rsquo;t modify the architecture, it&rsquo;s easier to think of GPT-2 as a <a href=https://en.wikipedia.org/wiki/Black_box target=_blank>black box</a>, taking in inputs and providing outputs. Like <a href=http://karpathy.github.io/2015/05/21/rnn-effectiveness/ target=_blank>previous forms of text generators</a>, the inputs are a sequence of tokens, and the outputs are the probability of the next token in the sequence, with these probabilities serving as weights for the AI to pick the next token in the sequence. In this case, both the input and output tokens are <a href=https://en.wikipedia.org/wiki/Byte_pair_encoding target=_blank>byte pair encodings</a>, which instead of using character tokens (slower to train but includes case/formatting) or word tokens (faster to train but does not include case/formatting) like most RNN approaches, the inputs are &ldquo;compressed&rdquo; to the shortest combination of bytes including case/formatting, which serves as a compromise between both approaches but unfortunately adds randomness to the final generation length. The byte pair encodings are later decoded into readable text for human generation.</p><p>The pretrained GPT-2 models were trained on websites linked from <a href=https://www.reddit.com target=_blank>Reddit</a>. As a result, the model has a very strong grasp of the English language, allowing this knowledge to transfer to other datasets and perform well with only a minor amount of additional finetuning. Due to the English bias in encoder construction, languages with non-Latin characters like Russian and <a href=https://en.wikipedia.org/wiki/CJK_characters target=_blank>CJK</a> will perform poorly in finetuning.</p><p>When finetuning GPT-2, I recommend using the 124M model (the default) as it&rsquo;s the best balance of speed, size, and creativity. If you have large amounts of training data (&gt;10 MB), then the 355M model may work better.</p><h2 id=gpt-2-simple-and-colaboratory>gpt-2-simple And Colaboratory</h2><p>In order to better utilize gpt-2-simple and showcase its features, I created my <a href=https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce target=_blank>own Colaboratory Notebook</a>, which can be copied into your own Google account. A Colaboratory Notebook is effectively a <a href=https://jupyter.org target=_blank>Jupyter Notebook</a> running on a free (w/ a Google Account) virtual machine with an Nvidia server GPU attached (<a href=https://twitter.com/BasedBlue/status/1164732922953379841 target=_blank>randomly</a> a K80 or a T4; T4 is ideal) that normally can be cost-prohibitive.</p><p><img src=/img/howto-gpt2/gpu.png alt></p><p>Once open, the first cell (run by pressing Shift+Enter in the cell or mousing-over the cell and pressing the &ldquo;Play&rdquo; button) of the notebook installs gpt-2-simple and its dependencies, and loads the package.</p><p><img src=/img/howto-gpt2/imports.png alt></p><p>Later in the notebook is <code>gpt2.download_gpt2()</code> which downloads the requested model type to the Colaboratory VM (the models are hosted on Google&rsquo;s servers, so it&rsquo;s a <em>very</em> fast download).</p><p>Expanding the Colaboratory sidebar reveals a UI that you can use to upload files. For example, the <a href=https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt target=_blank>tinyshakespeare dataset</a> (1MB) provided with the original <a href=https://github.com/karpathy/char-rnn target=_blank>char-rnn implementation</a>. Upload a text file via the UI (you can drag and drop), run the <code>file_name = '&lt;xxx&gt;'</code> cell with your filename changed in the cell.</p><p>Now we can start finetuning! This finetuning cell loads the specified dataset and trains for the specified number of steps (the default of 1,000 steps is enough to allow distinct text to emerge and takes about 45 minutes, but you can increase the number of steps if necessary).</p><p><img src=/img/howto-gpt2/finetuning.png alt></p><p>While the model is finetuning, the average training loss is output every-so-often to the cell. The <em>absolute value</em> of the loss is not important (the output text quality is subjective), but if the average loss stops decreasing, that&rsquo;s a sign the model has converged and additional training may not help improve the model.</p><p>By default, your model is saved in the <code>checkpoint/run1</code> folder, and you&rsquo;ll need to use that folder to load the model as well (you can specify the <code>run_name</code> when using other functions categorize finetuned models). If you want to export the model from Colaboratory, it&rsquo;s recommended you do so via <a href=https://www.google.com/drive/ target=_blank>Google Drive</a> (as Colaboratory does not like exporting large files). Run the <code>gpt2.mount_gdrive()</code> cell to mount your Google Drive in the Colaboratory VM, then run the <code>gpt2.copy_checkpoint_to_gdrive()</code> cell. You can then download the compressed model folder from Google Drive and run the model wherever you want. Likewise, you can use the <code>gpt2.copy_checkpoint_from_gdrive()</code> cell to retrieve a stored model and generate in the notebook.</p><p>Speaking of generation, once you have a finetuned model, you can now generate custom text from it! By default, the <code>gpt2.generate()</code> function will generate as much text as possible (1,024 tokens) with a little bit of randomness. An important caveat: <em>you will not get good generated text 100% of the time</em>, even with a properly trained model (the OpenAI demo above took <em>25 tries</em> to get good text!).</p><p><img src=/img/howto-gpt2/gen_long.png alt></p><p>You can also increase the <code>temperature</code> to increase &ldquo;creativity&rdquo; by allowing the network to more likely make suboptimal predictions, provide a <code>prefix</code> to specify how exactly you want your text to begin. There are many other useful configuration parameters, such as <code>top_p</code> for <a href=https://github.com/minimaxir/gpt-2-simple/issues/51 target=_blank>nucleus sampling</a>.</p><p><img src=/img/howto-gpt2/gen_long_params.png alt></p><p>As a bonus, you can bulk-generate text with gpt-2-simple by setting <code>nsamples</code> (number of texts to generate total) and <code>batch_size</code> (number of texts to generate at a time); the Colaboratory GPUs can support a <code>batch_size</code> of up to 20, and you can generate these to a text file with <code>gpt2.generate_to_file(file_name)</code> with the same parameters as <code>gpt2.generate()</code>. You can download the generated file locally via the sidebar, and use those to easily save and share the generated texts.</p><p><a href=https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce target=_blank>The notebook</a> has many more functions as well, with more parameters and detailed explanations! The <a href=https://github.com/minimaxir/gpt-2-simple target=_blank>gpt-2-simple README</a> lists additional features of gpt-2-simple if you want to use the model outside the notebook.</p><p>(NB: Currently, you&rsquo;ll need to reset the Notebook via Runtime → Restart Runtime to finetune a different model/dataset or load a different finetuned model.)</p><h2 id=gpt-2-for-short-texts>GPT-2 For Short Texts</h2><p>A weakness of GPT-2 and other out-of-the-box AI text generators is that they are built for longform content, and keep on generating text until you hit the specified length. Another reason I wanted to make gpt-2-simple was to add explicit processing tricks to the generated text to work around this issue for short texts. In this case, there are two additional parameters that can be passed to <code>gpt2.generate()</code>: <code>truncate</code> and <code>include_prefix</code>. For example, if each short text begins with a <code>&lt;|startoftext|&gt;</code> token and ends with a <code>&lt;|endoftext|&gt;</code>, then setting <code>prefix='&lt;|startoftext|&gt;'</code>, <code>truncate=&lt;|endoftext|&gt;'</code>, and <code>include_prefix=False</code>, and <code>length</code> is sufficient, then gpt-2-simple will automatically extract the shortform texts, even when generating in batches.</p><p>Let&rsquo;s finetune a GPT-2 model on Reddit submission titles. This query, when run on <a href=https://console.cloud.google.com/bigquery target=_blank>BigQuery</a> (for free), returns the top 16,000 titles by score between January and March 2019 for a given Reddit subreddit (in this case, <a href=https://www.reddit.com/r/AskReddit/ target=_blank>/r/AskReddit</a>) + minor text preprocessing, which can be downloaded locally as a 1.3 MB CSV (Save Results → CSV [local file]):</p><pre><code class=language-sql>#standardSQL
SELECT
  REGEXP_REPLACE(REGEXP_REPLACE(REGEXP_REPLACE(REGEXP_REPLACE(title, '&amp;amp;', '&amp;'), '&amp;lt;', '&lt;'), '&amp;gt;', '&gt;'), '�', '') AS title
FROM
  `fh-bigquery.reddit_posts.*`
WHERE
  _TABLE_SUFFIX BETWEEN '2019_01' AND '2019_03'
  AND LENGTH(title) &gt;= 8
  AND LOWER(subreddit) = 'askreddit'
ORDER BY
  score DESC
LIMIT
  16000
</code></pre><p>With gpt-2-simple, using a single-column CSV like the one generated above as the input dataset will automatically add <code>&lt;|startoftext|&gt;</code> and <code>&lt;|endoftext|&gt;</code> tokens appropriately. Finetune a new GPT-2 model as normal, and then generate with those additional parameters mentioned above:</p><p><img src=/img/howto-gpt2/gen_short.png alt></p><p>It&rsquo;s worth noting that despite a good amount of input data to the model, finetuned networks can easily <em>overfit</em> on short form text: some of these example titles are very close to existing /r/AskReddit titles. Overfitting can be rectified by training for less time, or adding more input data. Make sure to double check that your generated text is unique!</p><p>You can play with this Reddit-oriented variant in <a href=https://colab.research.google.com/drive/1RugXCYDcMvSACYNt9j0kB6zzqRKzAbBn target=_blank>this modified Colaboratory Notebook</a>.</p><h2 id=making-gpt-2-apps>Making GPT-2 Apps</h2><p>There have already been cool, non-nefarious uses of GPT-2, such as Adam King&rsquo;s <a href=https://talktotransformer.com target=_blank>TalkToTransformer</a> which provides a UI for the 774M model (and has gone viral many times) and <a href=https://tabnine.com target=_blank>TabNine</a>, which uses GPT-2 finetuned on GitHub code in order to create probabilistic code completion. On the <a href=https://pytorch.org target=_blank>PyTorch</a> side, Huggingface has released a <a href=https://github.com/huggingface/pytorch-transformers target=_blank>Transformers client</a> (w/ GPT-2 support) of their own, and also created apps such as <a href=https://transformer.huggingface.co target=_blank>Write With Transformer</a> to serve as a text autocompleter.</p><p>Many AI tutorials often show how to deploy a small model to a web service by using the <a href=https://palletsprojects.com/p/flask/ target=_blank>Flask</a> application framework. The problem with GPT-2 is that it&rsquo;s such a huge model that most conventional advice won&rsquo;t work well to get a performant app. And even if you do get it to run fast (e.g. by running the app on a GPU), it won&rsquo;t be <em>cheap</em>, especially if you want it to be resilient to a random surge of virality.</p><p>With gpt-2-simple, the solution I came up with is <a href=https://github.com/minimaxir/gpt-2-cloud-run target=_blank>gpt-2-cloud-run</a>; a small webapp intended to run GPT-2 via <a href=https://cloud.google.com/run/ target=_blank>Google Cloud Run</a> backed by gpt-2-simple. The advantage here is that Cloud Run only charges for compute used and can scale indefinitely if there&rsquo;s a traffic surge; for casual use, it&rsquo;s extremely cost effective compared to running a GPU 24/7. I&rsquo;ve used Cloud Run to make a GPT-2 text generator for <a href=https://minimaxir.com/apps/gpt2-reddit/ target=_blank>Reddit-wide submission titles</a> and a GPT-2 generator for <a href=https://minimaxir.com/apps/gpt2-mtg/ target=_blank>Magic: The Gathering cards</a>!</p><p><img src=/img/howto-gpt2/mtg.png alt></p><h2 id=attributing-ai-generated-text>Attributing AI-Generated Text</h2><p>One of the main reasons I developed textgenrnn and gpt-2-simple is to make AI text generation more <em>accessible</em> as you do not need a strong AI or technical background to create fun stories. However, in the case of GPT-2, I&rsquo;ve noticed an elevated amount of &ldquo;I trained an AI to generate text&rdquo; articles/Reddit posts/YouTube videos saying they used GPT-2 to train an AI, but not <em>how</em> they trained the AI: especially suspicious since finetuning is not an out-of-the-box feature that OpenAI provides. The fact that Keaton Patti&rsquo;s <a href=https://twitter.com/KeatonPatti/status/1161284670601990146 target=_blank>&ldquo;I forced a bot&rdquo; movie scripts</a> (that aren&rsquo;t written by a bot) frequently go megaviral due to that particular framing doesn&rsquo;t help.</p><p>Although it&rsquo;s not legally required, I ask that anyone who shares generated text via gpt-2-simple add a link to the repo and/or Colaboratory notebook not just for attribution, but to <em>spread knowledge</em> about the accessibility of AI text generation. It&rsquo;s a technology that should be transparent, not obfuscated for personal gain.</p><h2 id=the-future-of-gpt-2>The Future of GPT-2</h2><p>Hopefully, this article gave you ideas on how to finetune and generate texts creatively. There&rsquo;s still a <em>lot</em> of untapped potential, and there are still many cool applications that have been untouched, and many cool datasets that haven&rsquo;t been used for AI text generation. GPT-2 will likely be used more for mass-producing <a href=https://twitter.com/Fred_Delicious/status/1166783214750445573 target=_blank>crazy erotica</a> than fake news.</p><p>However, GPT-2 and the Transformer architecture aren&rsquo;t the end-game of AI text generation. Not by a long shot.</p><div class="alert alert-note"><div>If you liked this blog post, I have set up a <a href=https://www.patreon.com/minimaxir target=_blank>Patreon</a> to fund my machine learning/deep learning/software/hardware needs for my future crazy yet cool projects, and any monetary contributions to the Patreon are appreciated and will be put to good creative use.</div></div></div><div class=article-tags><a class="badge badge-light" href=/tags/gpt-2/>GPT-2</a>
<a class="badge badge-light" href=/tags/tensorflow/>TensorFlow</a></div><div class="media author-card" itemscope itemtype=http://schema.org/Person><img class="portrait mr-3" src="https://s.gravatar.com/avatar/28f09e3deff62333b3f32f19d3971d46?s=200')" itemprop=image alt=Avatar><div class=media-body><h5 class=card-title itemprop=name><a href=https://minimaxir.com/>Max Woolf</a></h5><h6 class=card-subtitle>Data Scientist at BuzzFeed</h6><p class=card-text itemprop=description>Ex-Apple. Carnegie Mellon graduate. Plotter of pretty charts. Former TechCrunch comment troll.</p><ul class=network-icon aria-hidden=true><li><a itemprop=sameAs href=https://twitter.com/minimaxir target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a itemprop=sameAs href=https://linkedin.com/in/minimaxir target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a itemprop=sameAs href=https://youtube.com/minimaxir target=_blank rel=noopener><i class="fab fa-youtube"></i></a></li><li><a itemprop=sameAs href=https://twitch.tv/minimaxir target=_blank rel=noopener><i class="fab fa-twitch"></i></a></li><li><a itemprop=sameAs href=mailto:max@minimaxir.com><i class="fas fa-envelope"></i></a></li></ul></div></div><div class=article-widget><div class=hr-light></div><h3>Related</h3><ul><li><a href=/2018/05/text-neural-networks/>How to Quickly Train a Text-Generating Neural Network for Free</a></li><li><a href=/2017/07/cpu-or-gpu/>Benchmarking TensorFlow on Cloud CPUs: Cheaper Deep Learning than Cloud GPUs</a></li><li><a href=/2017/06/reddit-deep-learning/>Predicting the Success of a Reddit Submission with Deep Learning and Keras</a></li></ul></div><div class=article-widget><div class=post-nav><div class=post-nav-item><div class=meta-nav>Next</div><a href=/2019/09/ctrl-fake-news/ rel=next>Experiments with Making Convincing AI-Generated Fake News</a></div><div class=post-nav-item><div class=meta-nav>Previous</div><a href=/2018/11/cheap-cron/ rel=prev>Run Any Scheduled Task/Cron Super-Cheap on Google Cloud Platform</a></div></div></div><section id=comments><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"minimaxir"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js></script><script>hljs.initHighlightingOnLoad();</script><script src=/js/academic.min.bc1d5e4f014b8d38d75521ad1ae2ab18.js></script><div class=container><footer class=site-footer><p class=powered-by>Copyright Max Woolf &copy; 2019 &middot;
Powered by the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a>.
<span class=float-right aria-hidden=true><a href=# id=back_to_top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&times;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>