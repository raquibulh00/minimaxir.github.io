<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.3.1"><meta name=author content="Max Woolf"><meta name=description content="With Reddit data in BigQuery, quantifying all the hundreds of millions of Reddit submissions and comments is trivial."><link rel=alternate hreflang=en-us href=/2015/10/reddit-bigquery/><meta name=theme-color content=#2962ff><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400italic,700|Source+Code+Pro:400,400italic,700&display=swap"><link rel=stylesheet href=/css/academic.min.d60353c45a1511432f9bc3071cf07140.css><link rel=stylesheet href=/css/academic.b011cb496b1db375ea969a485c505836.css><link rel=manifest href=/site.webmanifest><link rel=icon type=image/png href=/img/icon.png><link rel=apple-touch-icon type=image/png href=/img/icon-192.png><link rel=canonical href=/2015/10/reddit-bigquery/><meta property=twitter:card content=summary_large_image><meta property=twitter:site content=@minimaxir><meta property=twitter:creator content=@minimaxir><meta property=og:site_name content="Max Woolf's Blog"><meta property=og:url content=/2015/10/reddit-bigquery/><meta property=og:title content="How to Analyze Every Reddit Submission and Comment, in Seconds, for Free | Max Woolf's Blog"><meta property=og:description content="With Reddit data in BigQuery, quantifying all the hundreds of millions of Reddit submissions and comments is trivial."><meta property=og:image content=/2015/10/reddit-bigquery/featured.png><meta property=twitter:image content=/2015/10/reddit-bigquery/featured.png><meta property=og:locale content=en-us><meta property=article:published_time content=2015-10-02T08:00:00&#43;00:00><meta property=article:modified_time content=2015-10-02T08:00:00&#43;00:00><title>How to Analyze Every Reddit Submission and Comment, in Seconds, for Free | Max Woolf&#39;s Blog</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id=navbar-main><div class=container><a class=navbar-brand href=/>Max Woolf&#39;s Blog</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/about/><span>About</span></a></li><li class=nav-item><a class=nav-link href=/post/><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/portfolio/><span>Portfolio</span></a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=https://www.patreon.com/minimaxir target=_blank rel=noopener><span><i class="fab fa-patreon"></i>Patreon</span></a></li><li class=nav-item><a class=nav-link href=https://github.com/minimaxir target=_blank rel=noopener><span><i class="fab fa-github-alt"></i>GitHub</span></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><article class=article itemscope itemtype=http://schema.org/Article><div class="article-container pt-3"><h1 itemprop=name>How to Analyze Every Reddit Submission and Comment, in Seconds, for Free</h1><meta content="2015-10-02 08:00:00 &#43;0000 UTC" itemprop=datePublished><meta content="2015-10-02 08:00:00 &#43;0000 UTC" itemprop=dateModified><div class=article-metadata><span class=article-date><time>October 2, 2015</time></span>
<span class=middot-divider></span><span class=article-reading-time>7 min read</span>
<span class=middot-divider></span><a href=/2015/10/reddit-bigquery/#disqus_thread></a><span class=middot-divider></span><span class=article-categories><i class="fas fa-folder"></i><a href=/categories/data-science/>Data Science</a>, <a href=/categories/big-data/>Big Data</a></span><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=/2015/10/reddit-bigquery/&amp;text=How%20to%20Analyze%20Every%20Reddit%20Submission%20and%20Comment,%20in%20Seconds,%20for%20Free" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=/2015/10/reddit-bigquery/&amp;t=How%20to%20Analyze%20Every%20Reddit%20Submission%20and%20Comment,%20in%20Seconds,%20for%20Free" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook-f"></i></a></li><li><a href="mailto:?subject=How%20to%20Analyze%20Every%20Reddit%20Submission%20and%20Comment,%20in%20Seconds,%20for%20Free&amp;body=/2015/10/reddit-bigquery/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=/2015/10/reddit-bigquery/&amp;title=How%20to%20Analyze%20Every%20Reddit%20Submission%20and%20Comment,%20in%20Seconds,%20for%20Free" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://reddit.com/submit?url=/2015/10/reddit-bigquery/&amp;title=How%20to%20Analyze%20Every%20Reddit%20Submission%20and%20Comment,%20in%20Seconds,%20for%20Free" target=_blank rel=noopener class=share-btn-reddit><i class="fab fa-reddit-alien"></i></a></li></ul></div></div></div><div class=article-container><div class=article-style itemprop=articleBody><div class="alert alert-info"><div>This post uses the #legacySQL dialect of BigQuery SQL.</div></div><p>While working on my <a href=http://minimaxir.com/2014/12/reddit-statistics/ target=_blank>statistical analysis of 142 million Reddit submissions</a> last year, I had a surprising amount of trouble settings things up. It took a few hours to download the 40+ gigabytes of compressed data, and another few hours to parse the data and store in a local database. Even then, on my old-but-still-pretty-fast desktop PC, simple queries on the entire dataset took minutes to run, with complex queries occasionally taking upwards to an hour.</p><p>Over the past year, I&rsquo;ve had a lot of success using <a href=https://cloud.google.com/bigquery/ target=_blank>Google&rsquo;s BigQuery</a> tool for quick big data analysis without having to manage the data, such as the <a href=http://minimaxir.com/2015/08/nyc-map/ target=_blank>recent NYC Taxi data dump</a>. Recently, Jason Michael Baumgartner of <a href=https://pushshift.io target=_blank>Pushshift.io</a> (a.k.a <a href=https://www.reddit.com/user/Stuck_In_the_Matrix target=_blank>/u/Stuck_In_The_Matrix</a> on Reddit), who also provided me the original Reddit data, released <a href=https://www.reddit.com/r/datasets/comments/3mg812/full_reddit_submission_corpus_now_available_2006/ target=_blank>new Reddit datasets</a> containing all submissions and all comments until August 2015. Google BigQuery Developer Advocate Felipe Hoffa <a href=https://www.reddit.com/r/bigquery/comments/3mv82i/dataset_reddits_full_post_history_shared_on/ target=_blank>uploaded the dataset</a> to a public table in BigQuery for anyone to perform analysis on the data.</p><p><img src=/img/reddit-bigquery/bigquery-tool.png alt></p><p>With Reddit data in BigQuery, quantifying all the hundreds of millions of Reddit submissions and comments is trivial.</p><h2 id=hello-reddit>Hello Reddit!</h2><p><em>Although the data is retrieved and processed in seconds, making the data visualizations in this post takes slightly longer. You can view the R code needed to reproduce the visualizations in <a href=https://github.com/minimaxir/reddit-bigquery/blob/master/reddit_bigquery.ipynb target=_blank>this Jupyter notebook</a> open-sourced on GitHub.</em></p><p>BigQuery allows 1 terabyte (1000 GB) of <a href=https://cloud.google.com/bigquery/pricing target=_blank>free data processing</a> per month; which is much more than it sounds like, and you&rsquo;ll see why.</p><p>BigQuery syntax works similar to typical SQL. If you can perform basic SQL aggregations such as <code>COUNT</code> and <code>AVG</code> on a tiny database, you can perform the same aggregations on a 100+ GB dataset.</p><p>We can write a simple query to just calculate how many Reddit submissions are made each day, to both check the robustness of the data, and to show how much Reddit has grown. (note that the <code>created</code> field is in seconds UTC; you will need to convert it to a timestamp, then convert to a <code>DATE</code> which converts the timestamp to a YYYY-MM-DD string, which is the format you want since it sorts lexigraphically)</p><pre><code class=language-sql>SELECT DATE(SEC_TO_TIMESTAMP(created)) date_submission,
COUNT(*) as num_submissions
FROM [fh-bigquery:reddit_posts.full_corpus_201509]
GROUP BY date_submission
ORDER by date_submission
</code></pre><p>Which results in a simple timeseries, as shown above. BigQuery allows you to download the data as a CSV, and it can easily be visualized in any statistical program such as Excel.</p><p>Of course, I prefer R and ggplot2.</p><p><img src=/img/reddit-bigquery/reddit-bigquery-1.png alt></p><p>And that query only took 4.5 seconds to complete, with 1.46 GB data processed! BigQuery only counts the columns used in the query against the 1 TB quota; since the query only used one small column, the query is cheap. (if the query hits the raw submission title/comment text data, then the queries will be significantly larger data-wise)</p><h2 id=when-is-the-best-time-to-post-to-reddit>When is the best time to post to Reddit?</h2><p>One of the reasons people might look at Reddit data is for determining the best way to viral. One of the most important factors in making something go viral is timing; since Reddit has a ranking system based on both community approval and time-since-submission, along with the fact that there is more activity at certain times of the day, the time when a submission is made is especially important.</p><p>So here&rsquo;s another aggregation query that aggregates on the day of week a submission is made, and the hour when the submission is made; this creates a 7x24 matrix of timeslot possibilities. Both values are set to Eastern Standard Time, as U.S.-target websites tend to follow that timezone. Lastly, instead of checking the average amount of points for each submission at each timeslot (which would be skewed by the very high proportion of submissions with no upvotes), we&rsquo;ll look at how many submissions go viral (&gt;3000) at each timeslot with a conditional <code>SUM(IF())</code> statement (which is equivalent to Excel&rsquo;s <code>COUNTIF</code> conditional)</p><pre><code class=language-sql>SELECT
  DAYOFWEEK(SEC_TO_TIMESTAMP(created - 60*60*5)) as sub_dayofweek,
  HOUR(SEC_TO_TIMESTAMP(created - 60*60*5)) as sub_hour,
  SUM(IF(score &gt;= 3000, 1, 0)) as num_gte_3000,
FROM [fh-bigquery:reddit_posts.full_corpus_201509]
GROUP BY sub_dayofweek, sub_hour
ORDER BY sub_dayofweek, sub_hour
</code></pre><p>2.5 seconds, 2.39 GB processed, and a few R tricks results in this:</p><p><img src=/img/reddit-bigquery/reddit-bigquery-2.png alt></p><p>The day-of-week mostly does not matter, but hour matters significantly: about 3x as many submissions go viral when they are posted in the morning EST than if they are posted late in the day. (and rarely anything comparatively goes viral posted late at night, which is intuitive enough)</p><h2 id=creating-wordclouds-of-subreddit-comments>Creating Wordclouds of Subreddit Comments</h2><p>Wordclouds are always a fun representation of data, although not necessarily the most quantifiable. BigQuery can help derive word counts on large quantities of data, although the query is much more complex.</p><p>Due to the amount of data, we&rsquo;ll only look at the latest Reddit comment data (August 2015), and we&rsquo;ll look at the <a href=https://www.reddit.com/r/news target=_blank>/r/news</a> subreddit to see if there are any linguistic trends. In a subquery, the comment text (from non-bots!) is stripped of punctuation, set to lower case, and split into individual words; each word is counted and aggregated.</p><pre><code class=language-sql>SELECT word, COUNT(*) as num_words, AVG(score) as avg_score
FROM(FLATTEN((
  SELECT SPLIT(LOWER(REGEXP_REPLACE(body, r'[\.\&quot;,*:()\[\]/|\n]', ' ')), ' ') word, score
  FROM [fh-bigquery:reddit_comments.2015_08]
  WHERE author NOT IN (SELECT author FROM [fh-bigquery:reddit_comments.bots_201505])
  AND subreddit=&quot;news&quot;
  ), word))
GROUP EACH BY word
HAVING num_words &gt;= 10000
ORDER BY num_words DESC
</code></pre><p>5.0 seconds and 11.3 GB processed, along with removing a few stopwords via R results in this.</p><p><img src=/img/reddit-bigquery/reddit-bigquery-3.png alt></p><p>News is about <strong>people</strong>. And <strong>more</strong>. Makes sense. (An <code>avg_score</code> column for each word is included to allow for emulation of quantifiable impact of a given word, as used in my <a href=http://minimaxir.com/2015/01/linkbait/ target=_blank>BuzzFeed analysis</a>, although that is less useful for comments than submissions.)</p><h2 id=monthly-active-users>Monthly Active Users</h2><p>Although Reddit does provide a count of Subscribers for a given subreddit, most of those users are passive. One of the most important metrics of any startup is Monthly Active Users (MAUs), which we can get a reasonable approximation using the comment data. And not only that, we can aggregate the unique number of commenters by a given subreddit and by a given month, to see how subreddit activity changes over time relative to other subreddits.</p><p>How this works in BigQuery is a little more complicated, and requires the use of window functions:</p><ul><li>Aggregate by subreddit, month, and count of unique comment authors on <em>all</em> comments (this will result in a query with a lot of data processed!)</li><li>For each month, rank the subreddits by number of unique authors.</li><li>Take the top 20 subreddits by rank for each given month.</li></ul><pre><code class=language-sql>SELECT subreddit, date, unique_authors FROM
(SELECT subreddit, date, unique_authors, ROW_NUMBER() OVER (PARTITION BY date ORDER BY unique_authors DESC) rank FROM
(SELECT subreddit, LEFT(DATE(SEC_TO_TIMESTAMP(created_utc)), 7) as date, COUNT(UNIQUE(author)) as unique_authors FROM TABLE_QUERY([fh-bigquery:reddit_comments], &quot;table_id CONTAINS '20' AND LENGTH(table_id)&lt;8&quot;)
GROUP EACH BY subreddit, date
))
WHERE rank &lt;= 20
ORDER BY date ASC, unique_authors DESC
</code></pre><p>11.9 seconds and 53.3 GB (!) later, we can create a Top 20 Subreddits visualization for each month, and combine each image into a GIF with my trusty <a href=https://github.com/minimaxir/frames-to-gif-osx target=_blank>Convert Frames to GIF</a> tool.</p><p><img src=/img/reddit-bigquery/subreddit-ranks.gif alt></p><p>You can view the individual frames <a href=https://github.com/minimaxir/reddit-bigquery/tree/master/subreddit-ranks target=_blank>in the project GitHub repository</a>. There are many trends revealed, such how some subreddits die over time (<a href=https://www.reddit.com/r/fffffffuuuuuuuuuuuu target=_blank>r/fffffffuuuuuuuuuuuu</a>, <a href=https://www.reddit.com/r/technology target=_blank>/r/technology</a>), how some rise over time (<a href=https://www.reddit.com/r/pcmasterrace target=_blank>/r/pcmasterrace</a>), and how some subreddits have relative monthly spikes (<a href=https://www.reddit.com/r/thebutton target=_blank>/r/thebutton</a>). Note that the colors have no visual meaning but are used to help easily differentiate between subreddits.</p><p>These sample queries are only a small sample of what can be done with the Reddit data and BigQuery. Although some data scientists may argue that using is BigQuery is pointless since ~200GB of data <a href=http://yourdatafitsinram.com target=_blank>can fit in RAM</a>, the quick, dirty, and <em>cheap</em> option is much more pragmatic for the majority of potential data analysis on this Reddit dataset.</p><p><em>Again, you can view the R code needed to reproduce the visualizations in <a href=https://github.com/minimaxir/reddit-bigquery/blob/master/reddit_bigquery.ipynb target=_blank>this Jupyter notebook</a>.</em></p><div class="alert alert-note"><div>If you liked this blog post, I have set up a <a href=https://www.patreon.com/minimaxir target=_blank>Patreon</a> to fund my machine learning/deep learning/software/hardware needs for my future crazy yet cool projects, and any monetary contributions to the Patreon are appreciated and will be put to good creative use.</div></div></div><div class=article-tags><a class="badge badge-light" href=/tags/r/>R</a>
<a class="badge badge-light" href=/tags/ggplot2/>ggplot2</a>
<a class="badge badge-light" href=/tags/reddit/>Reddit</a>
<a class="badge badge-light" href=/tags/bigquery/>BigQuery</a></div><div class="media author-card" itemscope itemtype=http://schema.org/Person><img class="portrait mr-3" src="https://s.gravatar.com/avatar/28f09e3deff62333b3f32f19d3971d46?s=200')" itemprop=image alt=Avatar><div class=media-body><h5 class=card-title itemprop=name><a href=/>Max Woolf</a></h5><h6 class=card-subtitle>Data Scientist at BuzzFeed</h6><p class=card-text itemprop=description>Ex-Apple. Carnegie Mellon graduate. Plotter of pretty charts. Former TechCrunch comment troll.</p><ul class=network-icon aria-hidden=true><li><a itemprop=sameAs href=https://twitter.com/minimaxir target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a itemprop=sameAs href=https://linkedin.com/in/minimaxir target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a itemprop=sameAs href=https:/youtube.com/minimaxir target=_blank rel=noopener><i class="fab fa-youtube"></i></a></li><li><a itemprop=sameAs href=https:/twitch.tv/minimaxir target=_blank rel=noopener><i class="fab fa-twitch"></i></a></li><li><a itemprop=sameAs href=mailto:max@minimaxir.com><i class="fas fa-envelope"></i></a></li></ul></div></div><div class=article-widget><div class=hr-light></div><h3>Related</h3><ul><li><a href=/2014/12/reddit-statistics/>A Statistical Analysis of 142 Million Reddit Submissions</a></li><li><a href=/2013/11/daily-reddit/>Reddit is Growing Slowly, but Surely</a></li><li><a href=/2013/11/subreddit-size/>Which Subreddits on Reddit are the Largest?</a></li><li><a href=/2013/09/reddit-imgur-youtube/>1 in 3 Links Submitted To Reddit Go To Imgur Or YouTube</a></li><li><a href=/2015/09/bootstrap-resample/>Coding, Visualizing, and Animating Bootstrap Resampling</a></li></ul></div><div class=article-widget><div class=post-nav><div class=post-nav-item><div class=meta-nav>Next</div><a href=/2015/10/reddit-topwords/ rel=next>Quantifying and Visualizing the Reddit Hivemind</a></div><div class=post-nav-item><div class=meta-nav>Previous</div><a href=/2015/09/bootstrap-resample/ rel=prev>Coding, Visualizing, and Animating Bootstrap Resampling</a></div></div></div><section id=comments><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"minimaxir"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js></script><script id=dsq-count-scr src=//minimaxir.disqus.com/count.js async></script><script>hljs.initHighlightingOnLoad();</script><script src=/js/academic.min.bc1d5e4f014b8d38d75521ad1ae2ab18.js></script><div class=container><footer class=site-footer><p class=powered-by>Copyright Max Woolf &copy; 2019 &middot;
Powered by the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a>.
<span class=float-right aria-hidden=true><a href=# id=back_to_top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&times;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>