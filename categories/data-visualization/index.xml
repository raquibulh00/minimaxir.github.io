<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data Visualization on Max Woolf&#39;s Blog</title><link>https://minimaxir.com/categories/data-visualization/</link><description>Recent content in Data Visualization on Max Woolf&#39;s Blog</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright Max Woolf &amp;copy; {year}</copyright><lastBuildDate>Mon, 10 Sep 2018 09:15:00 -0700</lastBuildDate><atom:link href="https://minimaxir.com/categories/data-visualization/index.xml" rel="self" type="application/rss+xml"/><item><title>Problems with Predicting Post Performance on Reddit and Other Link Aggregators</title><link>https://minimaxir.com/2018/09/modeling-link-aggregators/</link><pubDate>Mon, 10 Sep 2018 09:15:00 -0700</pubDate><guid>https://minimaxir.com/2018/09/modeling-link-aggregators/</guid><description>
&lt;p&gt;&lt;a href=&#34;https://www.reddit.com&#34; target=&#34;_blank&#34;&gt;Reddit&lt;/a&gt;, &amp;ldquo;the front page of the internet&amp;rdquo; is a link aggregator where anyone can submit links to cool happenings. Over the years, Reddit has expanded from just being a link aggregator, to allowing image and videos, and as of recently, hosting images and videos itself.&lt;/p&gt;
&lt;p&gt;Reddit is broken down into subreddits, where each subreddit represents each own community around a particular interest, like &lt;a href=&#34;https://www.reddit.com/r/aww&#34; target=&#34;_blank&#34;&gt;/r/aww&lt;/a&gt; for pet photos and &lt;a href=&#34;https://www.reddit.com/r/politics/&#34; target=&#34;_blank&#34;&gt;/r/politics&lt;/a&gt; for U.S. politics. The posts on each subreddit are ranked by some function of both time elapsed since the submission was made, and the &lt;em&gt;score&lt;/em&gt; of the submission as determined by upvotes and downvotes from other users.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_aww.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s also an intrinsic pride in having something you&amp;rsquo;re responsible for providing to the community get lots of upvotes (the submitter also earns karma based on received upvotes, although karma is meaningless and doesn&amp;rsquo;t provide any user benefits). But the reality is that even on the largest subreddits, submissions with 1 point (the default score for new submissions) are the most prominent, with some subreddits having &lt;em&gt;over half&lt;/em&gt; of their submissions with only 1 point.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_dist_facet.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The exposure from having a submission go viral on Reddit (especially on larger subreddits) can be valuable especially if its your own original content. As a result, there has been a lot of &lt;a href=&#34;https://www.brandwatch.com/blog/how-to-get-on-the-front-page-of-reddit/&#34; target=&#34;_blank&#34;&gt;analysis&lt;/a&gt;/&lt;a href=&#34;https://www.reddit.com/r/starterpacks/comments/8rkfk9/reddit_front_page_starter_pack/&#34; target=&#34;_blank&#34;&gt;stereotypes&lt;/a&gt; on what techniques to do to help your submission make it to the top of the front page. But almost all claims of &amp;ldquo;cracking&amp;rdquo; the Reddit algorithm are &lt;a href=&#34;https://en.wikipedia.org/wiki/Post_hoc_ergo_propter_hoc&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;post hoc&lt;/em&gt; rationalizations&lt;/a&gt;, attributing success to things like submission timing and title verbiage of a single submission after the fact. The nature of algorithmic feeds inherently leads to a &lt;a href=&#34;https://en.wikipedia.org/wiki/Survivorship_bias&#34; target=&#34;_blank&#34;&gt;survivorship bias&lt;/a&gt;: although users may recognize certain types of posts that appear on the front page, there are many more which follow the same patterns but fail, which makes modeling a successful post very tricky.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve touched on analyzing Reddit post performance &lt;a href=&#34;https://minimaxir.com/2017/06/reddit-deep-learning/&#34; target=&#34;_blank&#34;&gt;before&lt;/a&gt;, but let&amp;rsquo;s give it another look and see if we can drill down on why Reddit posts do and do not do well.&lt;/p&gt;
&lt;h2 id=&#34;submission-timing&#34;&gt;Submission Timing&lt;/h2&gt;
&lt;p&gt;As with many US-based websites, the majority of Reddit users are most active during work hours (9 AM â€” 5 PM Eastern time weekdays). Most subreddits have submission patterns which fit accordingly.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_subreddit_prop.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But what&amp;rsquo;s interesting are the subreddits which &lt;em&gt;deviate&lt;/em&gt; from that standard. Gaming subreddits (&lt;a href=&#34;https://www.reddit.com/r/DestinyTheGame&#34; target=&#34;_blank&#34;&gt;/r/DestinyTheGame&lt;/a&gt;, &lt;a href=&#34;https://www.reddit.com/r/Overwatch&#34; target=&#34;_blank&#34;&gt;/r/Overwatch&lt;/a&gt;) have short activity after a Tuesday game update/patch, game &lt;em&gt;communication&lt;/em&gt; subreddits (&lt;a href=&#34;https://www.reddit.com/r/Fireteams&#34; target=&#34;_blank&#34;&gt;/r/Fireteams&lt;/a&gt;, &lt;a href=&#34;https://www.reddit.com/r/RocketLeagueExchange&#34; target=&#34;_blank&#34;&gt;/r/RocketLeagueExchange&lt;/a&gt;) are more active &lt;em&gt;outside&lt;/em&gt; of work hours as they assume you are playing the game at the time, and Not-Safe-For-Work subreddits (/r/dirtykikpals, /r/gonewild) are incidentally less active during work hours and more active late-night than other subreddits.&lt;/p&gt;
&lt;p&gt;Whenever you make a submission to Reddit, the submission appears in the subreddit&amp;rsquo;s &lt;code&gt;/new&lt;/code&gt; queue of the most recent submissions, where hopefully kind souls will find your submission and upvote it if it&amp;rsquo;s good.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_new.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, if it falls off the first page of the &lt;code&gt;/new&lt;/code&gt; queue, your submission might be as good as dead. As a result, there&amp;rsquo;s an element of game theory to timing your submission if you want it to not become another 1-point submission. Is it better to submit during peak hours when more users may see the submission before it falls off of &lt;code&gt;/new&lt;/code&gt;? Is it better to submit &lt;em&gt;before&lt;/em&gt; peak usage since there will be less competition, then continue the momentum once it hits the front page?&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a look at the median post performance at each given time slot for top subreddits:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_subreddit_hr_doy.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As the earlier distribution chart implied, the median score is around 1-2 for most subreddits, and that&amp;rsquo;s consistent across all time slots. Some subreddits with higher medians like /r/me_irl do appear to have a &lt;em&gt;slight&lt;/em&gt; benefit when posting before peak activity. When focusing on subreddits with high overall median scores, the difference is more explicit.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_subreddit_highmedian.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Subreddits like /r/PrequelMemes and /r/The_Donald &lt;em&gt;definitely&lt;/em&gt; have better performance on average when made before peak activity! Posting before peak usage &lt;em&gt;does&lt;/em&gt; appear to be a viable strategy, however for the majority of subreddits it doesn&amp;rsquo;t make much of a difference.&lt;/p&gt;
&lt;h2 id=&#34;submission-titles&#34;&gt;Submission Titles&lt;/h2&gt;
&lt;p&gt;Each Reddit subreddit has their own vocabulary and topics of discussion. Let&amp;rsquo;s break down text by subreddit by looking at the 75th percentile for score on posts containing a given two-word phrase:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_subreddit_topbigrams.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The one trend consistent across all subreddits is the effectiveness of first-person pronouns (&lt;em&gt;I/my&lt;/em&gt;) and original content (&lt;em&gt;fan art&lt;/em&gt;). Other than that, the vocabulary and sentiment for successful posts is very specific to the subreddit and culture is represents; no universal guaranteed-success memes.&lt;/p&gt;
&lt;h2 id=&#34;can-deep-learning-predict-post-performance&#34;&gt;Can Deep Learning Predict Post Performance?&lt;/h2&gt;
&lt;p&gt;Some might think &amp;ldquo;oh hey, this is an arbitrary statistical problem, you can just build an AI to solve it!&amp;rdquo; So, for the sake of argument, I did.&lt;/p&gt;
&lt;p&gt;Instead of using Reddit data for building a deep learning model, we&amp;rsquo;ll use data from &lt;a href=&#34;https://news.ycombinator.com&#34; target=&#34;_blank&#34;&gt;Hacker News&lt;/a&gt;, another link aggregator similar to Reddit with a strong focus on technology and startup entrepreneurship. The distribution of scores on posts, submission timings, upvoting, and front page ranking systems are all the same as on Reddit.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/hn.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The titles on Hacker News submissions are also shorter (80 characters max vs. Reddit&amp;rsquo;s 300 character max) and in concise English (no memes/shitposts allowed), which should help the model learn the title syntax and identify high-impact keywords easier. Like Reddit, the score data is super-skewed with most HN submissions at 1-2 points, and typical model training will quickly converge but try to predict that &lt;em&gt;every&lt;/em&gt; submission has a score of 1, which isn&amp;rsquo;t helpful!&lt;/p&gt;
&lt;p&gt;By constructing a model employing &lt;em&gt;many&lt;/em&gt; deep learning tricks with &lt;a href=&#34;https://keras.io&#34; target=&#34;_blank&#34;&gt;Keras&lt;/a&gt;/&lt;a href=&#34;https://www.tensorflow.org&#34; target=&#34;_blank&#34;&gt;TensorFlow&lt;/a&gt; to prevent model cheating and training on &lt;em&gt;hundreds of thousands&lt;/em&gt; of HN submissions (using post title, day-of-week, hour, and link domain like &lt;code&gt;github.com&lt;/code&gt; as model features), the model does converge and finds some signal among the noise (training R&lt;sup&gt;2&lt;/sup&gt; ~ 0.55 when trained for 50 epochs). However, it fails to offer any valuable predictions on new, unseen posts (test R&lt;sup&gt;2&lt;/sup&gt; &lt;em&gt;&amp;lt; 0.00&lt;/em&gt;) because it falls into the same exact human biases regarding titles: it saw submissions with titles that did very well during training, but can&amp;rsquo;t isolate the random chance why X and Y submissions are similar but X goes viral while Y does not.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/hn_test.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve made the Keras/TensorFlow model training code available in &lt;a href=&#34;https://www.kaggle.com/minimaxir/hacker-news-submission-score-predictor/notebook&#34; target=&#34;_blank&#34;&gt;this Kaggle Notebook&lt;/a&gt; if you want to fork it and try to improve the model.&lt;/p&gt;
&lt;h2 id=&#34;other-potential-modeling-factors&#34;&gt;Other Potential Modeling Factors&lt;/h2&gt;
&lt;p&gt;The deep learning model above makes optimistic assumptions about the underlying data, including that each post behaves independently, and the included features are the sole features which determine the score. These assumptions are questionable.&lt;/p&gt;
&lt;p&gt;The simple model forgoes the content of the submission itself, which is hard to retrieve for hundreds of thousands of data points. On Hacker News that&amp;rsquo;s mostly OK since most submissions are links/articles which accurately correlate to the content, although occasionally there are idiosyncratic short titles which do the opposite. On Reddit, obviously looking at content is necessary for image/video-oriented subreddits, which is hard to gather and analyze at scale.&lt;/p&gt;
&lt;p&gt;A very important concept of post performance is &lt;em&gt;momentum&lt;/em&gt;. A post having a high score is a positive signal in itself, which begets more votes (a famous Reddit problem is brigading from /r/all which can cause submission scores to skyrocket). If the front page of a subreddit has a large number of high-performing posts, they might also suppress posts coming out of the &lt;code&gt;/new&lt;/code&gt; queue because the score threshold is much higher. A simple model may not be able to capture these impacts; the model would need to incorporate the &lt;em&gt;state of the front page&lt;/em&gt; at the time of posting.&lt;/p&gt;
&lt;p&gt;Some also try to manipulate upvotes. Reddit became famous for adding the rule &amp;ldquo;asking for upvotes is a violation of intergalactic law&amp;rdquo; to their &lt;a href=&#34;https://www.reddithelp.com/en/categories/rules-reporting/account-and-community-restrictions/what-constitutes-vote-cheating-or&#34; target=&#34;_blank&#34;&gt;Content Policy&lt;/a&gt;, although some subreddits do it anyway &lt;a href=&#34;https://www.reddit.com/r/TheoryOfReddit/comments/5qqrod/for_years_reddit_told_us_that_saying_upvote_this/&#34; target=&#34;_blank&#34;&gt;without consequence&lt;/a&gt;. On Reddit, obvious spam posts can be downvoted to immediately counteract illicit upvotes. Hacker News has a &lt;a href=&#34;https://news.ycombinator.com/newsfaq.html&#34; target=&#34;_blank&#34;&gt;similar don&amp;rsquo;t-upvote rule&lt;/a&gt;, although there aren&amp;rsquo;t downvotes, just a flagging mechanism which quickly neutralizes spam/misleading posts. In general, there&amp;rsquo;s no &lt;em&gt;legitimate&lt;/em&gt; reason to highlight your own submission immediately after its posted (except for Reddit&amp;rsquo;s AMAs). Fortunately, gaming the system is less impactful on Reddit and Hacker News due to their sheer size and countermeasures, but it&amp;rsquo;s a good example of potential user behavior that makes modeling post performance difficult, and hopefully link aggregators of the future aren&amp;rsquo;t susceptible to such shenanigans.&lt;/p&gt;
&lt;h2 id=&#34;do-we-really-to-predict-post-score&#34;&gt;Do We Really to Predict Post Score?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s say you are submitting original content to Reddit or your own tech project to Hacker News. More points means a higher ranking means more exposure for your link, right? Not exactly. As noted from Reddit/HN screenshots above, the scores of popular submissions are all over the place ranking-wise, having been affected by age penalties.&lt;/p&gt;
&lt;p&gt;In practical terms, from my own purely anecdotal experience, submissions at a top ranking receive &lt;em&gt;substantially&lt;/em&gt; more clickthroughs despite being spatially close on the page to others.&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&amp;hellip;and now traffic at #3.&lt;br&gt;&lt;br&gt;Placement is absurdly important for search engines/social media sites. Difference between #1 and #3 is dramatic. &lt;a href=&#34;https://t.co/nGjWJBx6dU&#34;&gt;pic.twitter.com/nGjWJBx6dU&lt;/a&gt;&lt;/p&gt;&amp;mdash; Max Woolf (@minimaxir) &lt;a href=&#34;https://twitter.com/minimaxir/status/877219784907149316?ref_src=twsrc%5Etfw&#34;&gt;June 20, 2017&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://twitter.com/minimaxir/status/877219784907149316&#34; target=&#34;_blank&#34;&gt;that case&lt;/a&gt;, falling from #1 to #3 &lt;em&gt;immediately halved&lt;/em&gt; the referral traffic coming from Hacker News.&lt;/p&gt;
&lt;p&gt;Therefore, an ideal link aggregator predictive model to maximize clicks should try to predict the &lt;em&gt;rank&lt;/em&gt; of a submission (max rank, average rank over &lt;em&gt;n&lt;/em&gt; period, etc.), not necessarily the score it receives. You could theoretically create a model by making a snapshot of a Reddit subreddit/front page of Hacker News every minute or so which includes the post position at the time of the snapshot. As mentioned earlier, the snapshots can also be used as a model feature to identify whether the front page is active or stale. Unfortunately, snapshots can&amp;rsquo;t be retrieved retroactively, and both storing, processing, and analyzing snapshots at scale is a difficult and &lt;em&gt;expensive&lt;/em&gt; feat of data engineering.&lt;/p&gt;
&lt;p&gt;Presumably Reddit&amp;rsquo;s data scientists would be incorporating submission position as a part of their data analytics and modeling, but after inspecting what&amp;rsquo;s sent to Reddit&amp;rsquo;s servers when you perform an action like upvoting, I wasn&amp;rsquo;t able to find a sent position value when upvoting from the feed: only the post score and post upvote percentage at the time of the action were sent.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/chrome.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example, I upvoted the &lt;code&gt;Fact are facts&lt;/code&gt; submission at position #5: we&amp;rsquo;d expect a value between &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt; be sent with the post metadata within the analytics payload, but that&amp;rsquo;s not the case.&lt;/p&gt;
&lt;p&gt;Optimizing ranking instead of a tangible metric or classification accuracy is a relatively underdiscussed field of modern data science (besides &lt;a href=&#34;https://en.wikipedia.org/wiki/Search_engine_optimization&#34; target=&#34;_blank&#34;&gt;SEO&lt;/a&gt; for getting the top spot on a Google search), and it would be interesting to dive deeper into it for other applications.&lt;/p&gt;
&lt;h2 id=&#34;in-the-future&#34;&gt;In the future&lt;/h2&gt;
&lt;p&gt;The moral of this post is that you should not take it personally if a submission fails to hit the front page. It doesn&amp;rsquo;t necessarily mean it&amp;rsquo;s bad. Conversely, if a post does well, donâ€™t assume that similar posts will do just as well. There&amp;rsquo;s a lot of quality content that falls through the cracks due to dumb luck. Fortunately, both Reddit and Hacker News allow reposts, which helps alleviate this particular problem.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s still a lot that can be done to more deterministically predict the behavior of these algorithmic feeds. There&amp;rsquo;s also room to help make these link aggregators more &lt;em&gt;fair&lt;/em&gt;. Unfortunately, there&amp;rsquo;s even more undiscovered ways to game these algorithms, and we&amp;rsquo;ll see how things play out.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can view the BigQuery queries used to get the Reddit and Hacker News data, plus the R and ggplot2 used to create the data visualizations, in &lt;a href=&#34;http://minimaxir.com/notebooks/modeling-link-aggregators/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;. You can also view the images/code used for this post in &lt;a href=&#34;https://github.com/minimaxir/modeling-link-aggregators&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Analyzing IMDb Data The Intended Way, with R and ggplot2</title><link>https://minimaxir.com/2018/07/imdb-data-analysis/</link><pubDate>Mon, 16 Jul 2018 09:45:00 +0000</pubDate><guid>https://minimaxir.com/2018/07/imdb-data-analysis/</guid><description>
&lt;p&gt;&lt;a href=&#34;https://www.imdb.com&#34; target=&#34;_blank&#34;&gt;IMDb&lt;/a&gt;, the Internet Movie Database, has been a popular source for data analysis and visualizations over the years. The combination of user ratings for movies and detailed movie metadata have always been fun to &lt;a href=&#34;http://minimaxir.com/2016/01/movie-revenue-ratings/&#34; target=&#34;_blank&#34;&gt;play with&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/movie-revenue-ratings/box-office-rating-5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are a number of tools to help get IMDb data, such as &lt;a href=&#34;https://github.com/alberanid/imdbpy&#34; target=&#34;_blank&#34;&gt;IMDbPY&lt;/a&gt;, which makes it easy to programmatically scrape IMDb by pretending it&amp;rsquo;s a website user and extracting the relevant data from the page&amp;rsquo;s HTML output. While it &lt;em&gt;works&lt;/em&gt;, web scraping public data is a gray area in terms of legality; many large websites have a Terms of Service which forbids scraping, and can potentially send a DMCA take-down notice to websites redistributing scraped data.&lt;/p&gt;
&lt;p&gt;IMDb has &lt;a href=&#34;https://help.imdb.com/article/imdb/general-information/can-i-use-imdb-data-in-my-software/G5JTRESSHJBBHTGX#&#34; target=&#34;_blank&#34;&gt;data licensing terms&lt;/a&gt; which forbid scraping and require an attribution in the form of a &lt;strong&gt;Information courtesy of IMDb (&lt;a href=&#34;http://www.imdb.com&#34; target=&#34;_blank&#34;&gt;http://www.imdb.com&lt;/a&gt;). Used with permission.&lt;/strong&gt; statement, and has also &lt;a href=&#34;https://www.kaggle.com/tmdb/tmdb-movie-metadata/home&#34; target=&#34;_blank&#34;&gt;DMCAed a Kaggle IMDb dataset&lt;/a&gt; to hone the point.&lt;/p&gt;
&lt;p&gt;However, there is good news! IMDb publishes an &lt;a href=&#34;https://www.imdb.com/interfaces/&#34; target=&#34;_blank&#34;&gt;official dataset&lt;/a&gt; for casual data analysis! And it&amp;rsquo;s now very accessible, just choose a dataset and download (now with no hoops to jump through), and the files are in the standard &lt;a href=&#34;https://en.wikipedia.org/wiki/Tab-separated_values&#34; target=&#34;_blank&#34;&gt;TSV format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/datasets.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The uncompressed files are pretty large; not &amp;ldquo;big data&amp;rdquo; large (it fits into computer memory), but Excel will explode if you try to open them in it. You have to play with the data &lt;em&gt;smartly&lt;/em&gt;, and both &lt;a href=&#34;https://www.r-project.org&#34; target=&#34;_blank&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/index.html&#34; target=&#34;_blank&#34;&gt;ggplot2&lt;/a&gt; have neat tricks to do just that.&lt;/p&gt;
&lt;h2 id=&#34;first-steps&#34;&gt;First Steps&lt;/h2&gt;
&lt;p&gt;R is a popular programming language for statistical analysis. One of the most popular series of external packages is the &lt;code&gt;tidyverse&lt;/code&gt; package, which automatically imports the &lt;code&gt;ggplot2&lt;/code&gt; data visualization library and other useful packages which we&amp;rsquo;ll get to one-by-one. We&amp;rsquo;ll also use &lt;code&gt;scales&lt;/code&gt; which we&amp;rsquo;ll use later for prettier number formatting. First we&amp;rsquo;ll load these packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
library(scales)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we can load a TSV downloaded from IMDb using the &lt;code&gt;read_tsv&lt;/code&gt; function from &lt;code&gt;readr&lt;/code&gt; (a tidyverse package), which does what the name implies, at a much faster speed than base R (+ a couple other parameters to handle data encoding). Let&amp;rsquo;s start with the &lt;code&gt;ratings&lt;/code&gt; file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;df_ratings &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; read_tsv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;title.ratings.tsv&amp;#39;&lt;/span&gt;, na &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;\\N&amp;#34;&lt;/span&gt;, quote &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can preview what&amp;rsquo;s in the loaded data using &lt;code&gt;dplyr&lt;/code&gt; (a tidyverse package), which is what we&amp;rsquo;ll be using to manipulate data for this analysis. dplyr allows you to pipe commands, making it easy to create a sequence of manipulation commands. For now, we&amp;rsquo;ll use &lt;code&gt;head()&lt;/code&gt;, which displays the top few rows of the data frame.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_ratings %&amp;gt;% head()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/ratings.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each of the &lt;strong&gt;873k rows&lt;/strong&gt; corresponds to a single movie, an ID for the movie, its average rating (from 1 to 10), and the number of votes which contribute to that average. Since we have two numeric variables, why not test out ggplot2 by creating a scatterplot mapping them? ggplot2 takes in a data frame and names of columns as aesthetics, then you specify what type of shape to plot (a &amp;ldquo;geom&amp;rdquo;). Passing the plot to &lt;code&gt;ggsave&lt;/code&gt; saves it as a standalone, high-quality data visualization.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot &amp;lt;- ggplot(df_ratings, aes(x = numVotes, y = averageRating)) +
geom_point()
ggsave(&amp;quot;imdb-0.png&amp;quot;, plot, width = 4, height = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-0.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here is nearly &lt;em&gt;1 million&lt;/em&gt; points on a single chart; definitely don&amp;rsquo;t try to do that in Excel! However, it&amp;rsquo;s not a &lt;em&gt;useful&lt;/em&gt; chart since all the points are opaque and we&amp;rsquo;re not sure what the spatial density of points is. One approach to fix this issue is to create a heat map of points, which ggplot can do natively with &lt;code&gt;geom_bin2d&lt;/code&gt;. We can color the heat map with the &lt;a href=&#34;https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html&#34; target=&#34;_blank&#34;&gt;viridis&lt;/a&gt; colorblind-friendly palettes &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/scale_viridis.html&#34; target=&#34;_blank&#34;&gt;just introduced&lt;/a&gt; into ggplot2. We should also tweak the axes; the x-axis should be scaled logarithmically with &lt;code&gt;scale_x_log10&lt;/code&gt; since there are many movies with high numbers of votes and we can format those numbers with the &lt;code&gt;comma&lt;/code&gt; function from the &lt;code&gt;scales&lt;/code&gt; package (we can format the scale with &lt;code&gt;comma&lt;/code&gt; too). For the y-axis, we can add explicit number breaks for each rating; R can do this neatly by setting the breaks to &lt;code&gt;1:10&lt;/code&gt;. Putting it all together:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot &amp;lt;- ggplot(df_ratings, aes(x = numVotes, y = averageRating)) +
geom_bin2d() +
scale_x_log10(labels = comma) +
scale_y_continuous(breaks = 1:10) +
scale_fill_viridis_c(labels = comma)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not bad, although it unfortunately confirms that IMDb follows a &lt;a href=&#34;https://tvtropes.org/pmwiki/pmwiki.php/Main/FourPointScale&#34; target=&#34;_blank&#34;&gt;Four Point Scale&lt;/a&gt; where average ratings tend to fall between 6 â€” 9.&lt;/p&gt;
&lt;h2 id=&#34;mapping-movies-to-ratings&#34;&gt;Mapping Movies to Ratings&lt;/h2&gt;
&lt;p&gt;You may be asking &amp;ldquo;which ratings correspond to which movies?&amp;rdquo; That&amp;rsquo;s what the &lt;code&gt;tconst&lt;/code&gt; field is for. But first, let&amp;rsquo;s load the title data from &lt;code&gt;title.basics.tsv&lt;/code&gt; into &lt;code&gt;df_basics&lt;/code&gt; and take a look as before.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_basics &amp;lt;- read_tsv(&#39;title.basics.tsv&#39;, na = &amp;quot;\\N&amp;quot;, quote = &#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/basics1.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/basics2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have some neat movie metadata. Notably, this table has a &lt;code&gt;tconst&lt;/code&gt; field as well. Therefore, we can &lt;em&gt;join&lt;/em&gt; the two tables together, adding the movie information to the corresponding row in the rating table (in this case, a left join is more appropriate than an inner/full join)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_ratings &amp;lt;- df_ratings %&amp;gt;% left_join(df_basics)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Runtime minutes sounds interesting. Could there be a relationship between the length of a movie and its average rating on IMDb? Let&amp;rsquo;s make a heat map plot again, but with a few tweaks. With the new metadata, we can &lt;code&gt;filter&lt;/code&gt; the table to remove bad points; let&amp;rsquo;s keep movies only (as IMDb data also contains &lt;em&gt;television show data&lt;/em&gt;), with a runtime &amp;lt; 3 hours, and which have received atleast 10 votes by users to remove extraneous movies). X-axis should be tweaked to display the minutes-values in hours. The fill viridis palette can be changed to another one in the family (I personally like &lt;code&gt;inferno&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;More importantly, let&amp;rsquo;s discuss plot theming. If you want a minimalistic theme, add a &lt;code&gt;theme_minimal&lt;/code&gt; to the plot, and you can pass a &lt;code&gt;base_family&lt;/code&gt; to change the default font on the plot and a &lt;code&gt;base_size&lt;/code&gt; to change the font size. The &lt;code&gt;labs&lt;/code&gt; function lets you add labels to the plot (which you should &lt;em&gt;always&lt;/em&gt; do); you have your &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, and &lt;code&gt;y&lt;/code&gt; parameters, but you can also add a &lt;code&gt;subtitle&lt;/code&gt;, a &lt;code&gt;caption&lt;/code&gt; for attribution, and a &lt;code&gt;color&lt;/code&gt;/&lt;code&gt;fill&lt;/code&gt; to name the scale. Putting it all together:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot &amp;lt;- ggplot(df_ratings %&amp;gt;% filter(runtimeMinutes &amp;lt; 180, titleType == &amp;quot;movie&amp;quot;, numVotes &amp;gt;= 10), aes(x = runtimeMinutes, y = averageRating)) +
geom_bin2d() +
scale_x_continuous(breaks = seq(0, 180, 60), labels = 0:3) +
scale_y_continuous(breaks = 0:10) +
scale_fill_viridis_c(option = &amp;quot;inferno&amp;quot;, labels = comma) +
theme_minimal(base_family = &amp;quot;Source Sans Pro&amp;quot;, base_size = 8) +
labs(title = &amp;quot;Relationship between Movie Runtime and Average Mobie Rating&amp;quot;,
subtitle = &amp;quot;Data from IMDb retrieved July 4th, 2018&amp;quot;,
x = &amp;quot;Runtime (Hours)&amp;quot;,
y = &amp;quot;Average User Rating&amp;quot;,
caption = &amp;quot;Max Woolf â€” minimaxir.com&amp;quot;,
fill = &amp;quot;# Movies&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-2b.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that&amp;rsquo;s pretty nice-looking for only a few lines of code! Albeit unhelpful, as there doesn&amp;rsquo;t appear to be a correlation.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Note: for the rest of this post, the theming/labels code will be omitted for convenience)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;How about movie ratings vs. the year the movie was made? It&amp;rsquo;s a similar plot code-wise to the one above (one perk about &lt;code&gt;ggplot2&lt;/code&gt; is that there&amp;rsquo;s no shame in reusing chart code!), but we can add a &lt;code&gt;geom_smooth&lt;/code&gt;, which adds a nonparametric trendline with confidence bands for the trend; since we have a large amount of data, the bands are very tight. We can also fix the problem of &amp;ldquo;empty&amp;rdquo; bins by setting the color fill scale to logarithmic scaling. And since we&amp;rsquo;re adding a black trendline, let&amp;rsquo;s change the viridis palette to &lt;code&gt;plasma&lt;/code&gt; for better contrast.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot &amp;lt;- ggplot(df_ratings %&amp;gt;% filter(titleType == &amp;quot;movie&amp;quot;, numVotes &amp;gt;= 10), aes(x = startYear, y = averageRating)) +
geom_bin2d() +
geom_smooth(color=&amp;quot;black&amp;quot;) +
scale_x_continuous() +
scale_y_continuous(breaks = 1:10) +
scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;, labels = comma, trans = &#39;log10&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, this trend hasn&amp;rsquo;t changed much either, although the presence of average ratings outside the Four Point Scale has increased over time.&lt;/p&gt;
&lt;h2 id=&#34;mapping-lead-actors-to-movies&#34;&gt;Mapping Lead Actors to Movies&lt;/h2&gt;
&lt;p&gt;Now that we have a handle on working with the IMDb data, let&amp;rsquo;s try playing with the larger datasets. Since they take up a lot of computer memory, we only want to persist data we actually might use. After looking at the schema provided with the official datasets, the only really useful metadata about the actors is their birth year, so let&amp;rsquo;s load that, but only keep both actors/actresses (using the fast &lt;code&gt;str_detect&lt;/code&gt; function from &lt;code&gt;stringr&lt;/code&gt;, another tidyverse package) and the relevant fields.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_actors &amp;lt;- read_tsv(&#39;name.basics.tsv&#39;, na = &amp;quot;\\N&amp;quot;, quote = &#39;&#39;) %&amp;gt;%
filter(str_detect(primaryProfession, &amp;quot;actor|actress&amp;quot;)) %&amp;gt;%
select(nconst, primaryName, birthYear)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/actor.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The principals dataset, the large 1.28GB TSV, is the most interesting. It&amp;rsquo;s an unnested list of the credited persons in each movie, with an &lt;code&gt;ordering&lt;/code&gt; indicating their rank (where &lt;code&gt;1&lt;/code&gt; means first, &lt;code&gt;2&lt;/code&gt; means second, etc.).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/principals.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For this analysis, let&amp;rsquo;s only look at the &lt;strong&gt;lead actors/actresses&lt;/strong&gt;; specifically, for each movie (identified by the &lt;code&gt;tconst&lt;/code&gt; value), filter the dataset to where the &lt;code&gt;ordering&lt;/code&gt; value is the lowest (in this case, the person at rank &lt;code&gt;1&lt;/code&gt; may not necessarily be an actor/actress).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_principals &amp;lt;- read_tsv(&#39;title.principals.tsv&#39;, na = &amp;quot;\\N&amp;quot;, quote = &#39;&#39;) %&amp;gt;%
filter(str_detect(category, &amp;quot;actor|actress&amp;quot;)) %&amp;gt;%
select(tconst, ordering, nconst, category) %&amp;gt;%
group_by(tconst) %&amp;gt;%
filter(ordering == min(ordering))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both datasets have a &lt;code&gt;nconst&lt;/code&gt; field, so let&amp;rsquo;s join them together. And then join &lt;em&gt;that&lt;/em&gt; to the ratings table earlier via &lt;code&gt;tconst&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_principals &amp;lt;- df_principals %&amp;gt;% left_join(df_actors)
df_ratings &amp;lt;- df_ratings %&amp;gt;% left_join(df_principals)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a fully denormalized dataset in &lt;code&gt;df_ratings&lt;/code&gt;. Since we now have the movie release year and the birth year of the lead actor, we can now infer &lt;em&gt;the age of the lead actor at the movie release&lt;/em&gt;. With that goal, filter out the data on the criteria we&amp;rsquo;ve used for earlier data visualizations, plus only keeping rows which have an actor&amp;rsquo;s birth year.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_ratings_movies &amp;lt;- df_ratings %&amp;gt;%
filter(titleType == &amp;quot;movie&amp;quot;, !is.na(birthYear), numVotes &amp;gt;= 10) %&amp;gt;%
mutate(age_lead = startYear - birthYear)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/denorm1.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/denorm2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;plotting-ages&#34;&gt;Plotting Ages&lt;/h2&gt;
&lt;p&gt;Age discrimination in movie casting has been a recurring issue in Hollywood; in fact, in 2017 &lt;a href=&#34;https://www.hollywoodreporter.com/thr-esq/judge-pauses-enforcement-imdb-age-censorship-law-978797&#34; target=&#34;_blank&#34;&gt;a law was signed&lt;/a&gt; to force IMDb to remove an actor&amp;rsquo;s age upon request, which in February 2018 was &lt;a href=&#34;https://www.hollywoodreporter.com/thr-esq/californias-imdb-age-censorship-law-declared-unconstitutional-1086540&#34; target=&#34;_blank&#34;&gt;ruled to be unconstitutional&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Have the ages of movie leads changed over time? For this example, we&amp;rsquo;ll use a &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/geom_ribbon.html&#34; target=&#34;_blank&#34;&gt;ribbon plot&lt;/a&gt; to plot the ranges of ages of movie leads. A simple way to do that is, for each year, calculate the 25th &lt;a href=&#34;https://en.wikipedia.org/wiki/Percentile&#34; target=&#34;_blank&#34;&gt;percentile&lt;/a&gt; of the ages, the 50th percentile (i.e. the median), and the 75th percentile, where the 25th and 75th percentiles are the ribbon bounds and the line represents the median.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_actor_ages &amp;lt;- df_ratings_movies %&amp;gt;%
group_by(startYear) %&amp;gt;%
summarize(low_age = quantile(age_lead, 0.25, na.rm=T),
med_age = quantile(age_lead, 0.50, na.rm=T),
high_age = quantile(age_lead, 0.75, na.rm=T))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting it with ggplot2 is surprisingly simple, although you need to use different y aesthetics for the ribbon and the overlapping line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot &amp;lt;- ggplot(df_actor_ages %&amp;gt;% filter(startYear &amp;gt;= 1920) , aes(x = startYear)) +
geom_ribbon(aes(ymin = low_age, ymax = high_age), alpha = 0.2) +
geom_line(aes(y = med_age))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-8.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Turns out that in the 2000&amp;rsquo;s, the median age of lead actors started to &lt;em&gt;increase&lt;/em&gt;? Both the upper and lower bounds increased too. That doesn&amp;rsquo;t coalesce with the age discrimination complaints.&lt;/p&gt;
&lt;p&gt;Another aspect of these complaints is gender, as female actresses tend to be younger than male actors. Thanks to the magic of ggplot2 and dplyr, separating actors/actresses is relatively simple: add gender (encoded in &lt;code&gt;category&lt;/code&gt;) as a grouping variable, add it as a color/fill aesthetic in ggplot, and set colors appropriately (I recommend the &lt;a href=&#34;http://colorbrewer2.org/&#34; target=&#34;_blank&#34;&gt;ColorBrewer&lt;/a&gt; qualitative palettes for categorical variables).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_actor_ages_lead &amp;lt;- df_ratings_movies %&amp;gt;%
group_by(startYear, category) %&amp;gt;%
summarize(low_age = quantile(age_lead, 0.25, na.rm = T),
med_age = quantile(age_lead, 0.50, na.rm = T),
high_age = quantile(age_lead, 0.75, na.rm = T))
plot &amp;lt;- ggplot(df_actor_ages_lead %&amp;gt;% filter(startYear &amp;gt;= 1920), aes(x = startYear, fill = category, color = category)) +
geom_ribbon(aes(ymin = low_age, ymax = high_age), alpha = 0.2) +
geom_line(aes(y = med_age)) +
scale_fill_brewer(palette = &amp;quot;Set1&amp;quot;) +
scale_color_brewer(palette = &amp;quot;Set1&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-9.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s about a 10-year gap between the ages of male and female leads, and the gap doesn&amp;rsquo;t change overtime. But both start to rise at the same time.&lt;/p&gt;
&lt;p&gt;One possible explanation for this behavior is actor reuse: if Hollywood keeps casting the same actor/actresses, by construction the ages of the leads will start to steadily increase. Let&amp;rsquo;s verify that: with our list of movies and their lead actors, for each lead actor, order all their movies by release year, and add a ranking for the #th time that actor has been a lead actor. This is possible through the use of &lt;code&gt;row_number&lt;/code&gt; in dplyr, and &lt;a href=&#34;https://cran.r-project.org/web/packages/dplyr/vignettes/window-functions.html&#34; target=&#34;_blank&#34;&gt;window functions&lt;/a&gt; like &lt;code&gt;row_number&lt;/code&gt; are data science&amp;rsquo;s most useful secret.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_ratings_movies_nth &amp;lt;- df_ratings_movies %&amp;gt;%
group_by(nconst) %&amp;gt;%
arrange(startYear) %&amp;gt;%
mutate(nth_lead = row_number())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/row_number.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One more ribbon plot later (w/ same code as above + custom y-axis breaks):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-12.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Huh. The median and upper-bound #th time has &lt;em&gt;dropped&lt;/em&gt; over time? Hollywood has been promoting more newcomers as leads? That&amp;rsquo;s not what I expected!&lt;/p&gt;
&lt;p&gt;More work definitely needs to be done in this area. In the meantime, the official IMDb datasets are a lot more robust than I thought they would be! And I only used a fraction of the datasets; the rest tie into TV shows, which are a bit messier. Hopefully you&amp;rsquo;ve seen a good taste of the power of R and ggplot2 for playing with big-but-not-big data!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can view the R and ggplot used to create the data visualizations in &lt;a href=&#34;http://minimaxir.com/notebooks/imdb-data-analysis/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;, which includes many visualizations not used in this post. You can also view the images/code used for this post in &lt;a href=&#34;https://github.com/minimaxir/imdb-data-analysis&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Visualizing One Million NCAA Basketball Shots</title><link>https://minimaxir.com/2018/03/basketball-shots/</link><pubDate>Mon, 19 Mar 2018 09:20:00 -0700</pubDate><guid>https://minimaxir.com/2018/03/basketball-shots/</guid><description>
&lt;p&gt;So &lt;a href=&#34;https://www.ncaa.com/march-madness&#34; target=&#34;_blank&#34;&gt;March Madness&lt;/a&gt; is happing right now. In celebration, &lt;a href=&#34;https://www.google.com&#34; target=&#34;_blank&#34;&gt;Google&lt;/a&gt; uploaded &lt;a href=&#34;https://console.cloud.google.com/launcher/details/ncaa-bb-public/ncaa-basketball&#34; target=&#34;_blank&#34;&gt;massive basketball datasets&lt;/a&gt; from the &lt;a href=&#34;https://www.ncaa.com&#34; target=&#34;_blank&#34;&gt;NCAA&lt;/a&gt; and &lt;a href=&#34;https://www.sportradar.com/&#34; target=&#34;_blank&#34;&gt;Sportradar&lt;/a&gt; to &lt;a href=&#34;https://cloud.google.com/bigquery/&#34; target=&#34;_blank&#34;&gt;BigQuery&lt;/a&gt; for anyone to query and experiment. After learning that the &lt;a href=&#34;https://www.reddit.com/r/bigquery/comments/82nz17/dataset_statistics_for_ncaa_mens_and_womens/&#34; target=&#34;_blank&#34;&gt;dataset had location data&lt;/a&gt; on where basketball shots were made on the court, I played with it and a couple hours later, I created a decent heat map data visualization. The next day, I &lt;a href=&#34;https://www.reddit.com/r/dataisbeautiful/comments/837qnu/heat_map_of_1058383_basketball_shots_from_ncaa/&#34; target=&#34;_blank&#34;&gt;posted it&lt;/a&gt; to Reddit&amp;rsquo;s &lt;a href=&#34;https://www.reddit.com/r/dataisbeautiful&#34; target=&#34;_blank&#34;&gt;/r/dataisbeautiful subreddit&lt;/a&gt; where it earned about &lt;strong&gt;40,000 upvotes&lt;/strong&gt;. (!?)&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s dig a little deeper. Although visualizing basketball shots has been &lt;a href=&#34;http://www.slate.com/blogs/browbeat/2012/03/06/mapping_the_nba_how_geography_can_teach_players_where_to_shoot.html&#34; target=&#34;_blank&#34;&gt;done&lt;/a&gt; &lt;a href=&#34;http://toddwschneider.com/posts/ballr-interactive-nba-shot-charts-with-r-and-shiny/&#34; target=&#34;_blank&#34;&gt;before&lt;/a&gt;, this time we have access to an order of magnitude more public data to do some really cool stuff.&lt;/p&gt;
&lt;h2 id=&#34;full-court&#34;&gt;Full Court&lt;/h2&gt;
&lt;p&gt;The Sportradar play-by-play table on BigQuery &lt;code&gt;mbb_pbp_sr&lt;/code&gt; has more than 1 million NCAA men&amp;rsquo;s basketball shots since the 2013-2014 season, with more being added now during March Madness. Here&amp;rsquo;s a heat map of the locations where those shots were made on the full basketball court:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_attempts_unlog.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can clearly see at a glance that the majority of shots are made right in front of the basket. For 3-point shots, the center and the corners have higher numbers of shot attempts than the other areas. But not much else since the data is so spatially skewed: setting the bin color scale to logarithmic makes trends more apparent and helps things go viral on Reddit.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_attempts.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now there&amp;rsquo;s more going on here: shot behavior is clearly symmetric on each side of the court, and there&amp;rsquo;s a small gap between the 3-point line and where 3-pt shots are typically made, likely to ensure that it it&amp;rsquo;s not accidentally ruled as a 2-pt shot.&lt;/p&gt;
&lt;p&gt;How likely is it to score a shot from a given spot? Are certain spots better than others?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_perc_success.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Surprisingly, shot accuracy is about &lt;em&gt;equal&lt;/em&gt; from anywhere within typical shooting distance, except directly in front of the basket where it&amp;rsquo;s much higher. What is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Expected_value&#34; target=&#34;_blank&#34;&gt;expected value&lt;/a&gt; of a shot at a given position: that is, how many points on average will they earn for their team?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_avg_points.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The average points earned for 3-pt shots is about 1.5x higher than many 2-pt shot locations in the inner court due to the equal accuracy, but locations next to the basket have an even higher expected value. Perhaps the accuracy of shots close to the basket is higher (&amp;gt;1.5x) than 3-pt shots and outweighs the lower point value?&lt;/p&gt;
&lt;p&gt;Since both sides of the court are indeed the same, we can combine the two sides and just plot a half-court instead. (Cross-court shots, which many Redditors &lt;a href=&#34;https://www.reddit.com/r/dataisugly/comments/839rax/basketball_heat_map_shows_an_impressive_number_of/&#34; target=&#34;_blank&#34;&gt;argued&lt;/a&gt; that they invalidated my visualizations above, constitute only &lt;em&gt;0.16%&lt;/em&gt; of the basketball shots in the dataset, so they can be safely removed as outliers).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_attempts_half_log.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are still a few oddities, such as shots being made &lt;em&gt;behind&lt;/em&gt; the basket. Let&amp;rsquo;s drill down a bit.&lt;/p&gt;
&lt;h2 id=&#34;focusing-on-basketball-shot-type&#34;&gt;Focusing on Basketball Shot Type&lt;/h2&gt;
&lt;p&gt;The Sportradar dataset classifies a shot as one of 5 major types: a &lt;strong&gt;jump shot&lt;/strong&gt; where the player jumps-and-throws the basketball, a &lt;strong&gt;layup&lt;/strong&gt; where the player runs down the field toward the basket and throws a one-handed shot, a &lt;strong&gt;dunk&lt;/strong&gt; where the player slams the ball into the basket (looking cool in the process), a &lt;strong&gt;hook shot&lt;/strong&gt; where the player close to the basket throws the ball with a hook motion, and a &lt;strong&gt;tip shot&lt;/strong&gt; where the player intercepts a basket rebound at the tip of the basket and pushes it in.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_prop_attempts.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, the most frequent types of shots are the less flashy, more practical jump shots and layups. But is a certain type of shot &amp;ldquo;better?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_perc.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Layups are safer than jump shots, but dunks are the most accurate of all the types (however, players likely wouldn&amp;rsquo;t attempt a dunk unless they knew it would be successful). The accuracy of layups and other close-to-basket shots is indeed more than 1.5x better than the jump shots of 3-pt shots, which explains the expected value behavior above.&lt;/p&gt;
&lt;p&gt;Plotting the heat maps for each type of shot offers more insight into how they work:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_attempts_half_types_log.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;They&amp;rsquo;re wildly different heat maps which match the shot type descriptions above, but show we&amp;rsquo;ll need to separate data visualizations by type to accurately see trends.&lt;/p&gt;
&lt;h2 id=&#34;impact-of-game-elapsed-time-at-time-of-shot&#34;&gt;Impact of Game Elapsed Time At Time of Shot&lt;/h2&gt;
&lt;p&gt;A NCAA basketball game lasts for 40 minutes total (2 halves of 20 minutes each), with the possibility of overtime. The &lt;a href=&#34;https://bigquery.cloud.google.com/savedquery/4194148158:3359d86507814fb19a5997a770456baa&#34; target=&#34;_blank&#34;&gt;example BigQuery&lt;/a&gt; for the NCAA-provided data compares the percentage of 3-point shots made during the first 35 minutes of the game versus the last 5 minutes: at the end of the game, accuracy was lower by 4 percentage points (31.2% vs. 35.1%). It might be interesting to facet these visualizations by the elapsed time of the game to see if there are any behavioral changes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_prop_type_elapsed.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There isn&amp;rsquo;t much difference between the proportions within a given half, but there is a difference between the first half and the second half, where the second half has fewer jump shots and more aggressive layups and dunks. After looking at shot success percentage:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_perc_success_type_elapsed.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The jump shot accuracy loss at the end of the game with Sportradar data is similar to that of the NCAA data, which is a good sanity check (but it&amp;rsquo;s odd that the accuracy drop only happens in the last 5 minutes and not elsewhere in the 2nd half). Layup accuracy increases in the second half with the number of layups.&lt;/p&gt;
&lt;p&gt;We can also visualize heat maps for each combo of shot type with time elapsed bucket, but given the results above, the changes in behavior over time may not be very perceptible.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_attempts_half_interval_log.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;impact-of-winning-losing-before-shot&#34;&gt;Impact of Winning/Losing Before Shot&lt;/h2&gt;
&lt;p&gt;Another theory worth exploring is determining if there is any difference whether a team is winning or losing when they make their shot (technically, when the delta between the team score and the other team score is positive for winning teams, negative for losing teams, or 0 if tied). Are players more relaxed when they have a lead? Are players more prone to making mistakes when losing?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_prop_type_score.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Layups are the same across all buckets, but for teams that are winning, there are fewer jump shots and &lt;strong&gt;more dunkin&amp;rsquo; action&lt;/strong&gt; (nearly double the dunks!). However, the accuracy chart illustrates an issue:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_perc_success_type_score.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Accuracy for most types of shots is much better for teams that are winning&amp;hellip;which may be the &lt;em&gt;reason&lt;/em&gt; they&amp;rsquo;re winning. More research can be done in this area.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I fully admit I am not a basketball expert. But playing around with this data was a fun way to get a new perspective on how collegiate basketball games work. There&amp;rsquo;s a lot more work that can be done with big basketball data and game strategy; the NCAA-provided data doesn&amp;rsquo;t have location data, but it does have &lt;strong&gt;6x more shots&lt;/strong&gt;, which will be very helpful for further fun in this area.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can view the R code, ggplot2 code, and BigQueries used to create the data visualizations in &lt;a href=&#34;http://minimaxir.com/notebooks/basketball-shots/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;. You can also view the images/code used for this post in &lt;a href=&#34;https://github.com/minimaxir/ncaa-basketball&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Special thanks to Ewen Gallic for his implementation of a &lt;a href=&#34;http://egallic.fr/en/drawing-a-basketball-court-with-r/&#34; target=&#34;_blank&#34;&gt;basketball court in ggplot2&lt;/a&gt;, which saved me a lot of time!&lt;/em&gt;&lt;/p&gt;</description></item><item><title>A Visual Overview of Stack Overflow&#39;s Question Tags</title><link>https://minimaxir.com/2018/02/stack-overflow-questions/</link><pubDate>Fri, 09 Feb 2018 09:00:00 -0700</pubDate><guid>https://minimaxir.com/2018/02/stack-overflow-questions/</guid><description>
&lt;p&gt;&lt;a href=&#34;https://stackoverflow.com&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt; is the most popular contemporary knowledge base for programming questions. But most interact with the site by Googling a programming question and getting a top result that links to SO. There isn&amp;rsquo;t as much discussion about actually &lt;em&gt;asking&lt;/em&gt; questions on the site.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/python_last_list.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I &lt;em&gt;could&lt;/em&gt; use &lt;a href=&#34;https://stackoverflow.com/users/9314418/minimaxir?tab=profile&#34; target=&#34;_blank&#34;&gt;my Stack Overflow account&lt;/a&gt; and test out the process of creating a question, but &lt;del&gt;I already know everything about programming&lt;/del&gt; there may be another way to learn how SO works. Stack Overflow &lt;a href=&#34;https://archive.org/details/stackexchange&#34; target=&#34;_blank&#34;&gt;releases an archive&lt;/a&gt; of all questions on the site every 3 months, and this archive is &lt;a href=&#34;https://cloud.google.com/bigquery/public-data/stackoverflow&#34; target=&#34;_blank&#34;&gt;syndicated to BigQuery&lt;/a&gt;, making it trivial to retrieve and analyze the millions of SO questions over the years. Even though (now-former) Stack Overflow data scientist &lt;a href=&#34;https://twitter.com/drob&#34; target=&#34;_blank&#34;&gt;David Robinson&lt;/a&gt; has written &lt;a href=&#34;https://stackoverflow.blog/2017/09/06/incredible-growth-python/&#34; target=&#34;_blank&#34;&gt;many&lt;/a&gt; &lt;a href=&#34;https://stackoverflow.blog/2017/04/19/programming-languages-used-late-night/&#34; target=&#34;_blank&#34;&gt;interesting&lt;/a&gt; blog posts for Stack Overflow with their data, I figured why not give it a try.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/python_last_list_answer.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Unlike social media sites like &lt;a href=&#34;https://twitter.com&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt; and &lt;a href=&#34;https://www.reddit.com&#34; target=&#34;_blank&#34;&gt;Reddit&lt;/a&gt; where the majority of traffic is driven within the first days after something is posted, posts on evergreen content sources like Stack Overflow are still relevant many years later. In fact, the traffic to Stack Overflow for most of 2017 (derived by finding the difference between question view counts from archive snapshots) is approximately uniform across question age, with a slight bias toward older content.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/so_overview.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In 2017, Stack Overflow received about 40k-50k new questions each week, an impressive feat:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/weekly_count.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For the rest of this post, we&amp;rsquo;ll only look at questions made in 2017 (until December; about 2.3 million questions total) in order to get a sense of the current development landscape, and what&amp;rsquo;s to come in the future. But what types of questions are they?&lt;/p&gt;
&lt;h2 id=&#34;tag-breakdown&#34;&gt;Tag Breakdown&lt;/h2&gt;
&lt;p&gt;All questions on Stack Overflow are required to have atleast 1 tag indicating the programming language/technologies involved with the question, and can have up to 5 tags. In the example &amp;ldquo;how do you get the last element of a list in Python&amp;rdquo; &lt;a href=&#34;https://stackoverflow.com/questions/930397/getting-the-last-element-of-a-list-in-python&#34; target=&#34;_blank&#34;&gt;question&lt;/a&gt; above, the tags are &lt;code&gt;python&lt;/code&gt;, &lt;code&gt;list&lt;/code&gt;, and &lt;code&gt;indexing&lt;/code&gt;. In 2017, most of new questions had 2-3 tags. (i.e. people aren&amp;rsquo;t &lt;a href=&#34;http://minimaxir.com/2014/03/hashtag-tag/&#34; target=&#34;_blank&#34;&gt;tag spamming&lt;/a&gt; like on &lt;a href=&#34;https://www.instagram.com/?hl=en&#34; target=&#34;_blank&#34;&gt;Instagram&lt;/a&gt; for maximum exposure).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/so_tag_breakdown.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In theory, tag spamming might make a question more likely to be answered; however for all tag counts, the proportion of questions with accepted answer (the green checkmark) is &lt;strong&gt;36-39%&lt;/strong&gt;, so there&amp;rsquo;s not much practical benefit from minmaxing tag counts. Which types of tagged questions are most likely to be answered?&lt;/p&gt;
&lt;p&gt;First, here&amp;rsquo;s the breakdown of the top 40 tags on Stack Overflow, by the number of new questions containing that tag for each month throughout 2017. This can give a sense of each technology&amp;rsquo;s growth/decline throughout the year.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/monthly_count_tag.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both new web development technologies like &lt;code&gt;reactjs&lt;/code&gt; and &lt;code&gt;typescript&lt;/code&gt; and data science tools like &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;r&lt;/code&gt; are trending upward.&lt;/p&gt;
&lt;p&gt;For the Top 1,000 tags, here are the top 30 tags by the proportion of questions which received an acceptable answer:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/acceptable_answer_top_30.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In contrast, here are the bottom 30 out of the Top 1,000:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/acceptable_answer_bottom_30.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The top tags are newer, sexier technologies like &lt;code&gt;rust&lt;/code&gt; and &lt;code&gt;dart&lt;/code&gt;, with another strong hint of data science tooling with &lt;code&gt;dplyr&lt;/code&gt; (which I used to aggregate the data for this post!) and &lt;code&gt;data.table&lt;/code&gt;. In contrast, the bottom tags are less sexy and more corporate like &lt;code&gt;salesforce&lt;/code&gt;, &lt;code&gt;drupal&lt;/code&gt;, and &lt;code&gt;sharepoint-2013&lt;/code&gt; (that&amp;rsquo;s why consultants who specialize in these technologies can get paid very well!).&lt;/p&gt;
&lt;p&gt;It should be noted these two charts do not necessarily imply that one technology is &amp;ldquo;better&amp;rdquo; than another, and the difference in answer rates may be due to question difficulty and the number of people skilled in the tech available that can answer it effectively.&lt;/p&gt;
&lt;p&gt;The timing when questions are asked might vary by tag. Per &lt;a href=&#34;https://stackoverflow.blog/2017/04/19/programming-languages-used-late-night/&#34; target=&#34;_blank&#34;&gt;a Stack Overflow analysis&lt;/a&gt;, people typically ask questions during the 9 AM - 5 PM work hours (although in my case, I cannot easily adjust for the time zone of the asker). How does this data fare?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/monthly_count_hr_doy.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This visualization is a bit weird. I adjusted the times to the Eastern time since internet activity for U.S.-based websites tends to revolve around that time zone. But for most technologies, the peak question-asking times are well before 9 AM to 5 PM: do those technologies correspond more to greater use in Europe and Asia? (In contrast, data-oriented technologies like &lt;code&gt;r&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;excel&lt;/code&gt; &lt;em&gt;do&lt;/em&gt; peak during the 9-5 block).&lt;/p&gt;
&lt;h2 id=&#34;how-easy-is-it-to-get-an-answer-by-tag&#34;&gt;How easy is it to get an answer by tag?&lt;/h2&gt;
&lt;p&gt;Stack Overflow caters the homepage toward the logged-in user&amp;rsquo;s recommended tags. Therefore, it&amp;rsquo;s not a surprise that the distribution of view counts on 2017 questions for each tag are very similar, although there is a slight edge toward the new &amp;ldquo;hip&amp;rdquo; technologies like &lt;code&gt;typescript&lt;/code&gt;, &lt;code&gt;spring&lt;/code&gt;, and &lt;code&gt;swift&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/views_boxplot_tag.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At the least, the distribution ensures that atleast 10 people see your question for these popular topics, which is nifty when you consider posts on Twitter and Reddit can die without any visibility at all. But will they provide an acceptable answer?&lt;/p&gt;
&lt;p&gt;The time it takes to get an acceptable answer also varies significantly by tag:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/acceptable_answer_density.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A median time of &lt;em&gt;15 minutes&lt;/em&gt; for tags like &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;arrays&lt;/code&gt; is pretty impressive! And even in the worst case scenario for these popular tags, the median is only a couple hours, much lower than I thought it would be.&lt;/p&gt;
&lt;h2 id=&#34;the-relationship-between-tags&#34;&gt;The Relationship Between Tags&lt;/h2&gt;
&lt;p&gt;As one would expect, the types of questions asked for each tag are much different. Here&amp;rsquo;s a wordcloud for each of the tags, quantifying the words most frequently used in the questions on those tags:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/so_tag_wordcloud.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notably, each word cloud is significantly different from reach other, even when technologies are related (also surprisingly true in the case of &lt;code&gt;angular&lt;/code&gt; and &lt;code&gt;angularjs&lt;/code&gt;!).&lt;/p&gt;
&lt;p&gt;How are the tags related anyways? We can calculate an &lt;a href=&#34;https://en.wikipedia.org/wiki/Adjacency_matrix&#34; target=&#34;_blank&#34;&gt;adjacency matrix&lt;/a&gt; of the tag pairs in the questions to see which tags are related:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/so_tag_adjacency.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking down a given row/column, you can see which technologies have a lot of questions in common with another (for example, &lt;code&gt;javascript&lt;/code&gt; and &lt;code&gt;json&lt;/code&gt; are frequently asked in conjunction with other tags).&lt;/p&gt;
&lt;p&gt;Going back earlier to talking about tag abuse, do the presence of certain pairs of tags lead to notably different answer rates?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/so_tag_adjacency_percent.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Tag pairs which don&amp;rsquo;t make much sense (e.g. &lt;code&gt;ios&lt;/code&gt;+&lt;code&gt;android&lt;/code&gt;, &lt;code&gt;ios&lt;/code&gt;+&lt;code&gt;javascript&lt;/code&gt;, &lt;code&gt;android&lt;/code&gt;+&lt;code&gt;php&lt;/code&gt;) tend to have very low answer rates (20%-30%). But tags with already high answer rates like &lt;code&gt;regex&lt;/code&gt; don&amp;rsquo;t get much higher or much lower at a given pair.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;There&amp;rsquo;s a lot more than can be done looking at question tags on Stack Overflow. I was surprised to see that all types of programming languages have quick answer times and a high probability of receiving an acceptable answer! I&amp;rsquo;ll definitely keep an eye on the SO archives as they are released, and I&amp;rsquo;m excited to see how trends change in the future.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can view the R and ggplot2 code used to create the data visualizations in &lt;a href=&#34;http://minimaxir.com/notebooks/stack-overflow-questions/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;. You can also view the images/data used for this post in &lt;a href=&#34;https://github.com/minimaxir/stack-overflow-questions&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;</description></item><item><title>How to Make High Quality Data Visualizations for Websites With R and ggplot2</title><link>https://minimaxir.com/2017/08/ggplot2-web/</link><pubDate>Mon, 14 Aug 2017 09:00:00 -0700</pubDate><guid>https://minimaxir.com/2017/08/ggplot2-web/</guid><description>
&lt;p&gt;If you&amp;rsquo;ve been following my blog, I like to use &lt;a href=&#34;https://cran.r-project.org&#34; target=&#34;_blank&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;http://ggplot2.tidyverse.org/reference/&#34; target=&#34;_blank&#34;&gt;ggplot2&lt;/a&gt; for data visualization. A lot.&lt;/p&gt;
&lt;p&gt;One of my older blog posts, &lt;a href=&#34;http://minimaxir.com/2015/02/ggplot-tutorial/&#34; target=&#34;_blank&#34;&gt;An Introduction on How to Make Beautiful Charts With R and ggplot2&lt;/a&gt;, is still one of my most-trafficked posts years later, and even today I see techniques from that particular post incorporated into modern data visualizations on sites such as &lt;a href=&#34;https://www.reddit.com&#34; target=&#34;_blank&#34;&gt;Reddit&amp;rsquo;s&lt;/a&gt; &lt;a href=&#34;https://www.reddit.com/r/dataisbeautiful/&#34; target=&#34;_blank&#34;&gt;/r/dataisbeautiful&lt;/a&gt; subreddit.&lt;/p&gt;
&lt;p&gt;However, that post is a little outdated. Thanks to a few updates to ggplot2 since then and other advances in data visualization best-practices, making pretty charts for websites/blogs using R and ggplot2 is even more easy, quick, &lt;em&gt;and&lt;/em&gt; fun!&lt;/p&gt;
&lt;h2 id=&#34;quick-introduction-to-ggplot2&#34;&gt;Quick Introduction to ggplot2&lt;/h2&gt;
&lt;p&gt;ggplot2 uses a more concise setup toward creating charts as opposed to the more declarative style of Python&amp;rsquo;s &lt;a href=&#34;https://matplotlib.org&#34; target=&#34;_blank&#34;&gt;matplotlib&lt;/a&gt; and base R. And it also includes a few example datasets for practicing ggplot2 functionality; for example, the &lt;code&gt;mpg&lt;/code&gt; dataset is a &lt;a href=&#34;http://ggplot2.tidyverse.org/reference/mpg.html&#34; target=&#34;_blank&#34;&gt;dataset&lt;/a&gt; of the performance of popular models of cars in 1998 and 2008.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/mpg.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say you want to create a &lt;a href=&#34;https://en.wikipedia.org/wiki/Scatter_plot&#34; target=&#34;_blank&#34;&gt;scatter plot&lt;/a&gt;. Following &lt;a href=&#34;http://ggplot2.tidyverse.org/reference/geom_smooth.html&#34; target=&#34;_blank&#34;&gt;a great example&lt;/a&gt; from the ggplot2 documentation, let&amp;rsquo;s plot the highway mileage of the car vs. the &lt;a href=&#34;https://en.wikipedia.org/wiki/Engine_displacement&#34; target=&#34;_blank&#34;&gt;volume displacement&lt;/a&gt; of the engine. In ggplot2, first you instantiate the chart with the &lt;code&gt;ggplot()&lt;/code&gt; function, specifying the source dataset and the core aesthetics you want to plot, such as x, y, color, and fill. In this case, we set the core aesthetics to x = displacement and y = mileage, and add a &lt;code&gt;geom_point()&lt;/code&gt; layer to make a scatter plot:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- ggplot(mpg, aes(x = displ, y = hwy)) +
geom_point()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/plot1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, there is a negative correlation between the two metrics. I&amp;rsquo;m sure you&amp;rsquo;ve seen plots like these around the internet before. But with only a couple of lines of codes, you can make them look more contemporary.&lt;/p&gt;
&lt;p&gt;ggplot2 lets you add a well-designed theme with just one line of code. Relatively new to &lt;code&gt;ggplot2&lt;/code&gt; is &lt;code&gt;theme_minimal()&lt;/code&gt;, which &lt;a href=&#34;http://ggplot2.tidyverse.org/reference/ggtheme.html&#34; target=&#34;_blank&#34;&gt;generates&lt;/a&gt; a muted style similar to &lt;a href=&#34;http://fivethirtyeight.com&#34; target=&#34;_blank&#34;&gt;FiveThirtyEight&lt;/a&gt;&amp;rsquo;s modern data visualizations:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- p +
theme_minimal()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/plot2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But we can still add color. Setting a color aesthetic on a character/categorical variable will set the colors of the corresponding points, making it easy to differentiate at a glance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- ggplot(mpg, aes(x = displ, y = hwy, color=class)) +
geom_point() +
theme_minimal()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/plot3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Adding the color aesthetic certainly makes things much prettier. ggplot2 automatically adds a legend for the colors as well.
However, for this particular visualization, it is difficult to see trends in the points for each class. A easy way around this is to add a &lt;a href=&#34;https://en.wikipedia.org/wiki/Least_squares&#34; target=&#34;_blank&#34;&gt;least squares regression&lt;/a&gt; trendline for each class &lt;a href=&#34;http://ggplot2.tidyverse.org/reference/geom_smooth.html&#34; target=&#34;_blank&#34;&gt;using&lt;/a&gt; &lt;code&gt;geom_smooth()&lt;/code&gt; (which normally adds a smoothed line, but since there isn&amp;rsquo;t a lot of data for each group, we force it to a linear model and do not plot confidence intervals)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- p +
geom_smooth(method = &amp;quot;lm&amp;quot;, se = F)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/plot4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Pretty neat, and now comparative trends are much more apparent! For example, pickups and SUVs have similar efficiency, which makes intuitive sense.&lt;/p&gt;
&lt;p&gt;The chart axes should be labeled (&lt;em&gt;always&lt;/em&gt; label your charts!). All the typical labels, like &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;-axis, and &lt;code&gt;y&lt;/code&gt;-axis can be done with the &lt;code&gt;labs()&lt;/code&gt; function. But relatively new to ggplot2 are the &lt;code&gt;subtitle&lt;/code&gt; and &lt;code&gt;caption&lt;/code&gt; fields, both of do what you expect:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- p +
labs(title=&amp;quot;Efficiency of Popular Models of Cars&amp;quot;,
subtitle=&amp;quot;By Class of Car&amp;quot;,
x=&amp;quot;Engine Displacement (liters)&amp;quot;,
y=&amp;quot;Highway Miles per Gallon&amp;quot;,
caption=&amp;quot;by Max Woolf â€” minimaxir.com&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/plot5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s a pretty good start. Now let&amp;rsquo;s take it to the next level.&lt;/p&gt;
&lt;h2 id=&#34;how-to-save-a-ggplot2-chart-for-web&#34;&gt;How to Save A ggplot2 chart For Web&lt;/h2&gt;
&lt;p&gt;Something surprisingly undiscussed in the field of data visualization is how to &lt;em&gt;save&lt;/em&gt; a chart as a high quality image file. For example, with &lt;a href=&#34;https://products.office.com/en-us/excel&#34; target=&#34;_blank&#34;&gt;Excel&lt;/a&gt; charts, Microsoft &lt;a href=&#34;https://support.office.com/en-us/article/Save-a-chart-as-a-picture-in-Excel-for-Windows-254bbf9a-1ce1-459f-914a-4902e8ca9217&#34; target=&#34;_blank&#34;&gt;officially recommends&lt;/a&gt; to copy the chart, &lt;em&gt;paste it as an image back into Excel&lt;/em&gt;, then save the pasted image, without having any control over image quality and size in the browser (the &lt;em&gt;real&lt;/em&gt; best way to save an Excel/&lt;a href=&#34;https://www.apple.com/numbers/&#34; target=&#34;_blank&#34;&gt;Numbers&lt;/a&gt; chart as an image for a webpage is to copy/paste the chart object into a &lt;a href=&#34;https://products.office.com/en-us/powerpoint&#34; target=&#34;_blank&#34;&gt;PowerPoint&lt;/a&gt;/&lt;a href=&#34;https://www.apple.com/keynote/&#34; target=&#34;_blank&#34;&gt;Keynote&lt;/a&gt; slide, and export &lt;em&gt;the slide&lt;/em&gt; as an image. This also makes it extremely easy to annotate/brand said chart beforehand in PowerPoint/Keynote).&lt;/p&gt;
&lt;p&gt;R IDEs such as &lt;a href=&#34;https://www.rstudio.com&#34; target=&#34;_blank&#34;&gt;RStudio&lt;/a&gt; have a chart-saving UI with the typical size/filetype options. But if you save an image from this UI, the shapes and texts of the resulting image will be heavily aliased (R &lt;a href=&#34;https://danieljhocking.wordpress.com/2013/03/12/high-resolution-figures-in-r/&#34; target=&#34;_blank&#34;&gt;renders images at 72 dpi&lt;/a&gt; by default, which is much lower than that of modern HiDPI/Retina displays).&lt;/p&gt;
&lt;p&gt;The data visualizations used earlier in this post were generated in-line as a part of an &lt;a href=&#34;http://rmarkdown.rstudio.com/r_notebooks.html&#34; target=&#34;_blank&#34;&gt;R Notebook&lt;/a&gt;, but it is surprisingly difficult to extract the generated chart as a separate file. But ggplot2 also has &lt;code&gt;ggsave()&lt;/code&gt;, which saves the image to disk using antialiasing and makes the fonts/shapes in the chart look much better, and assumes a default dpi of 300. Saving charts using &lt;code&gt;ggsave()&lt;/code&gt;, and adjusting the sizes of the text and geoms to compensate for the higher dpi, makes the charts look very presentable. A width of 4 and a height of 3 results in a 1200x900px image, which if posted on a blog with a content width of ~600px (like mine), will render at full resolution on HiDPI/Retina displays, or downsample appropriately otherwise. Due to modern PNG compression, the file size/bandwidth cost for using larger images is minimal.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- ggplot(mpg, aes(x = displ, y = hwy, color=class)) +
geom_smooth(method = &amp;quot;lm&amp;quot;, se=F, size=0.5) +
geom_point(size=0.5) +
theme_minimal(base_size=9) +
labs(title=&amp;quot;Efficiency of Popular Models of Cars&amp;quot;,
subtitle=&amp;quot;By Class of Car&amp;quot;,
x=&amp;quot;Engine Displacement (liters)&amp;quot;,
y=&amp;quot;Highway Miles per Gallon&amp;quot;,
caption=&amp;quot;by Max Woolf â€” minimaxir.com&amp;quot;)
ggsave(&amp;quot;tutorial-0.png&amp;quot;, p, width=4, height=3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/tutorial-0.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Compare to the previous non-ggsave chart, which is more blurry around text/shapes:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/plot5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For posterity, here&amp;rsquo;s the same chart saved at 1200x900px using the RStudio image-saving UI:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/plot-1200-900.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that the antialiasing optimizations assume that you are &lt;em&gt;not&lt;/em&gt; uploading the final chart to a service like &lt;a href=&#34;https://medium.com&#34; target=&#34;_blank&#34;&gt;Medium&lt;/a&gt; or &lt;a href=&#34;https://wordpress.com&#34; target=&#34;_blank&#34;&gt;WordPress.com&lt;/a&gt;, which will compress the images and reduce the quality anyways. But if you are uploading it to Reddit or self-hosting your own blog, it&amp;rsquo;s definitely worth it.&lt;/p&gt;
&lt;h2 id=&#34;fancy-fonts&#34;&gt;Fancy Fonts&lt;/h2&gt;
&lt;p&gt;Changing the chart font is another way to add a personal flair.
Theme functions like &lt;code&gt;theme_minimal()&lt;/code&gt; accept a &lt;code&gt;base_family&lt;/code&gt; parameter. With that, you can specify any font family as the default instead of the base sans-serif. (On Windows, you may need to install the &lt;code&gt;extrafont&lt;/code&gt; package first). Fonts from &lt;a href=&#34;https://fonts.google.com&#34; target=&#34;_blank&#34;&gt;Google Fonts&lt;/a&gt; are free and work easily with ggplot2 once installed. For example, we can use &lt;a href=&#34;https://fonts.google.com/specimen/Roboto&#34; target=&#34;_blank&#34;&gt;Roboto&lt;/a&gt;, Google&amp;rsquo;s modern font which has also been getting a lot of usage on &lt;a href=&#34;https://stackoverflow.com&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;&amp;rsquo;s great ggplot2 &lt;a href=&#34;https://stackoverflow.blog/2017/06/15/developers-use-spaces-make-money-use-tabs/&#34; target=&#34;_blank&#34;&gt;data visualizations&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- p +
theme_minimal(base_size=9, base_family=&amp;quot;Roboto&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/tutorial-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A general text design guideline is to use fonts of different weights/widths for different hierarchies of content. In this case, we can use a bolder condensed font for the title, and deemphasize the subtitle and caption using lighter colors, all done using the &lt;code&gt;theme()&lt;/code&gt; &lt;a href=&#34;http://ggplot2.tidyverse.org/reference/theme.html&#34; target=&#34;_blank&#34;&gt;function&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- p +
theme(plot.subtitle = element_text(color=&amp;quot;#666666&amp;quot;),
plot.title = element_text(family=&amp;quot;Roboto Condensed Bold&amp;quot;),
plot.caption = element_text(color=&amp;quot;#AAAAAA&amp;quot;, size=6))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/tutorial-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s worth nothing that data visualizations posted on websites should be easily &lt;em&gt;legible&lt;/em&gt; for mobile-device users as well, hence the intentional use of larger fonts relative to charts typically produced in the desktop-oriented Excel.&lt;/p&gt;
&lt;p&gt;Additionally, all theming options can be set as a session default at the beginning of a script using &lt;code&gt;theme_set()&lt;/code&gt;, saving even more time instead of having to recreate the theme for each chart.&lt;/p&gt;
&lt;h2 id=&#34;the-ggplot2-colors&#34;&gt;The &amp;ldquo;ggplot2 colors&amp;rdquo;&lt;/h2&gt;
&lt;p&gt;The &amp;ldquo;ggplot2 colors&amp;rdquo; for categorical variables are infamous for being the primary indicator of a chart being made with ggplot2. But there is a science to it; ggplot2 by default selects colors using the &lt;code&gt;scale_color_hue()&lt;/code&gt; &lt;a href=&#34;http://ggplot2.tidyverse.org/reference/scale_hue.html&#34; target=&#34;_blank&#34;&gt;function&lt;/a&gt;, which selects colors in the HSL space by changing the hue [H] between 0 and 360, keeping saturation [S] and lightness [L] constant. As a result, ggplot2 selects the most &lt;em&gt;distinct&lt;/em&gt; colors possible while keeping lightness constant. For example, if you have 2 different categories, ggplot2 chooses the colors with h = 0 and h = 180; if 3 colors, h = 0, h = 120, h = 240, etc.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s smart, but does make a given chart lose distinctness when many other ggplot2 charts use the same selection methodology. A quick way to take advantage of this hue dispersion while still making the colors unique is to change the lightness; by default, &lt;code&gt;l = 65&lt;/code&gt;, but setting it slightly lower will make the charts look more professional/&lt;a href=&#34;https://www.bloomberg.com&#34; target=&#34;_blank&#34;&gt;Bloomberg&lt;/a&gt;-esque.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_color &amp;lt;- p +
scale_color_hue(l = 40)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/tutorial-4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;rcolorbrewer&#34;&gt;RColorBrewer&lt;/h2&gt;
&lt;p&gt;Another coloring option for ggplot2 charts are the &lt;a href=&#34;http://colorbrewer2.org/#type=sequential&amp;amp;scheme=BuGn&amp;amp;n=3&#34; target=&#34;_blank&#34;&gt;ColorBrewer&lt;/a&gt; palettes implemented with the &lt;code&gt;RColorBrewer&lt;/code&gt; package, which are supported natively in ggplot2 with functions such as &lt;code&gt;scale_color_brewer()&lt;/code&gt;. The sequential palettes like &amp;ldquo;Blues&amp;rdquo; and &amp;ldquo;Greens&amp;rdquo; do what the name implies:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_color &amp;lt;- p +
scale_color_brewer(palette=&amp;quot;Blues&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/tutorial-5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A famous diverging palette for visualizations on /r/dataisbeautiful is the &amp;ldquo;Spectral&amp;rdquo; palette, which is a lighter rainbow (recommended for dark backgrounds)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/tutorial-6.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, while the charts look pretty, it&amp;rsquo;s difficult to tell the categories apart. The qualitative palettes fix this problem, and have more distinct possibilities than the &lt;code&gt;scale_color_hue()&lt;/code&gt; approach mentioned earlier.&lt;/p&gt;
&lt;p&gt;Here are 3 examples of qualitative palettes, &amp;ldquo;Set1&amp;rdquo;, &amp;ldquo;Set2&amp;rdquo;, and &amp;ldquo;Set3,&amp;rdquo; whichever fit your preference.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/tutorial-7.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/tutorial-8.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/tutorial-9.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;viridis-and-accessibility&#34;&gt;Viridis and Accessibility&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s mix up the visualization a bit. A rarely-used-but-very-useful ggplot2 geom is &lt;code&gt;geom2d_bin()&lt;/code&gt;, which counts the number of points in a given 2d spatial area:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p &amp;lt;- ggplot(mpg, aes(x = displ, y = hwy)) +
geom_bin2d(bins=10) +
[...theming options...]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/tutorial-tile.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that the largest number of points are centered around (2,30). However, the default ggplot2 color palette for continuous variables is &lt;em&gt;boring&lt;/em&gt;. Yes, we can use the RColorBrewer sequential palettes above, but as noted, they aren&amp;rsquo;t perceptually distinct, and could cause issues for readers who are colorblind.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html&#34; target=&#34;_blank&#34;&gt;viridis R package&lt;/a&gt; provides a set of 4 high-contrast palettes which are very colorblind friendly, and works easily with ggplot2 by extending a &lt;code&gt;scale_fill_viridis()/scale_color_viridis()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;The default &amp;ldquo;viridis&amp;rdquo; palette has been increasingly popular on the web lately:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p_color &amp;lt;- p +
scale_fill_viridis(option=&amp;quot;viridis&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/tutorial-10.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;magma&amp;rdquo; and &amp;ldquo;inferno&amp;rdquo; are similar, and give the data visualization a fiery edge:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/tutorial-11.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/tutorial-12.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lastly, &amp;ldquo;plasma&amp;rdquo; is a mix between the 3 palettes above:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ggplot2-web/tutorial-13.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;FiveThirtyEight actually uses ggplot2 for their data journalism workflow &lt;a href=&#34;https://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/FiveThirtyEights-data-journalism-workflow-with-R?ocid=player&#34; target=&#34;_blank&#34;&gt;in an interesting way&lt;/a&gt;; they render the base chart using ggplot2, but export it as as a SVG/PDF vector file which can scale to any size, and then the design team annotates/customizes the data visualization in &lt;a href=&#34;http://www.adobe.com/products/illustrator.html&#34; target=&#34;_blank&#34;&gt;Adobe Illustrator&lt;/a&gt; before exporting it as a static PNG for the article (in general, I recommend using an external image editor to add text annotations to a data visualization because doing it manually in ggplot2 is inefficient).&lt;/p&gt;
&lt;p&gt;For general use cases, ggplot2 has very strong defaults for beautiful data visualizations. And certainly there is a lot &lt;em&gt;more&lt;/em&gt; you can do to make a visualization beautiful than what&amp;rsquo;s listed in this post, such as using facets and tweaking parameters of geoms for further distinction, but those are more specific to a given data visualization. In general, it takes little additional effort to make something &lt;em&gt;unique&lt;/em&gt; with ggplot2, and the effort is well worth it. And prettier charts are more persuasive, which is a good return-on-investment.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can view the R and ggplot2 code used to create the data visualizations in &lt;a href=&#34;http://minimaxir.com/notebooks/ggplot2-web/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;. You can also view the images/data used for this post in &lt;a href=&#34;https://github.com/minimaxir/ggplot2-web&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>