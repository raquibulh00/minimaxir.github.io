<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Big Data on Max Woolf&#39;s Blog</title><link>https://minimaxir.com/categories/big-data/</link><description>Recent content in Big Data on Max Woolf&#39;s Blog</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright Max Woolf &amp;copy; {year}</copyright><lastBuildDate>Mon, 10 Sep 2018 09:15:00 -0700</lastBuildDate><atom:link href="https://minimaxir.com/categories/big-data/index.xml" rel="self" type="application/rss+xml"/><item><title>Problems with Predicting Post Performance on Reddit and Other Link Aggregators</title><link>https://minimaxir.com/2018/09/modeling-link-aggregators/</link><pubDate>Mon, 10 Sep 2018 09:15:00 -0700</pubDate><guid>https://minimaxir.com/2018/09/modeling-link-aggregators/</guid><description>
&lt;p&gt;&lt;a href=&#34;https://www.reddit.com&#34; target=&#34;_blank&#34;&gt;Reddit&lt;/a&gt;, &amp;ldquo;the front page of the internet&amp;rdquo; is a link aggregator where anyone can submit links to cool happenings. Over the years, Reddit has expanded from just being a link aggregator, to allowing image and videos, and as of recently, hosting images and videos itself.&lt;/p&gt;
&lt;p&gt;Reddit is broken down into subreddits, where each subreddit represents each own community around a particular interest, like &lt;a href=&#34;https://www.reddit.com/r/aww&#34; target=&#34;_blank&#34;&gt;/r/aww&lt;/a&gt; for pet photos and &lt;a href=&#34;https://www.reddit.com/r/politics/&#34; target=&#34;_blank&#34;&gt;/r/politics&lt;/a&gt; for U.S. politics. The posts on each subreddit are ranked by some function of both time elapsed since the submission was made, and the &lt;em&gt;score&lt;/em&gt; of the submission as determined by upvotes and downvotes from other users.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_aww.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s also an intrinsic pride in having something you&amp;rsquo;re responsible for providing to the community get lots of upvotes (the submitter also earns karma based on received upvotes, although karma is meaningless and doesn&amp;rsquo;t provide any user benefits). But the reality is that even on the largest subreddits, submissions with 1 point (the default score for new submissions) are the most prominent, with some subreddits having &lt;em&gt;over half&lt;/em&gt; of their submissions with only 1 point.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_dist_facet.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The exposure from having a submission go viral on Reddit (especially on larger subreddits) can be valuable especially if its your own original content. As a result, there has been a lot of &lt;a href=&#34;https://www.brandwatch.com/blog/how-to-get-on-the-front-page-of-reddit/&#34; target=&#34;_blank&#34;&gt;analysis&lt;/a&gt;/&lt;a href=&#34;https://www.reddit.com/r/starterpacks/comments/8rkfk9/reddit_front_page_starter_pack/&#34; target=&#34;_blank&#34;&gt;stereotypes&lt;/a&gt; on what techniques to do to help your submission make it to the top of the front page. But almost all claims of &amp;ldquo;cracking&amp;rdquo; the Reddit algorithm are &lt;a href=&#34;https://en.wikipedia.org/wiki/Post_hoc_ergo_propter_hoc&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;post hoc&lt;/em&gt; rationalizations&lt;/a&gt;, attributing success to things like submission timing and title verbiage of a single submission after the fact. The nature of algorithmic feeds inherently leads to a &lt;a href=&#34;https://en.wikipedia.org/wiki/Survivorship_bias&#34; target=&#34;_blank&#34;&gt;survivorship bias&lt;/a&gt;: although users may recognize certain types of posts that appear on the front page, there are many more which follow the same patterns but fail, which makes modeling a successful post very tricky.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve touched on analyzing Reddit post performance &lt;a href=&#34;https://minimaxir.com/2017/06/reddit-deep-learning/&#34; target=&#34;_blank&#34;&gt;before&lt;/a&gt;, but let&amp;rsquo;s give it another look and see if we can drill down on why Reddit posts do and do not do well.&lt;/p&gt;
&lt;h2 id=&#34;submission-timing&#34;&gt;Submission Timing&lt;/h2&gt;
&lt;p&gt;As with many US-based websites, the majority of Reddit users are most active during work hours (9 AM — 5 PM Eastern time weekdays). Most subreddits have submission patterns which fit accordingly.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_subreddit_prop.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But what&amp;rsquo;s interesting are the subreddits which &lt;em&gt;deviate&lt;/em&gt; from that standard. Gaming subreddits (&lt;a href=&#34;https://www.reddit.com/r/DestinyTheGame&#34; target=&#34;_blank&#34;&gt;/r/DestinyTheGame&lt;/a&gt;, &lt;a href=&#34;https://www.reddit.com/r/Overwatch&#34; target=&#34;_blank&#34;&gt;/r/Overwatch&lt;/a&gt;) have short activity after a Tuesday game update/patch, game &lt;em&gt;communication&lt;/em&gt; subreddits (&lt;a href=&#34;https://www.reddit.com/r/Fireteams&#34; target=&#34;_blank&#34;&gt;/r/Fireteams&lt;/a&gt;, &lt;a href=&#34;https://www.reddit.com/r/RocketLeagueExchange&#34; target=&#34;_blank&#34;&gt;/r/RocketLeagueExchange&lt;/a&gt;) are more active &lt;em&gt;outside&lt;/em&gt; of work hours as they assume you are playing the game at the time, and Not-Safe-For-Work subreddits (/r/dirtykikpals, /r/gonewild) are incidentally less active during work hours and more active late-night than other subreddits.&lt;/p&gt;
&lt;p&gt;Whenever you make a submission to Reddit, the submission appears in the subreddit&amp;rsquo;s &lt;code&gt;/new&lt;/code&gt; queue of the most recent submissions, where hopefully kind souls will find your submission and upvote it if it&amp;rsquo;s good.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_new.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, if it falls off the first page of the &lt;code&gt;/new&lt;/code&gt; queue, your submission might be as good as dead. As a result, there&amp;rsquo;s an element of game theory to timing your submission if you want it to not become another 1-point submission. Is it better to submit during peak hours when more users may see the submission before it falls off of &lt;code&gt;/new&lt;/code&gt;? Is it better to submit &lt;em&gt;before&lt;/em&gt; peak usage since there will be less competition, then continue the momentum once it hits the front page?&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a look at the median post performance at each given time slot for top subreddits:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_subreddit_hr_doy.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As the earlier distribution chart implied, the median score is around 1-2 for most subreddits, and that&amp;rsquo;s consistent across all time slots. Some subreddits with higher medians like /r/me_irl do appear to have a &lt;em&gt;slight&lt;/em&gt; benefit when posting before peak activity. When focusing on subreddits with high overall median scores, the difference is more explicit.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_subreddit_highmedian.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Subreddits like /r/PrequelMemes and /r/The_Donald &lt;em&gt;definitely&lt;/em&gt; have better performance on average when made before peak activity! Posting before peak usage &lt;em&gt;does&lt;/em&gt; appear to be a viable strategy, however for the majority of subreddits it doesn&amp;rsquo;t make much of a difference.&lt;/p&gt;
&lt;h2 id=&#34;submission-titles&#34;&gt;Submission Titles&lt;/h2&gt;
&lt;p&gt;Each Reddit subreddit has their own vocabulary and topics of discussion. Let&amp;rsquo;s break down text by subreddit by looking at the 75th percentile for score on posts containing a given two-word phrase:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_subreddit_topbigrams.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The one trend consistent across all subreddits is the effectiveness of first-person pronouns (&lt;em&gt;I/my&lt;/em&gt;) and original content (&lt;em&gt;fan art&lt;/em&gt;). Other than that, the vocabulary and sentiment for successful posts is very specific to the subreddit and culture is represents; no universal guaranteed-success memes.&lt;/p&gt;
&lt;h2 id=&#34;can-deep-learning-predict-post-performance&#34;&gt;Can Deep Learning Predict Post Performance?&lt;/h2&gt;
&lt;p&gt;Some might think &amp;ldquo;oh hey, this is an arbitrary statistical problem, you can just build an AI to solve it!&amp;rdquo; So, for the sake of argument, I did.&lt;/p&gt;
&lt;p&gt;Instead of using Reddit data for building a deep learning model, we&amp;rsquo;ll use data from &lt;a href=&#34;https://news.ycombinator.com&#34; target=&#34;_blank&#34;&gt;Hacker News&lt;/a&gt;, another link aggregator similar to Reddit with a strong focus on technology and startup entrepreneurship. The distribution of scores on posts, submission timings, upvoting, and front page ranking systems are all the same as on Reddit.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/hn.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The titles on Hacker News submissions are also shorter (80 characters max vs. Reddit&amp;rsquo;s 300 character max) and in concise English (no memes/shitposts allowed), which should help the model learn the title syntax and identify high-impact keywords easier. Like Reddit, the score data is super-skewed with most HN submissions at 1-2 points, and typical model training will quickly converge but try to predict that &lt;em&gt;every&lt;/em&gt; submission has a score of 1, which isn&amp;rsquo;t helpful!&lt;/p&gt;
&lt;p&gt;By constructing a model employing &lt;em&gt;many&lt;/em&gt; deep learning tricks with &lt;a href=&#34;https://keras.io&#34; target=&#34;_blank&#34;&gt;Keras&lt;/a&gt;/&lt;a href=&#34;https://www.tensorflow.org&#34; target=&#34;_blank&#34;&gt;TensorFlow&lt;/a&gt; to prevent model cheating and training on &lt;em&gt;hundreds of thousands&lt;/em&gt; of HN submissions (using post title, day-of-week, hour, and link domain like &lt;code&gt;github.com&lt;/code&gt; as model features), the model does converge and finds some signal among the noise (training R&lt;sup&gt;2&lt;/sup&gt; ~ 0.55 when trained for 50 epochs). However, it fails to offer any valuable predictions on new, unseen posts (test R&lt;sup&gt;2&lt;/sup&gt; &lt;em&gt;&amp;lt; 0.00&lt;/em&gt;) because it falls into the same exact human biases regarding titles: it saw submissions with titles that did very well during training, but can&amp;rsquo;t isolate the random chance why X and Y submissions are similar but X goes viral while Y does not.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/hn_test.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve made the Keras/TensorFlow model training code available in &lt;a href=&#34;https://www.kaggle.com/minimaxir/hacker-news-submission-score-predictor/notebook&#34; target=&#34;_blank&#34;&gt;this Kaggle Notebook&lt;/a&gt; if you want to fork it and try to improve the model.&lt;/p&gt;
&lt;h2 id=&#34;other-potential-modeling-factors&#34;&gt;Other Potential Modeling Factors&lt;/h2&gt;
&lt;p&gt;The deep learning model above makes optimistic assumptions about the underlying data, including that each post behaves independently, and the included features are the sole features which determine the score. These assumptions are questionable.&lt;/p&gt;
&lt;p&gt;The simple model forgoes the content of the submission itself, which is hard to retrieve for hundreds of thousands of data points. On Hacker News that&amp;rsquo;s mostly OK since most submissions are links/articles which accurately correlate to the content, although occasionally there are idiosyncratic short titles which do the opposite. On Reddit, obviously looking at content is necessary for image/video-oriented subreddits, which is hard to gather and analyze at scale.&lt;/p&gt;
&lt;p&gt;A very important concept of post performance is &lt;em&gt;momentum&lt;/em&gt;. A post having a high score is a positive signal in itself, which begets more votes (a famous Reddit problem is brigading from /r/all which can cause submission scores to skyrocket). If the front page of a subreddit has a large number of high-performing posts, they might also suppress posts coming out of the &lt;code&gt;/new&lt;/code&gt; queue because the score threshold is much higher. A simple model may not be able to capture these impacts; the model would need to incorporate the &lt;em&gt;state of the front page&lt;/em&gt; at the time of posting.&lt;/p&gt;
&lt;p&gt;Some also try to manipulate upvotes. Reddit became famous for adding the rule &amp;ldquo;asking for upvotes is a violation of intergalactic law&amp;rdquo; to their &lt;a href=&#34;https://www.reddithelp.com/en/categories/rules-reporting/account-and-community-restrictions/what-constitutes-vote-cheating-or&#34; target=&#34;_blank&#34;&gt;Content Policy&lt;/a&gt;, although some subreddits do it anyway &lt;a href=&#34;https://www.reddit.com/r/TheoryOfReddit/comments/5qqrod/for_years_reddit_told_us_that_saying_upvote_this/&#34; target=&#34;_blank&#34;&gt;without consequence&lt;/a&gt;. On Reddit, obvious spam posts can be downvoted to immediately counteract illicit upvotes. Hacker News has a &lt;a href=&#34;https://news.ycombinator.com/newsfaq.html&#34; target=&#34;_blank&#34;&gt;similar don&amp;rsquo;t-upvote rule&lt;/a&gt;, although there aren&amp;rsquo;t downvotes, just a flagging mechanism which quickly neutralizes spam/misleading posts. In general, there&amp;rsquo;s no &lt;em&gt;legitimate&lt;/em&gt; reason to highlight your own submission immediately after its posted (except for Reddit&amp;rsquo;s AMAs). Fortunately, gaming the system is less impactful on Reddit and Hacker News due to their sheer size and countermeasures, but it&amp;rsquo;s a good example of potential user behavior that makes modeling post performance difficult, and hopefully link aggregators of the future aren&amp;rsquo;t susceptible to such shenanigans.&lt;/p&gt;
&lt;h2 id=&#34;do-we-really-to-predict-post-score&#34;&gt;Do We Really to Predict Post Score?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s say you are submitting original content to Reddit or your own tech project to Hacker News. More points means a higher ranking means more exposure for your link, right? Not exactly. As noted from Reddit/HN screenshots above, the scores of popular submissions are all over the place ranking-wise, having been affected by age penalties.&lt;/p&gt;
&lt;p&gt;In practical terms, from my own purely anecdotal experience, submissions at a top ranking receive &lt;em&gt;substantially&lt;/em&gt; more clickthroughs despite being spatially close on the page to others.&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&amp;hellip;and now traffic at #3.&lt;br&gt;&lt;br&gt;Placement is absurdly important for search engines/social media sites. Difference between #1 and #3 is dramatic. &lt;a href=&#34;https://t.co/nGjWJBx6dU&#34;&gt;pic.twitter.com/nGjWJBx6dU&lt;/a&gt;&lt;/p&gt;&amp;mdash; Max Woolf (@minimaxir) &lt;a href=&#34;https://twitter.com/minimaxir/status/877219784907149316?ref_src=twsrc%5Etfw&#34;&gt;June 20, 2017&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://twitter.com/minimaxir/status/877219784907149316&#34; target=&#34;_blank&#34;&gt;that case&lt;/a&gt;, falling from #1 to #3 &lt;em&gt;immediately halved&lt;/em&gt; the referral traffic coming from Hacker News.&lt;/p&gt;
&lt;p&gt;Therefore, an ideal link aggregator predictive model to maximize clicks should try to predict the &lt;em&gt;rank&lt;/em&gt; of a submission (max rank, average rank over &lt;em&gt;n&lt;/em&gt; period, etc.), not necessarily the score it receives. You could theoretically create a model by making a snapshot of a Reddit subreddit/front page of Hacker News every minute or so which includes the post position at the time of the snapshot. As mentioned earlier, the snapshots can also be used as a model feature to identify whether the front page is active or stale. Unfortunately, snapshots can&amp;rsquo;t be retrieved retroactively, and both storing, processing, and analyzing snapshots at scale is a difficult and &lt;em&gt;expensive&lt;/em&gt; feat of data engineering.&lt;/p&gt;
&lt;p&gt;Presumably Reddit&amp;rsquo;s data scientists would be incorporating submission position as a part of their data analytics and modeling, but after inspecting what&amp;rsquo;s sent to Reddit&amp;rsquo;s servers when you perform an action like upvoting, I wasn&amp;rsquo;t able to find a sent position value when upvoting from the feed: only the post score and post upvote percentage at the time of the action were sent.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/chrome.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example, I upvoted the &lt;code&gt;Fact are facts&lt;/code&gt; submission at position #5: we&amp;rsquo;d expect a value between &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt; be sent with the post metadata within the analytics payload, but that&amp;rsquo;s not the case.&lt;/p&gt;
&lt;p&gt;Optimizing ranking instead of a tangible metric or classification accuracy is a relatively underdiscussed field of modern data science (besides &lt;a href=&#34;https://en.wikipedia.org/wiki/Search_engine_optimization&#34; target=&#34;_blank&#34;&gt;SEO&lt;/a&gt; for getting the top spot on a Google search), and it would be interesting to dive deeper into it for other applications.&lt;/p&gt;
&lt;h2 id=&#34;in-the-future&#34;&gt;In the future&lt;/h2&gt;
&lt;p&gt;The moral of this post is that you should not take it personally if a submission fails to hit the front page. It doesn&amp;rsquo;t necessarily mean it&amp;rsquo;s bad. Conversely, if a post does well, don’t assume that similar posts will do just as well. There&amp;rsquo;s a lot of quality content that falls through the cracks due to dumb luck. Fortunately, both Reddit and Hacker News allow reposts, which helps alleviate this particular problem.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s still a lot that can be done to more deterministically predict the behavior of these algorithmic feeds. There&amp;rsquo;s also room to help make these link aggregators more &lt;em&gt;fair&lt;/em&gt;. Unfortunately, there&amp;rsquo;s even more undiscovered ways to game these algorithms, and we&amp;rsquo;ll see how things play out.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can view the BigQuery queries used to get the Reddit and Hacker News data, plus the R and ggplot2 used to create the data visualizations, in &lt;a href=&#34;http://minimaxir.com/notebooks/modeling-link-aggregators/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;. You can also view the images/code used for this post in &lt;a href=&#34;https://github.com/minimaxir/modeling-link-aggregators&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Analyzing IMDb Data The Intended Way, with R and ggplot2</title><link>https://minimaxir.com/2018/07/imdb-data-analysis/</link><pubDate>Mon, 16 Jul 2018 09:45:00 +0000</pubDate><guid>https://minimaxir.com/2018/07/imdb-data-analysis/</guid><description>
&lt;p&gt;&lt;a href=&#34;https://www.imdb.com&#34; target=&#34;_blank&#34;&gt;IMDb&lt;/a&gt;, the Internet Movie Database, has been a popular source for data analysis and visualizations over the years. The combination of user ratings for movies and detailed movie metadata have always been fun to &lt;a href=&#34;http://minimaxir.com/2016/01/movie-revenue-ratings/&#34; target=&#34;_blank&#34;&gt;play with&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/movie-revenue-ratings/box-office-rating-5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are a number of tools to help get IMDb data, such as &lt;a href=&#34;https://github.com/alberanid/imdbpy&#34; target=&#34;_blank&#34;&gt;IMDbPY&lt;/a&gt;, which makes it easy to programmatically scrape IMDb by pretending it&amp;rsquo;s a website user and extracting the relevant data from the page&amp;rsquo;s HTML output. While it &lt;em&gt;works&lt;/em&gt;, web scraping public data is a gray area in terms of legality; many large websites have a Terms of Service which forbids scraping, and can potentially send a DMCA take-down notice to websites redistributing scraped data.&lt;/p&gt;
&lt;p&gt;IMDb has &lt;a href=&#34;https://help.imdb.com/article/imdb/general-information/can-i-use-imdb-data-in-my-software/G5JTRESSHJBBHTGX#&#34; target=&#34;_blank&#34;&gt;data licensing terms&lt;/a&gt; which forbid scraping and require an attribution in the form of a &lt;strong&gt;Information courtesy of IMDb (&lt;a href=&#34;http://www.imdb.com&#34; target=&#34;_blank&#34;&gt;http://www.imdb.com&lt;/a&gt;). Used with permission.&lt;/strong&gt; statement, and has also &lt;a href=&#34;https://www.kaggle.com/tmdb/tmdb-movie-metadata/home&#34; target=&#34;_blank&#34;&gt;DMCAed a Kaggle IMDb dataset&lt;/a&gt; to hone the point.&lt;/p&gt;
&lt;p&gt;However, there is good news! IMDb publishes an &lt;a href=&#34;https://www.imdb.com/interfaces/&#34; target=&#34;_blank&#34;&gt;official dataset&lt;/a&gt; for casual data analysis! And it&amp;rsquo;s now very accessible, just choose a dataset and download (now with no hoops to jump through), and the files are in the standard &lt;a href=&#34;https://en.wikipedia.org/wiki/Tab-separated_values&#34; target=&#34;_blank&#34;&gt;TSV format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/datasets.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The uncompressed files are pretty large; not &amp;ldquo;big data&amp;rdquo; large (it fits into computer memory), but Excel will explode if you try to open them in it. You have to play with the data &lt;em&gt;smartly&lt;/em&gt;, and both &lt;a href=&#34;https://www.r-project.org&#34; target=&#34;_blank&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/index.html&#34; target=&#34;_blank&#34;&gt;ggplot2&lt;/a&gt; have neat tricks to do just that.&lt;/p&gt;
&lt;h2 id=&#34;first-steps&#34;&gt;First Steps&lt;/h2&gt;
&lt;p&gt;R is a popular programming language for statistical analysis. One of the most popular series of external packages is the &lt;code&gt;tidyverse&lt;/code&gt; package, which automatically imports the &lt;code&gt;ggplot2&lt;/code&gt; data visualization library and other useful packages which we&amp;rsquo;ll get to one-by-one. We&amp;rsquo;ll also use &lt;code&gt;scales&lt;/code&gt; which we&amp;rsquo;ll use later for prettier number formatting. First we&amp;rsquo;ll load these packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
library(scales)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we can load a TSV downloaded from IMDb using the &lt;code&gt;read_tsv&lt;/code&gt; function from &lt;code&gt;readr&lt;/code&gt; (a tidyverse package), which does what the name implies, at a much faster speed than base R (+ a couple other parameters to handle data encoding). Let&amp;rsquo;s start with the &lt;code&gt;ratings&lt;/code&gt; file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;df_ratings &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; read_tsv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;title.ratings.tsv&amp;#39;&lt;/span&gt;, na &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;\\N&amp;#34;&lt;/span&gt;, quote &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can preview what&amp;rsquo;s in the loaded data using &lt;code&gt;dplyr&lt;/code&gt; (a tidyverse package), which is what we&amp;rsquo;ll be using to manipulate data for this analysis. dplyr allows you to pipe commands, making it easy to create a sequence of manipulation commands. For now, we&amp;rsquo;ll use &lt;code&gt;head()&lt;/code&gt;, which displays the top few rows of the data frame.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_ratings %&amp;gt;% head()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/ratings.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each of the &lt;strong&gt;873k rows&lt;/strong&gt; corresponds to a single movie, an ID for the movie, its average rating (from 1 to 10), and the number of votes which contribute to that average. Since we have two numeric variables, why not test out ggplot2 by creating a scatterplot mapping them? ggplot2 takes in a data frame and names of columns as aesthetics, then you specify what type of shape to plot (a &amp;ldquo;geom&amp;rdquo;). Passing the plot to &lt;code&gt;ggsave&lt;/code&gt; saves it as a standalone, high-quality data visualization.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot &amp;lt;- ggplot(df_ratings, aes(x = numVotes, y = averageRating)) +
geom_point()
ggsave(&amp;quot;imdb-0.png&amp;quot;, plot, width = 4, height = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-0.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here is nearly &lt;em&gt;1 million&lt;/em&gt; points on a single chart; definitely don&amp;rsquo;t try to do that in Excel! However, it&amp;rsquo;s not a &lt;em&gt;useful&lt;/em&gt; chart since all the points are opaque and we&amp;rsquo;re not sure what the spatial density of points is. One approach to fix this issue is to create a heat map of points, which ggplot can do natively with &lt;code&gt;geom_bin2d&lt;/code&gt;. We can color the heat map with the &lt;a href=&#34;https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html&#34; target=&#34;_blank&#34;&gt;viridis&lt;/a&gt; colorblind-friendly palettes &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/scale_viridis.html&#34; target=&#34;_blank&#34;&gt;just introduced&lt;/a&gt; into ggplot2. We should also tweak the axes; the x-axis should be scaled logarithmically with &lt;code&gt;scale_x_log10&lt;/code&gt; since there are many movies with high numbers of votes and we can format those numbers with the &lt;code&gt;comma&lt;/code&gt; function from the &lt;code&gt;scales&lt;/code&gt; package (we can format the scale with &lt;code&gt;comma&lt;/code&gt; too). For the y-axis, we can add explicit number breaks for each rating; R can do this neatly by setting the breaks to &lt;code&gt;1:10&lt;/code&gt;. Putting it all together:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot &amp;lt;- ggplot(df_ratings, aes(x = numVotes, y = averageRating)) +
geom_bin2d() +
scale_x_log10(labels = comma) +
scale_y_continuous(breaks = 1:10) +
scale_fill_viridis_c(labels = comma)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not bad, although it unfortunately confirms that IMDb follows a &lt;a href=&#34;https://tvtropes.org/pmwiki/pmwiki.php/Main/FourPointScale&#34; target=&#34;_blank&#34;&gt;Four Point Scale&lt;/a&gt; where average ratings tend to fall between 6 — 9.&lt;/p&gt;
&lt;h2 id=&#34;mapping-movies-to-ratings&#34;&gt;Mapping Movies to Ratings&lt;/h2&gt;
&lt;p&gt;You may be asking &amp;ldquo;which ratings correspond to which movies?&amp;rdquo; That&amp;rsquo;s what the &lt;code&gt;tconst&lt;/code&gt; field is for. But first, let&amp;rsquo;s load the title data from &lt;code&gt;title.basics.tsv&lt;/code&gt; into &lt;code&gt;df_basics&lt;/code&gt; and take a look as before.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_basics &amp;lt;- read_tsv(&#39;title.basics.tsv&#39;, na = &amp;quot;\\N&amp;quot;, quote = &#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/basics1.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/basics2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have some neat movie metadata. Notably, this table has a &lt;code&gt;tconst&lt;/code&gt; field as well. Therefore, we can &lt;em&gt;join&lt;/em&gt; the two tables together, adding the movie information to the corresponding row in the rating table (in this case, a left join is more appropriate than an inner/full join)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_ratings &amp;lt;- df_ratings %&amp;gt;% left_join(df_basics)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Runtime minutes sounds interesting. Could there be a relationship between the length of a movie and its average rating on IMDb? Let&amp;rsquo;s make a heat map plot again, but with a few tweaks. With the new metadata, we can &lt;code&gt;filter&lt;/code&gt; the table to remove bad points; let&amp;rsquo;s keep movies only (as IMDb data also contains &lt;em&gt;television show data&lt;/em&gt;), with a runtime &amp;lt; 3 hours, and which have received atleast 10 votes by users to remove extraneous movies). X-axis should be tweaked to display the minutes-values in hours. The fill viridis palette can be changed to another one in the family (I personally like &lt;code&gt;inferno&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;More importantly, let&amp;rsquo;s discuss plot theming. If you want a minimalistic theme, add a &lt;code&gt;theme_minimal&lt;/code&gt; to the plot, and you can pass a &lt;code&gt;base_family&lt;/code&gt; to change the default font on the plot and a &lt;code&gt;base_size&lt;/code&gt; to change the font size. The &lt;code&gt;labs&lt;/code&gt; function lets you add labels to the plot (which you should &lt;em&gt;always&lt;/em&gt; do); you have your &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, and &lt;code&gt;y&lt;/code&gt; parameters, but you can also add a &lt;code&gt;subtitle&lt;/code&gt;, a &lt;code&gt;caption&lt;/code&gt; for attribution, and a &lt;code&gt;color&lt;/code&gt;/&lt;code&gt;fill&lt;/code&gt; to name the scale. Putting it all together:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot &amp;lt;- ggplot(df_ratings %&amp;gt;% filter(runtimeMinutes &amp;lt; 180, titleType == &amp;quot;movie&amp;quot;, numVotes &amp;gt;= 10), aes(x = runtimeMinutes, y = averageRating)) +
geom_bin2d() +
scale_x_continuous(breaks = seq(0, 180, 60), labels = 0:3) +
scale_y_continuous(breaks = 0:10) +
scale_fill_viridis_c(option = &amp;quot;inferno&amp;quot;, labels = comma) +
theme_minimal(base_family = &amp;quot;Source Sans Pro&amp;quot;, base_size = 8) +
labs(title = &amp;quot;Relationship between Movie Runtime and Average Mobie Rating&amp;quot;,
subtitle = &amp;quot;Data from IMDb retrieved July 4th, 2018&amp;quot;,
x = &amp;quot;Runtime (Hours)&amp;quot;,
y = &amp;quot;Average User Rating&amp;quot;,
caption = &amp;quot;Max Woolf — minimaxir.com&amp;quot;,
fill = &amp;quot;# Movies&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-2b.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that&amp;rsquo;s pretty nice-looking for only a few lines of code! Albeit unhelpful, as there doesn&amp;rsquo;t appear to be a correlation.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Note: for the rest of this post, the theming/labels code will be omitted for convenience)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;How about movie ratings vs. the year the movie was made? It&amp;rsquo;s a similar plot code-wise to the one above (one perk about &lt;code&gt;ggplot2&lt;/code&gt; is that there&amp;rsquo;s no shame in reusing chart code!), but we can add a &lt;code&gt;geom_smooth&lt;/code&gt;, which adds a nonparametric trendline with confidence bands for the trend; since we have a large amount of data, the bands are very tight. We can also fix the problem of &amp;ldquo;empty&amp;rdquo; bins by setting the color fill scale to logarithmic scaling. And since we&amp;rsquo;re adding a black trendline, let&amp;rsquo;s change the viridis palette to &lt;code&gt;plasma&lt;/code&gt; for better contrast.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot &amp;lt;- ggplot(df_ratings %&amp;gt;% filter(titleType == &amp;quot;movie&amp;quot;, numVotes &amp;gt;= 10), aes(x = startYear, y = averageRating)) +
geom_bin2d() +
geom_smooth(color=&amp;quot;black&amp;quot;) +
scale_x_continuous() +
scale_y_continuous(breaks = 1:10) +
scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;, labels = comma, trans = &#39;log10&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, this trend hasn&amp;rsquo;t changed much either, although the presence of average ratings outside the Four Point Scale has increased over time.&lt;/p&gt;
&lt;h2 id=&#34;mapping-lead-actors-to-movies&#34;&gt;Mapping Lead Actors to Movies&lt;/h2&gt;
&lt;p&gt;Now that we have a handle on working with the IMDb data, let&amp;rsquo;s try playing with the larger datasets. Since they take up a lot of computer memory, we only want to persist data we actually might use. After looking at the schema provided with the official datasets, the only really useful metadata about the actors is their birth year, so let&amp;rsquo;s load that, but only keep both actors/actresses (using the fast &lt;code&gt;str_detect&lt;/code&gt; function from &lt;code&gt;stringr&lt;/code&gt;, another tidyverse package) and the relevant fields.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_actors &amp;lt;- read_tsv(&#39;name.basics.tsv&#39;, na = &amp;quot;\\N&amp;quot;, quote = &#39;&#39;) %&amp;gt;%
filter(str_detect(primaryProfession, &amp;quot;actor|actress&amp;quot;)) %&amp;gt;%
select(nconst, primaryName, birthYear)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/actor.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The principals dataset, the large 1.28GB TSV, is the most interesting. It&amp;rsquo;s an unnested list of the credited persons in each movie, with an &lt;code&gt;ordering&lt;/code&gt; indicating their rank (where &lt;code&gt;1&lt;/code&gt; means first, &lt;code&gt;2&lt;/code&gt; means second, etc.).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/principals.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For this analysis, let&amp;rsquo;s only look at the &lt;strong&gt;lead actors/actresses&lt;/strong&gt;; specifically, for each movie (identified by the &lt;code&gt;tconst&lt;/code&gt; value), filter the dataset to where the &lt;code&gt;ordering&lt;/code&gt; value is the lowest (in this case, the person at rank &lt;code&gt;1&lt;/code&gt; may not necessarily be an actor/actress).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_principals &amp;lt;- read_tsv(&#39;title.principals.tsv&#39;, na = &amp;quot;\\N&amp;quot;, quote = &#39;&#39;) %&amp;gt;%
filter(str_detect(category, &amp;quot;actor|actress&amp;quot;)) %&amp;gt;%
select(tconst, ordering, nconst, category) %&amp;gt;%
group_by(tconst) %&amp;gt;%
filter(ordering == min(ordering))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both datasets have a &lt;code&gt;nconst&lt;/code&gt; field, so let&amp;rsquo;s join them together. And then join &lt;em&gt;that&lt;/em&gt; to the ratings table earlier via &lt;code&gt;tconst&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_principals &amp;lt;- df_principals %&amp;gt;% left_join(df_actors)
df_ratings &amp;lt;- df_ratings %&amp;gt;% left_join(df_principals)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a fully denormalized dataset in &lt;code&gt;df_ratings&lt;/code&gt;. Since we now have the movie release year and the birth year of the lead actor, we can now infer &lt;em&gt;the age of the lead actor at the movie release&lt;/em&gt;. With that goal, filter out the data on the criteria we&amp;rsquo;ve used for earlier data visualizations, plus only keeping rows which have an actor&amp;rsquo;s birth year.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_ratings_movies &amp;lt;- df_ratings %&amp;gt;%
filter(titleType == &amp;quot;movie&amp;quot;, !is.na(birthYear), numVotes &amp;gt;= 10) %&amp;gt;%
mutate(age_lead = startYear - birthYear)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/denorm1.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/denorm2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;plotting-ages&#34;&gt;Plotting Ages&lt;/h2&gt;
&lt;p&gt;Age discrimination in movie casting has been a recurring issue in Hollywood; in fact, in 2017 &lt;a href=&#34;https://www.hollywoodreporter.com/thr-esq/judge-pauses-enforcement-imdb-age-censorship-law-978797&#34; target=&#34;_blank&#34;&gt;a law was signed&lt;/a&gt; to force IMDb to remove an actor&amp;rsquo;s age upon request, which in February 2018 was &lt;a href=&#34;https://www.hollywoodreporter.com/thr-esq/californias-imdb-age-censorship-law-declared-unconstitutional-1086540&#34; target=&#34;_blank&#34;&gt;ruled to be unconstitutional&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Have the ages of movie leads changed over time? For this example, we&amp;rsquo;ll use a &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/geom_ribbon.html&#34; target=&#34;_blank&#34;&gt;ribbon plot&lt;/a&gt; to plot the ranges of ages of movie leads. A simple way to do that is, for each year, calculate the 25th &lt;a href=&#34;https://en.wikipedia.org/wiki/Percentile&#34; target=&#34;_blank&#34;&gt;percentile&lt;/a&gt; of the ages, the 50th percentile (i.e. the median), and the 75th percentile, where the 25th and 75th percentiles are the ribbon bounds and the line represents the median.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_actor_ages &amp;lt;- df_ratings_movies %&amp;gt;%
group_by(startYear) %&amp;gt;%
summarize(low_age = quantile(age_lead, 0.25, na.rm=T),
med_age = quantile(age_lead, 0.50, na.rm=T),
high_age = quantile(age_lead, 0.75, na.rm=T))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting it with ggplot2 is surprisingly simple, although you need to use different y aesthetics for the ribbon and the overlapping line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot &amp;lt;- ggplot(df_actor_ages %&amp;gt;% filter(startYear &amp;gt;= 1920) , aes(x = startYear)) +
geom_ribbon(aes(ymin = low_age, ymax = high_age), alpha = 0.2) +
geom_line(aes(y = med_age))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-8.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Turns out that in the 2000&amp;rsquo;s, the median age of lead actors started to &lt;em&gt;increase&lt;/em&gt;? Both the upper and lower bounds increased too. That doesn&amp;rsquo;t coalesce with the age discrimination complaints.&lt;/p&gt;
&lt;p&gt;Another aspect of these complaints is gender, as female actresses tend to be younger than male actors. Thanks to the magic of ggplot2 and dplyr, separating actors/actresses is relatively simple: add gender (encoded in &lt;code&gt;category&lt;/code&gt;) as a grouping variable, add it as a color/fill aesthetic in ggplot, and set colors appropriately (I recommend the &lt;a href=&#34;http://colorbrewer2.org/&#34; target=&#34;_blank&#34;&gt;ColorBrewer&lt;/a&gt; qualitative palettes for categorical variables).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_actor_ages_lead &amp;lt;- df_ratings_movies %&amp;gt;%
group_by(startYear, category) %&amp;gt;%
summarize(low_age = quantile(age_lead, 0.25, na.rm = T),
med_age = quantile(age_lead, 0.50, na.rm = T),
high_age = quantile(age_lead, 0.75, na.rm = T))
plot &amp;lt;- ggplot(df_actor_ages_lead %&amp;gt;% filter(startYear &amp;gt;= 1920), aes(x = startYear, fill = category, color = category)) +
geom_ribbon(aes(ymin = low_age, ymax = high_age), alpha = 0.2) +
geom_line(aes(y = med_age)) +
scale_fill_brewer(palette = &amp;quot;Set1&amp;quot;) +
scale_color_brewer(palette = &amp;quot;Set1&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-9.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s about a 10-year gap between the ages of male and female leads, and the gap doesn&amp;rsquo;t change overtime. But both start to rise at the same time.&lt;/p&gt;
&lt;p&gt;One possible explanation for this behavior is actor reuse: if Hollywood keeps casting the same actor/actresses, by construction the ages of the leads will start to steadily increase. Let&amp;rsquo;s verify that: with our list of movies and their lead actors, for each lead actor, order all their movies by release year, and add a ranking for the #th time that actor has been a lead actor. This is possible through the use of &lt;code&gt;row_number&lt;/code&gt; in dplyr, and &lt;a href=&#34;https://cran.r-project.org/web/packages/dplyr/vignettes/window-functions.html&#34; target=&#34;_blank&#34;&gt;window functions&lt;/a&gt; like &lt;code&gt;row_number&lt;/code&gt; are data science&amp;rsquo;s most useful secret.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_ratings_movies_nth &amp;lt;- df_ratings_movies %&amp;gt;%
group_by(nconst) %&amp;gt;%
arrange(startYear) %&amp;gt;%
mutate(nth_lead = row_number())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/row_number.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One more ribbon plot later (w/ same code as above + custom y-axis breaks):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-12.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Huh. The median and upper-bound #th time has &lt;em&gt;dropped&lt;/em&gt; over time? Hollywood has been promoting more newcomers as leads? That&amp;rsquo;s not what I expected!&lt;/p&gt;
&lt;p&gt;More work definitely needs to be done in this area. In the meantime, the official IMDb datasets are a lot more robust than I thought they would be! And I only used a fraction of the datasets; the rest tie into TV shows, which are a bit messier. Hopefully you&amp;rsquo;ve seen a good taste of the power of R and ggplot2 for playing with big-but-not-big data!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can view the R and ggplot used to create the data visualizations in &lt;a href=&#34;http://minimaxir.com/notebooks/imdb-data-analysis/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;, which includes many visualizations not used in this post. You can also view the images/code used for this post in &lt;a href=&#34;https://github.com/minimaxir/imdb-data-analysis&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Visualizing One Million NCAA Basketball Shots</title><link>https://minimaxir.com/2018/03/basketball-shots/</link><pubDate>Mon, 19 Mar 2018 09:20:00 -0700</pubDate><guid>https://minimaxir.com/2018/03/basketball-shots/</guid><description>
&lt;p&gt;So &lt;a href=&#34;https://www.ncaa.com/march-madness&#34; target=&#34;_blank&#34;&gt;March Madness&lt;/a&gt; is happing right now. In celebration, &lt;a href=&#34;https://www.google.com&#34; target=&#34;_blank&#34;&gt;Google&lt;/a&gt; uploaded &lt;a href=&#34;https://console.cloud.google.com/launcher/details/ncaa-bb-public/ncaa-basketball&#34; target=&#34;_blank&#34;&gt;massive basketball datasets&lt;/a&gt; from the &lt;a href=&#34;https://www.ncaa.com&#34; target=&#34;_blank&#34;&gt;NCAA&lt;/a&gt; and &lt;a href=&#34;https://www.sportradar.com/&#34; target=&#34;_blank&#34;&gt;Sportradar&lt;/a&gt; to &lt;a href=&#34;https://cloud.google.com/bigquery/&#34; target=&#34;_blank&#34;&gt;BigQuery&lt;/a&gt; for anyone to query and experiment. After learning that the &lt;a href=&#34;https://www.reddit.com/r/bigquery/comments/82nz17/dataset_statistics_for_ncaa_mens_and_womens/&#34; target=&#34;_blank&#34;&gt;dataset had location data&lt;/a&gt; on where basketball shots were made on the court, I played with it and a couple hours later, I created a decent heat map data visualization. The next day, I &lt;a href=&#34;https://www.reddit.com/r/dataisbeautiful/comments/837qnu/heat_map_of_1058383_basketball_shots_from_ncaa/&#34; target=&#34;_blank&#34;&gt;posted it&lt;/a&gt; to Reddit&amp;rsquo;s &lt;a href=&#34;https://www.reddit.com/r/dataisbeautiful&#34; target=&#34;_blank&#34;&gt;/r/dataisbeautiful subreddit&lt;/a&gt; where it earned about &lt;strong&gt;40,000 upvotes&lt;/strong&gt;. (!?)&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s dig a little deeper. Although visualizing basketball shots has been &lt;a href=&#34;http://www.slate.com/blogs/browbeat/2012/03/06/mapping_the_nba_how_geography_can_teach_players_where_to_shoot.html&#34; target=&#34;_blank&#34;&gt;done&lt;/a&gt; &lt;a href=&#34;http://toddwschneider.com/posts/ballr-interactive-nba-shot-charts-with-r-and-shiny/&#34; target=&#34;_blank&#34;&gt;before&lt;/a&gt;, this time we have access to an order of magnitude more public data to do some really cool stuff.&lt;/p&gt;
&lt;h2 id=&#34;full-court&#34;&gt;Full Court&lt;/h2&gt;
&lt;p&gt;The Sportradar play-by-play table on BigQuery &lt;code&gt;mbb_pbp_sr&lt;/code&gt; has more than 1 million NCAA men&amp;rsquo;s basketball shots since the 2013-2014 season, with more being added now during March Madness. Here&amp;rsquo;s a heat map of the locations where those shots were made on the full basketball court:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_attempts_unlog.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can clearly see at a glance that the majority of shots are made right in front of the basket. For 3-point shots, the center and the corners have higher numbers of shot attempts than the other areas. But not much else since the data is so spatially skewed: setting the bin color scale to logarithmic makes trends more apparent and helps things go viral on Reddit.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_attempts.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now there&amp;rsquo;s more going on here: shot behavior is clearly symmetric on each side of the court, and there&amp;rsquo;s a small gap between the 3-point line and where 3-pt shots are typically made, likely to ensure that it it&amp;rsquo;s not accidentally ruled as a 2-pt shot.&lt;/p&gt;
&lt;p&gt;How likely is it to score a shot from a given spot? Are certain spots better than others?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_perc_success.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Surprisingly, shot accuracy is about &lt;em&gt;equal&lt;/em&gt; from anywhere within typical shooting distance, except directly in front of the basket where it&amp;rsquo;s much higher. What is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Expected_value&#34; target=&#34;_blank&#34;&gt;expected value&lt;/a&gt; of a shot at a given position: that is, how many points on average will they earn for their team?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_avg_points.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The average points earned for 3-pt shots is about 1.5x higher than many 2-pt shot locations in the inner court due to the equal accuracy, but locations next to the basket have an even higher expected value. Perhaps the accuracy of shots close to the basket is higher (&amp;gt;1.5x) than 3-pt shots and outweighs the lower point value?&lt;/p&gt;
&lt;p&gt;Since both sides of the court are indeed the same, we can combine the two sides and just plot a half-court instead. (Cross-court shots, which many Redditors &lt;a href=&#34;https://www.reddit.com/r/dataisugly/comments/839rax/basketball_heat_map_shows_an_impressive_number_of/&#34; target=&#34;_blank&#34;&gt;argued&lt;/a&gt; that they invalidated my visualizations above, constitute only &lt;em&gt;0.16%&lt;/em&gt; of the basketball shots in the dataset, so they can be safely removed as outliers).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_attempts_half_log.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are still a few oddities, such as shots being made &lt;em&gt;behind&lt;/em&gt; the basket. Let&amp;rsquo;s drill down a bit.&lt;/p&gt;
&lt;h2 id=&#34;focusing-on-basketball-shot-type&#34;&gt;Focusing on Basketball Shot Type&lt;/h2&gt;
&lt;p&gt;The Sportradar dataset classifies a shot as one of 5 major types: a &lt;strong&gt;jump shot&lt;/strong&gt; where the player jumps-and-throws the basketball, a &lt;strong&gt;layup&lt;/strong&gt; where the player runs down the field toward the basket and throws a one-handed shot, a &lt;strong&gt;dunk&lt;/strong&gt; where the player slams the ball into the basket (looking cool in the process), a &lt;strong&gt;hook shot&lt;/strong&gt; where the player close to the basket throws the ball with a hook motion, and a &lt;strong&gt;tip shot&lt;/strong&gt; where the player intercepts a basket rebound at the tip of the basket and pushes it in.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_prop_attempts.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, the most frequent types of shots are the less flashy, more practical jump shots and layups. But is a certain type of shot &amp;ldquo;better?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_perc.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Layups are safer than jump shots, but dunks are the most accurate of all the types (however, players likely wouldn&amp;rsquo;t attempt a dunk unless they knew it would be successful). The accuracy of layups and other close-to-basket shots is indeed more than 1.5x better than the jump shots of 3-pt shots, which explains the expected value behavior above.&lt;/p&gt;
&lt;p&gt;Plotting the heat maps for each type of shot offers more insight into how they work:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_attempts_half_types_log.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;They&amp;rsquo;re wildly different heat maps which match the shot type descriptions above, but show we&amp;rsquo;ll need to separate data visualizations by type to accurately see trends.&lt;/p&gt;
&lt;h2 id=&#34;impact-of-game-elapsed-time-at-time-of-shot&#34;&gt;Impact of Game Elapsed Time At Time of Shot&lt;/h2&gt;
&lt;p&gt;A NCAA basketball game lasts for 40 minutes total (2 halves of 20 minutes each), with the possibility of overtime. The &lt;a href=&#34;https://bigquery.cloud.google.com/savedquery/4194148158:3359d86507814fb19a5997a770456baa&#34; target=&#34;_blank&#34;&gt;example BigQuery&lt;/a&gt; for the NCAA-provided data compares the percentage of 3-point shots made during the first 35 minutes of the game versus the last 5 minutes: at the end of the game, accuracy was lower by 4 percentage points (31.2% vs. 35.1%). It might be interesting to facet these visualizations by the elapsed time of the game to see if there are any behavioral changes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_prop_type_elapsed.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There isn&amp;rsquo;t much difference between the proportions within a given half, but there is a difference between the first half and the second half, where the second half has fewer jump shots and more aggressive layups and dunks. After looking at shot success percentage:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_perc_success_type_elapsed.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The jump shot accuracy loss at the end of the game with Sportradar data is similar to that of the NCAA data, which is a good sanity check (but it&amp;rsquo;s odd that the accuracy drop only happens in the last 5 minutes and not elsewhere in the 2nd half). Layup accuracy increases in the second half with the number of layups.&lt;/p&gt;
&lt;p&gt;We can also visualize heat maps for each combo of shot type with time elapsed bucket, but given the results above, the changes in behavior over time may not be very perceptible.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_attempts_half_interval_log.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;impact-of-winning-losing-before-shot&#34;&gt;Impact of Winning/Losing Before Shot&lt;/h2&gt;
&lt;p&gt;Another theory worth exploring is determining if there is any difference whether a team is winning or losing when they make their shot (technically, when the delta between the team score and the other team score is positive for winning teams, negative for losing teams, or 0 if tied). Are players more relaxed when they have a lead? Are players more prone to making mistakes when losing?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_prop_type_score.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Layups are the same across all buckets, but for teams that are winning, there are fewer jump shots and &lt;strong&gt;more dunkin&amp;rsquo; action&lt;/strong&gt; (nearly double the dunks!). However, the accuracy chart illustrates an issue:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_perc_success_type_score.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Accuracy for most types of shots is much better for teams that are winning&amp;hellip;which may be the &lt;em&gt;reason&lt;/em&gt; they&amp;rsquo;re winning. More research can be done in this area.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I fully admit I am not a basketball expert. But playing around with this data was a fun way to get a new perspective on how collegiate basketball games work. There&amp;rsquo;s a lot more work that can be done with big basketball data and game strategy; the NCAA-provided data doesn&amp;rsquo;t have location data, but it does have &lt;strong&gt;6x more shots&lt;/strong&gt;, which will be very helpful for further fun in this area.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can view the R code, ggplot2 code, and BigQueries used to create the data visualizations in &lt;a href=&#34;http://minimaxir.com/notebooks/basketball-shots/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;. You can also view the images/code used for this post in &lt;a href=&#34;https://github.com/minimaxir/ncaa-basketball&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Special thanks to Ewen Gallic for his implementation of a &lt;a href=&#34;http://egallic.fr/en/drawing-a-basketball-court-with-r/&#34; target=&#34;_blank&#34;&gt;basketball court in ggplot2&lt;/a&gt;, which saved me a lot of time!&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Playing with 80 Million Amazon Product Review Ratings Using Apache Spark</title><link>https://minimaxir.com/2017/01/amazon-spark/</link><pubDate>Mon, 02 Jan 2017 09:00:00 -0700</pubDate><guid>https://minimaxir.com/2017/01/amazon-spark/</guid><description>
&lt;p&gt;&lt;a href=&#34;https://www.amazon.com&#34; target=&#34;_blank&#34;&gt;Amazon&lt;/a&gt; product reviews and ratings are a very important business. Customers on Amazon often make purchasing decisions based on those reviews, and a single bad review can cause a potential purchaser to reconsider. A couple years ago, I wrote a blog post titled &lt;a href=&#34;http://minimaxir.com/2014/06/reviewing-reviews/&#34; target=&#34;_blank&#34;&gt;A Statistical Analysis of 1.2 Million Amazon Reviews&lt;/a&gt;, which was well-received.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/amazon/amzn-basic-score.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Back then, I was only limited to 1.2M reviews because attempting to process more data caused out-of-memory issues and my R code took &lt;em&gt;hours&lt;/em&gt; to run.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://spark.apache.org&#34; target=&#34;_blank&#34;&gt;Apache Spark&lt;/a&gt;, which makes processing gigantic amounts of data efficient and sensible, has become very popular in the past couple years (for good tutorials on using Spark with Python, I recommend the &lt;a href=&#34;https://courses.edx.org/courses/course-v1:BerkeleyX+CS105x+1T2016/info&#34; target=&#34;_blank&#34;&gt;free&lt;/a&gt; &lt;a href=&#34;https://courses.edx.org/courses/course-v1:BerkeleyX+CS110x+2T2016/info&#34; target=&#34;_blank&#34;&gt;eDX&lt;/a&gt; &lt;a href=&#34;https://courses.edx.org/courses/course-v1:BerkeleyX+CS120x+2T2016/info&#34; target=&#34;_blank&#34;&gt;courses&lt;/a&gt;). Although data scientists often use Spark to process data with distributed cloud computing via &lt;a href=&#34;https://aws.amazon.com/ec2/&#34; target=&#34;_blank&#34;&gt;Amazon EC2&lt;/a&gt; or &lt;a href=&#34;https://azure.microsoft.com/en-us/services/hdinsight/apache-spark/&#34; target=&#34;_blank&#34;&gt;Microsoft Azure&lt;/a&gt;, Spark works just fine even on a typical laptop, given enough memory (for this post, I use a 2016 MacBook Pro/16GB RAM, with 8GB allocated to the Spark driver).&lt;/p&gt;
&lt;p&gt;I wrote a &lt;a href=&#34;https://github.com/minimaxir/amazon-spark/blob/master/amazon_preprocess.py&#34; target=&#34;_blank&#34;&gt;simple Python script&lt;/a&gt; to combine the per-category ratings-only data from the &lt;a href=&#34;http://jmcauley.ucsd.edu/data/amazon/&#34; target=&#34;_blank&#34;&gt;Amazon product reviews dataset&lt;/a&gt; curated by Julian McAuley, Rahul Pandey, and Jure Leskovec for their 2015 paper &lt;a href=&#34;http://cseweb.ucsd.edu/~jmcauley/pdfs/kdd15.pdf&#34; target=&#34;_blank&#34;&gt;Inferring Networks of Substitutable and Complementary Products&lt;/a&gt;. The result is a 4.53 GB CSV that would definitely not open in Microsoft Excel. The truncated and combined dataset includes the &lt;strong&gt;user_id&lt;/strong&gt; of the user leaving the review, the &lt;strong&gt;item_id&lt;/strong&gt; indicating the Amazon product receiving the review, the &lt;strong&gt;rating&lt;/strong&gt; the user gave the product from 1 to 5, and the &lt;strong&gt;timestamp&lt;/strong&gt; indicating the time when the review was written (truncated to the Day). We can also infer the &lt;strong&gt;category&lt;/strong&gt; of the reviewed product from the name of the data subset.&lt;/p&gt;
&lt;p&gt;Afterwards, using the new &lt;a href=&#34;http://spark.rstudio.com&#34; target=&#34;_blank&#34;&gt;sparklyr&lt;/a&gt; package for R, I can easily start a local Spark cluster with a single &lt;code&gt;spark_connect()&lt;/code&gt; command and load the entire CSV into the cluster in seconds with a single &lt;code&gt;spark_read_csv()&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/amazon-spark/output.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are 80.74 million records total in the dataset, or as the output helpfully reports, &lt;code&gt;8.074e+07&lt;/code&gt; records. Performing advanced queries with traditional tools like &lt;a href=&#34;https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html&#34; target=&#34;_blank&#34;&gt;dplyr&lt;/a&gt; or even Python&amp;rsquo;s &lt;a href=&#34;http://pandas.pydata.org&#34; target=&#34;_blank&#34;&gt;pandas&lt;/a&gt; on such a dataset would take a considerable amount of time to execute.&lt;/p&gt;
&lt;p&gt;With sparklyr, manipulating actually-big-data is &lt;em&gt;just as easy&lt;/em&gt; as performing an analysis on a dataset with only a few records (and an order of magnitude easier than the Python approaches taught in the eDX class mentioned above!).&lt;/p&gt;
&lt;h2 id=&#34;exploratory-analysis&#34;&gt;Exploratory Analysis&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;(You can view the R code used to process the data with Spark and generate the data visualizations in &lt;a href=&#34;http://minimaxir.com/notebooks/amazon-spark/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There are &lt;strong&gt;20,368,412&lt;/strong&gt; unique users who provided reviews in this dataset. &lt;strong&gt;51.9%&lt;/strong&gt; of those users have only written one review.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/amazon-spark/user_count_cum.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Relatedly, there are &lt;strong&gt;8,210,439&lt;/strong&gt; unique products in this dataset, where &lt;strong&gt;43.3%&lt;/strong&gt; have only one review.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/amazon-spark/item_count_cum.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;After removing duplicate ratings, I added a few more features to each rating which may help illustrate how review behavior changed over time: a ranking value indicating the # review that the author of a given review has written (1st review by author, 2nd review by author, etc.), a ranking value indicating the # review that the product of a given review has received (1st review for product, 2nd review for product, etc.), and the month and year the review was made.&lt;/p&gt;
&lt;p&gt;The first two added features require a &lt;em&gt;very&lt;/em&gt; large amount of processing power, and highlight the convenience of Spark&amp;rsquo;s speed (and the fact that Spark uses all CPU cores by default, while typical R/Python approaches are single-threaded!)&lt;/p&gt;
&lt;p&gt;These changes are cached into a Spark DataFrame &lt;code&gt;df_t&lt;/code&gt;. If I wanted to determine which Amazon product category receives the best review ratings on average, I can aggregate the data by category, calculate the average rating score for each category, and sort. Thanks to the power of Spark, the data processing for this many-millions-of-records takes seconds.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_agg &amp;lt;- df_t %&amp;gt;%
group_by(category) %&amp;gt;%
summarize(count = n(), avg_rating = mean(rating)) %&amp;gt;%
arrange(desc(avg_rating)) %&amp;gt;%
collect()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/amazon-spark/avg.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Or, visualized in chart form using &lt;a href=&#34;http://ggplot2.org&#34; target=&#34;_blank&#34;&gt;ggplot2&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/amazon-spark/avg_rating_desc.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Digital Music/CD products receive the highest reviews on average, while Video Games and Cell Phones receive the lowest reviews on average, with a &lt;strong&gt;0.77&lt;/strong&gt; rating range between them. This does make some intuitive sense; Digital Music and CDs are types of products where you know &lt;em&gt;exactly&lt;/em&gt; what you are getting with no chance of a random product defect, while Cell Phones and Accessories can have variable quality from shady third-party sellers (Video Games in particular are also prone to irrational &lt;a href=&#34;http://steamed.kotaku.com/steam-games-are-now-even-more-susceptible-to-review-bom-1774940065&#34; target=&#34;_blank&#34;&gt;review bombing&lt;/a&gt; over minor grievances).&lt;/p&gt;
&lt;p&gt;We can refine this visualization by splitting each bar into a percentage breakdown of each rating from 1-5. This could be plotted with a pie chart for each category, however a stacked bar chart, scaled to 100%, looks much cleaner.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/amazon-spark/category_breakdown.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The new visualization does help support the theory above; the top categories have a significantly higher percentage of 4/5-star ratings than the bottom categories, and a much a lower proportion of 1/2/3-star ratings. The inverse holds true for the bottom categories.&lt;/p&gt;
&lt;p&gt;How have these breakdowns changed over time? Are there other factors in play?&lt;/p&gt;
&lt;h2 id=&#34;rating-breakdowns-over-time&#34;&gt;Rating Breakdowns Over Time&lt;/h2&gt;
&lt;p&gt;Perhaps the advent of the binary Like/Dislike behaviors in social media in the 2000&amp;rsquo;s have translated into a change in behavior for a 5-star review system. Here are the rating breakdowns for reviews written in each month from January 2000 to July 2014:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/amazon-spark/time_breakdown.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The voting behavior oscillates very slightly over time with no clear spikes or inflection points, which dashes that theory.&lt;/p&gt;
&lt;h2 id=&#34;distribution-of-average-scores&#34;&gt;Distribution of Average Scores&lt;/h2&gt;
&lt;p&gt;We should look at the global averages of Amazon product scores (i.e. what customers see when they buy products), and the users who give the ratings. We would expect the distributions to match, so any deviations would be interesting.&lt;/p&gt;
&lt;p&gt;Products on average, when looking at products with atleast 5 ratings, have a &lt;strong&gt;4.16&lt;/strong&gt; overall rating.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/amazon-spark/item_histogram.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When looking at a similar graph for the overall ratings given by users, (5 ratings minimum), the average rating is slightly higher at &lt;strong&gt;4.20&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/amazon-spark/user_histogram.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The primary difference between the two distributions is that there is significantly higher proportion of Amazon customers giving &lt;em&gt;only&lt;/em&gt; 5-star reviews. Normalizing and overlaying the two charts clearly highlights that discrepancy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/amazon-spark/user_item_histogram.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-marginal-review&#34;&gt;The Marginal Review&lt;/h2&gt;
&lt;p&gt;A few posts ago, I discussed how the &lt;a href=&#34;http://minimaxir.com/2016/11/first-comment/&#34; target=&#34;_blank&#34;&gt;first comment on a Reddit post&lt;/a&gt; has dramatically more influence than subsequent comments. Does user rating behavior change after making more and more reviews? Is the typical rating behavior different for the first review of a given product?&lt;/p&gt;
&lt;p&gt;Here is the ratings breakdown for the &lt;em&gt;n&lt;/em&gt;-th Amazon review a user gives:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/amazon-spark/user_nth_breakdown.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first user review has a slightly higher proportion of being a 1-star review than subsequent reviews. Otherwise, the voting behavior is mostly the same overtime, although users have an increased proportion of giving a 4-star review instead of a 5-star review as they get more comfortable.&lt;/p&gt;
&lt;p&gt;In contrast, here is the ratings breakdown for the &lt;em&gt;n&lt;/em&gt;-th review an Amazon product received:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/amazon-spark/item_nth_breakdown.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first product review has a slightly higher proportion of being a 5-star review than subsequent reviews. However, after the 10th review, there is &lt;em&gt;zero&lt;/em&gt; change in the distribution of ratings, which implies that the marginal rating behavior is independent from the current score after that threshold.&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Granted, this blog post is more playing with data and less analyzing data. What might be interesting to look into for future technical posts is conditional behavior, such as predicting the rating of a review given the previous ratings on that product/by that user. However, this post shows that while &amp;ldquo;big data&amp;rdquo; may be an inscrutable buzzword nowadays, you don&amp;rsquo;t have to work for a Fortune 500 company to be able to understand it. Even with a data set consisting of 5 simple features, you can extract a large number of insights.&lt;/p&gt;
&lt;p&gt;And this post doesn&amp;rsquo;t even look at the text of the Amazon product reviews or the metadata associated with the products! I do have a few ideas lined up there which I won&amp;rsquo;t spoil.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can view all the R and ggplot2 code used to visualize the Amazon data in &lt;a href=&#34;http://minimaxir.com/notebooks/amazon-spark/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;. You can also view the images/data used for this post in &lt;a href=&#34;https://github.com/minimaxir/amazon-spark&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;</description></item><item><title>What Percent of the Top-Voted Comments in Reddit Threads Were Also 1st Comment?</title><link>https://minimaxir.com/2016/11/first-comment/</link><pubDate>Mon, 07 Nov 2016 06:30:00 -0700</pubDate><guid>https://minimaxir.com/2016/11/first-comment/</guid><description>
&lt;p&gt;&lt;a href=&#34;https://www.reddit.com&#34; target=&#34;_blank&#34;&gt;Reddit&lt;/a&gt; threads can be a crowded place. In popular subreddits such as &lt;a href=&#34;https://www.reddit.com/r/AskReddit/&#34; target=&#34;_blank&#34;&gt;/r/AskReddit&lt;/a&gt; and &lt;a href=&#34;https://www.reddit.com/r/pics/&#34; target=&#34;_blank&#34;&gt;/r/pics&lt;/a&gt;, Reddit submissions can receive hundreds, even &lt;em&gt;thousands&lt;/em&gt; of unique comments. Some comments inevitably become lost in the noise. Reddit&amp;rsquo;s &lt;a href=&#34;https://redditblog.com/2009/10/15/reddits-new-comment-sorting-system/&#34; target=&#34;_blank&#34;&gt;ranking algorithm&lt;/a&gt; attempts to rectify this by determining comment ranking using both time and community voting; comments in a thread, by default, are ordered based on the &lt;strong&gt;points score&lt;/strong&gt; (upvotes - downvotes) the comment receives, subject to a rank decay based on the age of the comment.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/first-comment/reddit_askreddit.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In theory, this system should allow comments that posted later in the thread&amp;rsquo;s lifetime to rank much higher temporarily, then Redditors can vote on the new comment; if the new comment is good, it can now rise to the top and therefore the content which would otherwise be buried is now surfaced. Anecdotally, that doesn&amp;rsquo;t be the case with Reddit&amp;rsquo;s modern algorithm; comments made late in the thread appear at the bottom, where they likely will not receive any upvotes (this led to a minor &amp;ldquo;&lt;a href=&#34;https://www.google.com/#q=site:reddit.com+%22late+to+this+thread%22&#34; target=&#34;_blank&#34;&gt;I know I&amp;rsquo;m late to this thread but&amp;hellip;&lt;/a&gt;&amp;rdquo; meme).&lt;/p&gt;
&lt;p&gt;I, of course, am not satisfied with anecdotes. A month ago, a Redditor asked &amp;ldquo;&lt;a href=&#34;https://www.reddit.com/r/TheoryOfReddit/comments/53d5ep/what_percentage_of_the_top_comment_in_threads/&#34; target=&#34;_blank&#34;&gt;What percentage of the top comment in threads were also the first comment?&lt;/a&gt;&amp;rdquo; Why not calculate it &lt;em&gt;exactly&lt;/em&gt; using big data?&lt;/p&gt;
&lt;h2 id=&#34;getting-the-reddit-data&#34;&gt;Getting the Reddit Data&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;You can view all the &lt;a href=&#34;https://www.r-project.org&#34; target=&#34;_blank&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;http://ggplot2.org&#34; target=&#34;_blank&#34;&gt;ggplot2&lt;/a&gt; code used to query, analyze, and visualize the Reddit data in &lt;a href=&#34;http://minimaxir.com/notebooks/first-comment/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In order to process a great amount of Reddit data, I turned to &lt;a href=&#34;https://cloud.google.com/bigquery/&#34; target=&#34;_blank&#34;&gt;BigQuery&lt;/a&gt;, which now has data for &lt;a href=&#34;https://www.reddit.com/r/datasets/comments/590re2/updated_reddit_comments_and_posts_updated_on/&#34; target=&#34;_blank&#34;&gt;all Reddit comments&lt;/a&gt; until September 2016.&lt;/p&gt;
&lt;p&gt;For this analysis, I will only look at the &lt;strong&gt;top-level comments&lt;/strong&gt; (i.e. comments which are not replies to other comments), since those are the ones most affected by the ordering and submission of new comments. Additionally I will only look at comments within Reddit threads with &lt;strong&gt;atleast 30 top-level comments&lt;/strong&gt; to ensure I only look at threads with sufficient discussion and where late posts are more likely to become hidden. It also mirrors the &amp;ldquo;late to this thread&amp;rdquo; meme: can posts be &lt;em&gt;too&lt;/em&gt; late?&lt;/p&gt;
&lt;p&gt;The queried data will be all comments posted from January 2015 to September 2016: this give a good balance of sample size and foundation around the modern comment ranking algorithms. The total number of Reddit comments analyzed, after filtering on threads with sufficient conversation and limiting the scope to the first 100 comments of a thread scoring within the Top 100, is &lt;strong&gt;n = 86,561,476&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;With clever use of BigQuery window functions, I obtained the aggregate data, counting the number of comments from the filtered Reddit threads at each voting rank and created rank.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/first-comment/data.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;visualizing-the-discussion&#34;&gt;Visualizing the Discussion&lt;/h2&gt;
&lt;p&gt;Filtering on the top-voted comments (&lt;code&gt;score_rank = 1&lt;/code&gt;) only, &lt;em&gt;what percent of the top-voted comments in Reddit threads were also 1st Comment?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/first-comment/reddit-first-4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The answer is &lt;strong&gt;17.24%&lt;/strong&gt; of all top-voted comments! That&amp;rsquo;s certainly more than what I expected! Additionally, 56% of the top-voted comments were posted within the first 5 comments, and 77% within the first 10 comments. The chart follows a &lt;a href=&#34;https://en.wikipedia.org/wiki/Power_law&#34; target=&#34;_blank&#34;&gt;power-law distribution&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s invert it: filtering on only the first comments (&lt;code&gt;created_rank = 1&lt;/code&gt;) made in comment threads, &lt;em&gt;what percentage of the 1st Comments in Reddit threads were also the top-voted comment?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/first-comment/reddit-first-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By construction, the answer is the same as before (17.24%), however the followup proportions are slightly different, with the first comment ranking within the Top 5 comments 46% of the time, and within the Top 10 comments 62% of the time.&lt;/p&gt;
&lt;p&gt;It may be worth it to visualize both dimensions at the same time using a heatmap, with the created rank on one axis, score rank on the other, and a z-axis representing the number of comments at each rank pairing. We can also add a faint contour line to help visualize clusters of the data. Putting it together:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/first-comment/reddit-first-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Woah, most of the values are constrained between the semisquare constrained by the first 5 comments and the top 5 comments! But it&amp;rsquo;s harder to see trends, so let&amp;rsquo;s try applying a logarithmic base-10 scaling on the comment count:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/first-comment/reddit-first-2a.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Much better! We can see a grouping of the 5x5 semisquare, but also smaller groupings of a 30x30 shape (this may possibly be due to the 30 comment filter threshold), a faint 60x60 shape, and &lt;em&gt;voids&lt;/em&gt; in the upper-left and lower-right corners.&lt;/p&gt;
&lt;p&gt;From the 2D heatmap, there appears to be a &lt;strong&gt;positive correlation&lt;/strong&gt; between the rank of the comment and the time it was submitted. Ideally, if Reddit&amp;rsquo;s algorithm correctly cycled posts so that each comment gets a fair chance at going viral, then there should be &lt;strong&gt;no correlation&lt;/strong&gt; between score rank and time posted.&lt;/p&gt;
&lt;h2 id=&#34;analysis-by-subreddit&#34;&gt;Analysis by Subreddit&lt;/h2&gt;
&lt;p&gt;When working with Reddit data, it is always important to facet the analysis by subreddit, as subreddits can have idiosyncratic behaviors which deviate from general Reddit behavior. As noted in the original Reddit thread with the initial question, it is possible that the percentage of first comments becoming top comment is &amp;ldquo;higher in lighter subs (funny, pics, videos) than more serious subs (askscience, history, etc).&amp;rdquo;&lt;/p&gt;
&lt;p&gt;I tweaked the BigQuery above to retrieve the same data for each of the Top 100 subreddits (determined by unique commenter count over the same time period). Afterward, via scripting, I created a 1D proportion-of-first-comments-by-score-rank and 2D heatmaps for each subreddit. You can view and download the 1D charts &lt;a href=&#34;https://github.com/minimaxir/first-comment/tree/master/img-1d&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;, and the 2D heatmaps &lt;a href=&#34;https://github.com/minimaxir/first-comment/tree/master/img-2d&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For example, here&amp;rsquo;s the chart of first-comment-rankings for &lt;a href=&#34;https://www.reddit.com/r/IAmA/&#34; target=&#34;_blank&#34;&gt;/r/IAmA&lt;/a&gt;, one of Reddit&amp;rsquo;s biggest subreddits where normal Redditors can ask celebrities any question they want.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/first-comment/IAmA-1d.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unlike the all-Reddit chart, the distribution of first-comment proportions is more uniform instead of following a power law. It makes sense in theory; people would likely upvote top-level questions which the original poster replied to, so there should be less of a bias toward the first top-level comment.&lt;/p&gt;
&lt;p&gt;What does the 2D heatmap show?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/first-comment/IAmA-2d.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Damn it.&lt;/p&gt;
&lt;p&gt;While the 1D behavior is different, the overall 2D behavior is the same albeit with larger voids (indeed, in the heatmap, you can see at &lt;code&gt;created_rank = 1&lt;/code&gt;, the vertical strip doesn&amp;rsquo;t fit the pattern).&lt;/p&gt;
&lt;p&gt;It turns out that most /r/IAmA threads have this comment:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/first-comment/automoderator.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As it&amp;rsquo;s made by a robot, it&amp;rsquo;s always the first comment, and it gets ignored/downvoted in normal circumstances. Other subreddits with the same pattern of 1D irregularities, 2D regularities, and AutoModerator usage are &lt;a href=&#34;https://www.reddit.com/r/gameofthrones/&#34; target=&#34;_blank&#34;&gt;/r/gameofthrones&lt;/a&gt;, &lt;a href=&#34;https://www.reddit.com/r/photoshopbattles/&#34; target=&#34;_blank&#34;&gt;/r/photoshopbattles&lt;/a&gt;, and &lt;a href=&#34;https://www.reddit.com/r/WritingPrompts/&#34; target=&#34;_blank&#34;&gt;/r/WritingPrompts&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some subreddits have more uniformity than typical Reddit rank behavior. In &lt;a href=&#34;https://www.reddit.com/r/funny/&#34; target=&#34;_blank&#34;&gt;/r/funny&lt;/a&gt;, &lt;a href=&#34;https://www.reddit.com/r/leagueoflegends/&#34; target=&#34;_blank&#34;&gt;/r/leagueoflegends&lt;/a&gt;, &lt;a href=&#34;https://www.reddit.com/r/pics/&#34; target=&#34;_blank&#34;&gt;/r/pics&lt;/a&gt;, &lt;a href=&#34;https://www.reddit.com/r/todayilearned/&#34; target=&#34;_blank&#34;&gt;/r/todayilearned&lt;/a&gt;, and &lt;a href=&#34;https://www.reddit.com/r/video/&#34; target=&#34;_blank&#34;&gt;/r/videos&lt;/a&gt; (i.e. many default subreddits), there is no upper-left void (early comments can be poorly ranked) and the bottom-right void is minimized but still present.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/first-comment/funny-2d.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/first-comment/leagueoflegends-2d.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Inversely, there are subreddits where the correlation is obvious. &lt;a href=&#34;https://www.reddit.com/r/pcmasterrace/&#34; target=&#34;_blank&#34;&gt;/r/pcmasterrace&lt;/a&gt; and /r/gonewild both exhibit very straight lines, and are subreddits where the comments themselves are not very constructive, so whatever gets posted gets upvoted anyways.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/first-comment/pcmasterrace-2d.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/first-comment/gonewild-2d.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Rushing to say &lt;strong&gt;FIRST!!1!11!&lt;/strong&gt; in a comments section of a blog post or forum thread is a meme that long predates Reddit. However, rushing to make the first comment in a Reddit thread may have strategic merit if you want to get your voice heard.&lt;/p&gt;
&lt;p&gt;Even in the most optimistic circumstances, comments that are late to a thread have a very, very low probability of becoming one of the top comments. In fairness, it&amp;rsquo;s hard to determine with public Reddit data if tweaking the ranking algorithm such that new comments will always rank at the top initially will actually improve the Reddit user experience as a whole. On the other hand, this behavior presents an opportunity: if there is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Long_tail&#34; target=&#34;_blank&#34;&gt;long tail&lt;/a&gt; of Reddit content that is unjustifiably being buried due to lack of attention, then perhaps there is a &lt;em&gt;business opportunity&lt;/em&gt; in creating a service to discover and resurface quality comments&amp;hellip;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can view all the &lt;a href=&#34;https://www.r-project.org&#34; target=&#34;_blank&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;http://ggplot2.org&#34; target=&#34;_blank&#34;&gt;ggplot2&lt;/a&gt; code used to query, analyze, and visualize the Reddit data in &lt;a href=&#34;http://minimaxir.com/notebooks/first-comment/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;. You can also view the images/data used for this post in &lt;a href=&#34;https://github.com/minimaxir/first-comment&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>