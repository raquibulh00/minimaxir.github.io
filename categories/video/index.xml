<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Video on Max Woolf&#39;s Blog</title><link>/categories/video/</link><description>Recent content in Video on Max Woolf&#39;s Blog</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright Max Woolf &amp;copy; {year}</copyright><lastBuildDate>Thu, 07 Jan 2016 08:30:00 +0000</lastBuildDate><atom:link href="/categories/video/index.xml" rel="self" type="application/rss+xml"/><item><title>Movie Review Aggregator Ratings Have No Relationship with Box Office Success</title><link>/2016/01/movie-revenue-ratings/</link><pubDate>Thu, 07 Jan 2016 08:30:00 +0000</pubDate><guid>/2016/01/movie-revenue-ratings/</guid><description>
&lt;p&gt;&lt;a href=&#34;http://www.rottentomatoes.com&#34; target=&#34;_blank&#34;&gt;Rotten Tomatoes&lt;/a&gt; has become synonymous with movie quality in recent years. The Rotten Tomatoes Tomatometer aggregates all reviews written by movie critics for a given movie on the internet, determines whether each reviewer rates the movie as &amp;ldquo;Fresh&amp;rdquo; or &amp;ldquo;Rotten&amp;rdquo; and calculates an average. If the proportion of Fresh reviews for a given movie is greater than or equal to 60%, the movie itself is considered &amp;ldquo;Fresh&amp;rdquo; and receives a special icon.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/movie-revenue-ratings/examples.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Top Movies like Christopher Nolan&amp;rsquo;s &lt;a href=&#34;http://www.rottentomatoes.com/m/the_dark_knight/&#34; target=&#34;_blank&#34;&gt;The Dark Knight&lt;/a&gt; received a 94% Rotten Tomatoes rating, and generated $533.3 million in domestic box office revenue. But other movies, like Michael Bay&amp;rsquo;s &lt;a href=&#34;http://www.rottentomatoes.com/m/transformers_revenge_of_the_fallen/&#34; target=&#34;_blank&#34;&gt;Transformers: Revenge of the Fallen&lt;/a&gt;, received a 19% Tomatometer rating, but still generated $402.1 million in domestic box office revenue.&lt;/p&gt;
&lt;p&gt;How strong is the relationship between Tomatometer scores and box office success, anyways? Or are other, better metrics? Time to make some pretty charts.&lt;/p&gt;
&lt;p&gt;I obtained a large amount of movie data from the &lt;a href=&#34;http://www.omdbapi.com&#34; target=&#34;_blank&#34;&gt;OMDb API&lt;/a&gt;, which provides easy access to movie metadata from IMDb and Rotten Tomatoes. This data contains Rotten Tomatoes Tomatometer scores, Rotten Tomatoes Audience Scores, IMDb User Rankings, and Metacritic Scores. If you want to know how I processed the data in R and plotted the charts using ggplot2, I have &lt;a href=&#34;https://www.youtube.com/watch?v=F5Hjlkxw_2A&#34; target=&#34;_blank&#34;&gt;prepared a screencast&lt;/a&gt; for your viewing pleasure.&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
&lt;iframe src=&#34;//www.youtube.com/embed/F5Hjlkxw_2A&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;For this analysis, we will be looking at the &lt;a href=&#34;http://www.r-statistics.com/2013/05/log-transformations-for-skewed-and-wide-distributions-from-practical-data-science-with-r/&#34; target=&#34;_blank&#34;&gt;log-transformation&lt;/a&gt; of domestic box office revenue, since the values are skewed by mega-blockbusters like the ones mentioned previously. Revenues are not inflation-adjusted since the rating data is only present for recent years and due to the log-transformation already present, inflation correction would not impact this particular analysis much.&lt;/p&gt;
&lt;h2 id=&#34;rotten-tomatoes-tomatometer&#34;&gt;Rotten Tomatoes Tomatometer&lt;/h2&gt;
&lt;p&gt;After processing, I have a data subset of 4,863 movies with both Tomatometer and Box Office Gross values. Let&amp;rsquo;s plot all those movies on a scatterplot of log(BoxOffice) vs. Meter with each point having a slight transparency; that way, clusters of points will be come apparent where the areas are darker on the chart.&lt;/p&gt;
&lt;p&gt;We expect a positive linear relationship: movies with high Tomatometer scores to have high box office revenue, and inversely movies with low score to have low box office revenue.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/movie-revenue-ratings/box-office-rating-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Wait, why does the trendline have a &lt;em&gt;negative&lt;/em&gt; slope?&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient&#34; target=&#34;_blank&#34;&gt;Pearson correlation&lt;/a&gt; between the Tomatometer scores and log(BoxOffice) is &lt;strong&gt;-0.18&lt;/strong&gt;, implying a weak &lt;em&gt;negative&lt;/em&gt; linear relationship between the two variables. Not what I expected.&lt;/p&gt;
&lt;p&gt;There do appear to be clusters in the data. There is a group of points between $10M and $100M revenue and 0% to 20% Tomatometer rating. Another group is present between $1,000 and $1M revenue and 80% to 100% RT rating. Both of these areas are outside of a linear relationship: perhaps these clusters are skewing trends too?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try another visualization of the data using &lt;a href=&#34;https://en.wikipedia.org/wiki/Contour_line&#34; target=&#34;_blank&#34;&gt;contour maps&lt;/a&gt;, which allow the data to become 3D, so-to-speak. Using a 2D &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34;&gt;kernel density estimator&lt;/a&gt;, we can identify and color areas on the plot according to the number of points present in that area; the greater the color saturation, the more points present in the given area.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/movie-revenue-ratings/box-office-rating-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The two clusters mentioned previously are now much more apparent. It appears there are two distinct sets of movies: blockbusters which critics hate, and limited-appeal films which critics loves. Incidentally, there is no discernible difference between movies which are Fresh (&amp;gt;60%) and Rotten.&lt;/p&gt;
&lt;h2 id=&#34;metacritic&#34;&gt;Metacritic&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;http://www.metacritic.com&#34; target=&#34;_blank&#34;&gt;Metacritic&lt;/a&gt; score is also &lt;a href=&#34;http://www.metacritic.com/about-metascores&#34; target=&#34;_blank&#34;&gt;derived from review data&lt;/a&gt; by critics; however, instead of calculating a binary review sentiment and calculating a proportion from that sentiment, Metacritic gives a quantification from 0 to 100 to each critic review and averages them together.&lt;/p&gt;
&lt;p&gt;Does that change the results for 4,479 movies?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/movie-revenue-ratings/box-office-rating-7.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Correlation between Metacritic score and log(BoxOffice) is &lt;strong&gt;-0.13&lt;/strong&gt;, which puts the analysis in a similar state as the Rotten Tomatoes data. However, the blockbuster cluster has shifted right, and the lesser-appeal cluster has shifted left.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/movie-revenue-ratings/box-office-rating-8.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Clusters are much closer together.&lt;/p&gt;
&lt;p&gt;Perhaps a review metric by non-critics will tell a different story.&lt;/p&gt;
&lt;h2 id=&#34;rotten-tomatoes-audience-score&#34;&gt;Rotten Tomatoes Audience Score&lt;/h2&gt;
&lt;p&gt;The Audience Score is calculated in a similar way to the Rotten Tomatoes Tomatometer score: user to the site rate a movie from 0 to 5 stars in half-star increments (i.e. effectively a scale from 0-10) and the proportion of reviews with 3.5 star ratings or higher becomes the Audience Score.&lt;/p&gt;
&lt;p&gt;This also presents a cognitive bias in ratings: the &lt;a href=&#34;http://tvtropes.org/pmwiki/pmwiki.php/Main/FourPointScale&#34; target=&#34;_blank&#34;&gt;Four Point Scale&lt;/a&gt;, where having a discrete form of ranking may cause people to tend to rate toward the top of the scale and make the entire metric skewed or misleading.&lt;/p&gt;
&lt;p&gt;How does the Audience Score compare for 5,163 movies? After all, the audience is the group of people who determine how much money a movie makes at the Box Office.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/movie-revenue-ratings/box-office-rating-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Correlation between the Audience score and log(BoxOffice) is &lt;strong&gt;0.05&lt;/strong&gt;, which is a positive linear correlation, but representative of barely any practical correlation.&lt;/p&gt;
&lt;p&gt;Speaking of the Four Point Scale, notice how, like with Metacritic score, there are barely any movies between 0% and 20% Audience Score. Is there really a skew? Let&amp;rsquo;s look at the contours:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/movie-revenue-ratings/box-office-rating-4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The locations of the clusters are much different than that of Tomatometer clusters. Both clusters are closer together, with the blockbuster cluster between 50% and 60% audience score and the lesser-appeal cluster between 70% and 80%. Hence, the low correlation.&lt;/p&gt;
&lt;h2 id=&#34;imdb&#34;&gt;IMDb&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.imdb.com&#34; target=&#34;_blank&#34;&gt;IMDb&lt;/a&gt; works &lt;a href=&#34;http://www.imdb.com/help/show_leaf?votestopfaq&#34; target=&#34;_blank&#34;&gt;almost the same way&lt;/a&gt; as the Metacritic for non-critics: ratings from IMDb users between 1-10 (note that 0 is missing!) are averaged to get a final score.&lt;/p&gt;
&lt;p&gt;How do 5,167 movies fare?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/movie-revenue-ratings/box-office-rating-5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What?!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The point groupings are at the &lt;em&gt;same&lt;/em&gt; positions of ratings, and the correlation between IMDb ratings and log(BoxOffice) is &lt;strong&gt;0.00&lt;/strong&gt;. Yes, there&amp;rsquo;s &lt;em&gt;zero&lt;/em&gt; correlation!&lt;/p&gt;
&lt;p&gt;Checking the contour map confirms it:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/movie-revenue-ratings/box-office-rating-6.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That is &lt;em&gt;literally&lt;/em&gt; a Four Point Scale between 5 and 8!&lt;/p&gt;
&lt;p&gt;The Rotten Tomatoes metric is the only metric that actually &lt;em&gt;uses&lt;/em&gt; the entire rating scale. None of the other potential metrics provide more insight into a potential reason for high box-office revenue. Perhaps the movie rating system itself is broken.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s not to say that movies need high box-office revenues to be considered successful. However, working with movie profitability, and by extension movie budget, is opening another can-of-worms with respect to data integrity. (that said, on Reddit, /u/chartmkr recently &lt;a href=&#34;https://www.reddit.com/r/dataisbeautiful/comments/3zpp3w/movie_budgets_and_box_office_success_19552015_oc/&#34; target=&#34;_blank&#34;&gt;posted a visualization&lt;/a&gt; of Gross vs. Budget which is interesting).&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;ll still be fun to point to a Rotten Tomatoes Tomatometer rating as a kneejerk reaction to whether a movie rocks/sucks. Although, the reasons for movie financial success at the box office definitely warrant further investigation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UPDATE 1/11/15&lt;/strong&gt;: On a &lt;a href=&#34;https://news.ycombinator.com/item?id=10872076&#34; target=&#34;_blank&#34;&gt;discussion on Hacker News&lt;/a&gt;, it was suggested that the blockbuster movies and the indie movies cancel each other out, i.e. blockbusters have a positive correlation and indies have a negative correlation.&lt;/p&gt;
&lt;p&gt;For the blockbuster cluster alone, the log-correlation is &lt;strong&gt;0.23&lt;/strong&gt; (not weak but not great positive correlation). For the indie cluster alone, the log-correlation is &lt;strong&gt;-0.12&lt;/strong&gt; (same as original analysis).&lt;/p&gt;
&lt;p&gt;For future analysis, it may be worthwhile to split these two clusters. I stand by the original analysis for this post: very frequently I&amp;rsquo;ve heard the question &amp;ldquo;is this a good movie?&amp;rdquo; and the response is &amp;ldquo;what does the RT score say?&amp;rdquo; Both Box Office revenues and RT scores are important measures of quality (depending on perspective), and users who want to see or purchase a movie may not necessarily care if it&amp;rsquo;s indie or a blockbuster.&lt;/p&gt;
&lt;p&gt;User cwyers &lt;a href=&#34;https://news.ycombinator.com/item?id=10878019&#34; target=&#34;_blank&#34;&gt;suggested&lt;/a&gt; that Simpson&amp;rsquo;s Paradox may be in play since the number of theaters showing a movie is positively correlated to box office revenue, adding a potentially-confounding affect. I will see if I can obtain that data for future analysis.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can access the open-sourced Jupyter notebook and high-resolution charts from this article in &lt;a href=&#34;https://github.com/minimaxir/movie-revenue-ratings&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;. If you use the code or data visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Unfortunately, I cannot redistribute the data itself due to licensing concerns.&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Let&#39;s Code an Analysis and Visualizations of Yelp Data using R and ggplot2</title><link>/2015/12/lets-code-1/</link><pubDate>Mon, 28 Dec 2015 09:00:00 +0000</pubDate><guid>/2015/12/lets-code-1/</guid><description>
&lt;p&gt;One of the reasons I have open-sourced the code for my complicated data visualizations is transparency for the creation process. 2015 was a &lt;a href=&#34;http://qz.com/580859/the-most-misleading-charts-of-2015-fixed/&#34; target=&#34;_blank&#34;&gt;year of misleading and incorrect data visualizations&lt;/a&gt;, and I don&amp;rsquo;t want to help contribute to the misconception that data can be used for trickery. &amp;ldquo;Big data&amp;rdquo; in particular is a area where the steps to reproduce results are rarely released publicly in a step-by-step manner, often in an attempt to make the resulting analysis unimpeachable.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s time to take things to the next level of transparency by recording &lt;a href=&#34;https://en.wikipedia.org/wiki/Screencast&#34; target=&#34;_blank&#34;&gt;screencasts&lt;/a&gt; of my data analysis and visualizations.&lt;/p&gt;
&lt;p&gt;Last week, ggplot2 author Hadley Wickham released &lt;a href=&#34;http://blog.rstudio.org/2015/12/21/ggplot2-2-0-0/&#34; target=&#34;_blank&#34;&gt;a surprise update&lt;/a&gt; for my favorite R package, bumping the version to 2.0.0. Why not celebrate by playing around with ggplot2 and making some pretty charts?&lt;/p&gt;
&lt;h2 id=&#34;let-s-code&#34;&gt;Let&amp;rsquo;s Code!&lt;/h2&gt;
&lt;p&gt;I have recorded a screencast of myself coding in R to play around with data from &lt;a href=&#34;http://www.yelp.com/dataset_challenge&#34; target=&#34;_blank&#34;&gt;Yelp Dataset Challenge&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/watch?v=Emt9bn0D5ZI&#34; target=&#34;_blank&#34;&gt;uploaded it to YouTube&lt;/a&gt;. Additionally, the video can be played at an unusually high quality for screencasting: 1440p on supported browsers, at 60 frames per second.&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
&lt;iframe src=&#34;//www.youtube.com/embed/Emt9bn0D5ZI&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;This particular screencast is also my first significant attempt at working with audio/video editing and voice-over. Feel free to provide suggestions for future videos.&lt;/p&gt;
&lt;p&gt;Since the screencast is 40 minutes long (inadvertently!), I&amp;rsquo;ve written an abridged summary of the screencast, along with some clarification of points made.&lt;/p&gt;
&lt;h2 id=&#34;yelp-data-v2&#34;&gt;Yelp Data v2&lt;/h2&gt;
&lt;p&gt;A year ago I made a &lt;a href=&#34;http://minimaxir.com/2014/09/one-star-five-stars/&#34; target=&#34;_blank&#34;&gt;blog post analyzing the same Yelp data&lt;/a&gt;. Now that the data set contains 1.6 million reviews (as opposed to just 1.1 million back then), it might be interesting to look at it again to see if anything has changed. The data is formatted as by-line JSON: I wrote a pair of Python scripts to convert it to CSV for easy import into R.&lt;/p&gt;
&lt;p&gt;The screencast centralizes on three R packages: readr, dplyr, and ggplot2. (all authored by Hadley Wickham)&lt;/p&gt;
&lt;p&gt;Loading the dataset into R is easy and fast with &lt;code&gt;read_csv&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;df_reviews &amp;lt;- read_csv(&amp;quot;yelp_reviews.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since dplyr was loaded beforehand, read_csv loads the data into a tbl_df instead of a normal data.frame. When you call a normal data.frame by itself, &lt;em&gt;all data is printed to console&lt;/em&gt;, which is a problem when you have 1.6M rows (yes, that happened during a test recording). Calling a tbl_df results in a very descriptive overview of the data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/overview.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Most columns are self-explanatory. &lt;code&gt;review_length&lt;/code&gt; is approximate number of words in the review, &lt;code&gt;pos_words&lt;/code&gt; is the number of positive words in the review, &lt;code&gt;neg_words&lt;/code&gt; is what you expect, &lt;code&gt;net_sentiment&lt;/code&gt; is pos_words - neg_words.&lt;/p&gt;
&lt;p&gt;A quick way to analyze the distribution of numerical data is to perform a summary on the data frame, which returns a by-column &lt;a href=&#34;https://en.wikipedia.org/wiki/Five-number_summary&#34; target=&#34;_blank&#34;&gt;five-number summary&lt;/a&gt; + mean:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/summary.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ratings are biased toward 4 and 5 star reviews. There is a lot of skew for review length.&lt;/p&gt;
&lt;p&gt;dplyr makes it easy to add columns in-line with the &lt;code&gt;mutate&lt;/code&gt; command. Let&amp;rsquo;s normalize the pos_words column:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;df_reviews &amp;lt;- df_reviews %&amp;gt;% mutate(pos_norm = pos_words / review_length)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we could do similar steps for the neg_words column too. Or use mutate to transform the data of an existing column.&lt;/p&gt;
&lt;p&gt;Onto ggplot2. If you want a quick histogram of univariate data, qplot does just that. Let&amp;rsquo;s visualize the distribution of stars.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;qplot(data=df_reviews, stars)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/lc1-qplot-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Definitely a skew toward 4 and 5 star reviews.&lt;/p&gt;
&lt;p&gt;We can do that for other variables too, like review length.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/lc1-qplot-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What about bivariate data? If you give two variables to qplot, it will create a scatter plot. Perhaps there is a relationship between the number of stars and the number of positive words?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;qplot(data=df_reviews, stars, pos_words)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&amp;hellip;and then we run into a problem. In this case, ggplot2 has to plot 1.6M points to screen, which can take awhile, especially if you are simultaneously using your GPU for video recording. Eventually, we get this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/lc1-qplot-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At first glance, there appears to be a positive correlation between star rating and number of positive words, but that&amp;rsquo;s misleading: since we don&amp;rsquo;t have alpha transparency on the points, the density is ambiguous. (fixing it requires working outside of a qplot).&lt;/p&gt;
&lt;h2 id=&#34;serious-business-data&#34;&gt;Serious Business Data&lt;/h2&gt;
&lt;p&gt;We load the Yelp Businesses data into R through the same way as the reviews data. Here&amp;rsquo;s an overview of the data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/businesses.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both data frames have a &lt;code&gt;business_id&lt;/code&gt; column. We can merge them with a &lt;code&gt;left_join&lt;/code&gt;, a la SQL. If both data frames have a column with the same name, it will merge on that column by default.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;df_reviews &amp;lt;- df_reviews %&amp;gt;% left_join(df_businesses)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then the R console helpfully points out that both dataframes also have a &amp;ldquo;stars&amp;rdquo; column. Uh-oh.&lt;/p&gt;
&lt;p&gt;We reset the df_reviews data frame from scratch and merge again, explicitly stating the &amp;ldquo;by&amp;rdquo; column for merging. Now we know &lt;em&gt;where&lt;/em&gt; reviews were made, and that might provide helpful information.&lt;/p&gt;
&lt;h2 id=&#34;aggregation-station&#34;&gt;Aggregation Station&lt;/h2&gt;
&lt;p&gt;It might be interesting to know the average star rating by city. dplyr allows for &lt;code&gt;group_by&lt;/code&gt; and &lt;code&gt;summarize&lt;/code&gt; operations in a similar manner as SQL.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;df_cities &amp;lt;- df_reviews %&amp;gt;% group_by(city) %&amp;gt;% summarize(avg_stars = mean(stars.x))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/cities.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&amp;hellip;that&amp;rsquo;s not good. The original Yelp Dataset Challenge page mentioned that the dataset is only from specific cities, not &amp;ldquo;1023 E Frye Rd.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/Dataset_Map.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hmrph.&lt;/p&gt;
&lt;p&gt;From the map, it appears there is no overlap between any of the cities with geographic states, so let&amp;rsquo;s use &lt;code&gt;state&lt;/code&gt; instead. Additionally, we can add a count of reviews from that state, and sort by that count descending.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;df_states &amp;lt;- df_reviews %&amp;gt;% group_by(state) %&amp;gt;% summarize(avg_stars = mean(stars.x), count=n()) %&amp;gt;% arrange(desc(count))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/states.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks good enough, but that&amp;rsquo;s tempting fate.&lt;/p&gt;
&lt;h2 id=&#34;ggplot-all-the-things&#34;&gt;ggplot All the Things&lt;/h2&gt;
&lt;p&gt;We can plot state vs. avg_stars with ggplot2. Setting it up is easy:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;ggplot(data=df_states, aes(state, avg_stars))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/lc1-ggplot-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The blank plot is actually new to 2.0.0: running the code without any layers would normally throw an error. The axis values appear valid. Let&amp;rsquo;s add columns via &lt;code&gt;geom_bar&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;ggplot(data=df_states, aes(state, avg_stars)) + geom_bar()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&amp;hellip;and this results in an error. geom_bar by itself does histograms on raw values, as shown in the qplots. The correct fix is to add a &lt;code&gt;stat=&amp;quot;identity&amp;quot;&lt;/code&gt; parameter to geom_bar, which tells it to scale the bars by the given value of the aesthetic.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/lc1-ggplot-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Better. But the x-axis is cluttered and the States would look better on the y-axis. Time for a &lt;code&gt;coord_flip&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/lc1-ggplot-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Better. Now time to fix the order. You may notice that the order of the states is alphabetical going from the bottom of the axis to the top, and R will always set this order for any character vector. We want the sort the labels by their average star rating, descending. To do that we change the internal factor labels of state volume to the specified order.&lt;/p&gt;
&lt;p&gt;In the recording, this took awhile due to several brain farts (which happen often when dealing with factor ordering). First, we need to remove a few states with few reviews using a filter The easiest way to do this is to sort the original data frame by avg_stars descending, then set the factor order by using the new state order &lt;em&gt;in reverse&lt;/em&gt;. (Ok, ok, it might be easier to just sort ascending and not reverse, but it makes the overview harder to visualize)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;df_states &amp;lt;- df_states %&amp;gt;% arrange(desc(avg_stars)) %&amp;gt;% filter(count &amp;gt; 2000) %&amp;gt;% mutate(state = factor(state, levels=rev(state)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/states-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Rerunning the plot code afterward yields:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/lc1-ggplot-4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Good! Why not add labels for each point? This can be done with geom_text, along with adding &lt;code&gt;hjust=1&lt;/code&gt; to offset the label, changing the size, and setting the text to white. We can round the avg_star values to 2 decimal places as well.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;ggplot(data=df_states, aes(state, avg_stars)) + geom_bar(stat=&amp;quot;identity&amp;quot;) + coord_flip() + geom_text(aes(label=round(avg_stars, 2)), hjust=1, color=&amp;quot;white&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/lc1-ggplot-5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &amp;ldquo;3.7&amp;rdquo; label requires using the &lt;code&gt;sprintf&lt;/code&gt; function instead of &lt;code&gt;round&lt;/code&gt; to print &amp;ldquo;3.70&amp;rdquo;, which is not fun. Otherwise, these labels are nice so far. Why not add a theme and axis labels?&lt;/p&gt;
&lt;p&gt;I go to my &lt;a href=&#34;http://minimaxir.com/2015/02/ggplot-tutorial/&#34; target=&#34;_blank&#34;&gt;previous ggplot2 tutorial&lt;/a&gt; and copy-paste the FiveThirtyEight-inspired theme from there because I am efficient. (The theme required loading the RColorBrewer package, though). The axis labels are added through the &lt;code&gt;labs&lt;/code&gt; function. (note that since the axes are flipped, the labels must be flipped too!)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;ggplot(data=df_states, aes(state, avg_stars)) + geom_bar(stat=&amp;quot;identity&amp;quot;) + coord_flip() + geom_text(aes(label=round(avg_stars, 2)), hjust=2, size=2, color=&amp;quot;white&amp;quot;) + fte_theme() + labs(y=&amp;quot;Average Star Rating by State&amp;quot;, x=&amp;quot;State&amp;quot;, title=&amp;quot;Average Yelp Review Star Ratings by State&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/lc1-ggplot-6.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Why not add 95% confidence intervals for each average? (Note that the normality assumptions for the confidence interval may not be entirely valid). We can calculate the standard error of the mean and rebuild the dataframe and reorder factors again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;df_states &amp;lt;- df_reviews %&amp;gt;% group_by(state) %&amp;gt;% summarize(avg_stars = mean(stars.x), count=n(), se_mean=sd(stars.x)/sqrt(count)) %&amp;gt;% arrange(desc(avg_stars)) %&amp;gt;% filter(count &amp;gt; 2000) %&amp;gt;% mutate(state = factor(state, levels=rev(state)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Time to add a &lt;code&gt;geom_errorbar&lt;/code&gt; (not a &lt;code&gt;geom_crossbar&lt;/code&gt;!)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;ggplot(data=df_states, aes(state, avg_stars)) + geom_bar(stat=&amp;quot;identity&amp;quot;) + coord_flip() + geom_text(aes(label=round(avg_stars, 2)), hjust=2, size=2, color=&amp;quot;white&amp;quot;) + fte_theme() + labs(y=&amp;quot;Average Star Rating by State&amp;quot;, x=&amp;quot;State&amp;quot;, title=&amp;quot;Average Yelp Review Star Ratings by State&amp;quot;) + geom_errorbar(aes(ymin=avg_stars - 1.96 * se_mean, ymax=avg_stars + 1.96 * se_mean))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/lets-code-1/lc1-ggplot-7.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Averages are very stable for all cities due to the large sample size.&lt;/p&gt;
&lt;p&gt;At this point I realized the recording is too long and I end it there. For a normal blog post, I&amp;rsquo;d add more theming, adjust colors so they don&amp;rsquo;t clash, and add annotations, such as a line representing the true review average from the population. And ideally, performing statistical tests to determine if any averages are different from the population average.&lt;/p&gt;
&lt;p&gt;Hopefully this gives some insight into the mechanical process of creating simple data visualizations with R and ggplot2 (the &amp;ldquo;abridged summary&amp;rdquo; ended up being as long as a typical blog post!). As my screencast shows, programming is a recurring process of saying &amp;ldquo;this is easy to do!&amp;rdquo; then failing miserably for stupid reasons. Even after the 40 minute screencast, there&amp;rsquo;s still much, much more polish needed for the data visualization. My blog posts take a very long time to produce for those reasons; the clear, clean code from the finished product is not indicative of the unexpected errors that occur when writing it.{% comment %}At the least, they are &lt;em&gt;fixable&lt;/em&gt; errors, which is a strong benefit of being a good QA engineer.{% endcomment %}&lt;/p&gt;
&lt;p&gt;I did this recording &amp;ldquo;blind&amp;rdquo; to test whether or not it&amp;rsquo;s feasible for me to &lt;em&gt;stream&lt;/em&gt; the coding of data visualization on services like &lt;a href=&#34;http://www.twitch.tv&#34; target=&#34;_blank&#34;&gt;Twitch&lt;/a&gt;. It&amp;rsquo;s definitely possible, but has more logistical challenges. (namely, that &lt;a href=&#34;https://obsproject.com&#34; target=&#34;_blank&#34;&gt;OBS&lt;/a&gt; is fussy outside of Windows and I still need to figure out how to configure it optimally). I admit the code in this screencast may not be the highest-quality code (in retrospect I should have put the code in an editor instead of directly in the console, and reuse dataframe/ggplot objects), but the transparent process for coding data visualizations is important. If there is enough interest, I may revisit Yelp data again, or even more advanced datasets.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can access the R code used for the data visualizations and the Python scripts used to process the raw Yelp dataset &lt;a href=&#34;https://github.com/minimaxir/lets-code-1&#34; target=&#34;_blank&#34;&gt;in this GitHub repository&lt;/a&gt;. However, the raw data itself cannot be redistributed.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For those wondering what I used for recording the screencast:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Computer: &lt;em&gt;Late 2013 13&amp;rdquo; Retina MacBook Pro running OS X 10.11.2&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Recording Software: &lt;em&gt;Screenflow 4.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Microphone: &lt;em&gt;Shure MV5 Digital Condenser&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Music: &lt;em&gt;Various artists from the &amp;ldquo;No Attribution Required&amp;rdquo; section of the YouTube Audio Library&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>