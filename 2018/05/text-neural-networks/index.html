<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.3.1"><meta name=author content="Max Woolf"><meta name=description content="Train your own text-generating neural network and generate text whenever you want with just a few clicks!"><link rel=alternate hreflang=en-us href=/2018/05/text-neural-networks/><meta name=theme-color content=#2962ff><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400italic,700|Source+Code+Pro:400,400italic,700&display=swap"><link rel=stylesheet href=/css/academic.min.d60353c45a1511432f9bc3071cf07140.css><link rel=stylesheet href=/css/academic.b011cb496b1db375ea969a485c505836.css><link rel=manifest href=/site.webmanifest><link rel=icon type=image/png href=/img/icon.png><link rel=apple-touch-icon type=image/png href=/img/icon-192.png><link rel=canonical href=/2018/05/text-neural-networks/><meta property=twitter:card content=summary_large_image><meta property=twitter:site content=@minimaxir><meta property=twitter:creator content=@minimaxir><meta property=og:site_name content="Max Woolf's Blog"><meta property=og:url content=/2018/05/text-neural-networks/><meta property=og:title content="How to Quickly Train a Text-Generating Neural Network for Free | Max Woolf's Blog"><meta property=og:description content="Train your own text-generating neural network and generate text whenever you want with just a few clicks!"><meta property=og:image content=/2018/05/text-neural-networks/featured.png><meta property=twitter:image content=/2018/05/text-neural-networks/featured.png><meta property=og:locale content=en-us><meta property=article:published_time content=2018-05-18T09:00:00&#43;00:00><meta property=article:modified_time content=2018-05-18T09:00:00&#43;00:00><title>How to Quickly Train a Text-Generating Neural Network for Free | Max Woolf&#39;s Blog</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id=navbar-main><div class=container><a class=navbar-brand href=/>Max Woolf&#39;s Blog</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/about/><span>About</span></a></li><li class=nav-item><a class=nav-link href=/post/><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/portfolio/><span>Portfolio</span></a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=https://www.patreon.com/minimaxir target=_blank rel=noopener><span><i class="fab fa-patreon"></i>Patreon</span></a></li><li class=nav-item><a class=nav-link href=https://github.com/minimaxir target=_blank rel=noopener><span><i class="fab fa-github-alt"></i>GitHub</span></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><article class=article itemscope itemtype=http://schema.org/Article><div class="article-container pt-3"><h1 itemprop=name>How to Quickly Train a Text-Generating Neural Network for Free</h1><meta content="2018-05-18 09:00:00 &#43;0000 UTC" itemprop=datePublished><meta content="2018-05-18 09:00:00 &#43;0000 UTC" itemprop=dateModified><div class=article-metadata><span class=article-date><time>May 18, 2018</time></span>
<span class=middot-divider></span><span class=article-reading-time>9 min read</span>
<span class=middot-divider></span><a href=/2018/05/text-neural-networks/#disqus_thread></a><span class=middot-divider></span><span class=article-categories><i class="fas fa-folder"></i><a href=/categories/ai/>AI</a>, <a href=/categories/text-generation/>Text Generation</a></span><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=/2018/05/text-neural-networks/&amp;text=How%20to%20Quickly%20Train%20a%20Text-Generating%20Neural%20Network%20for%20Free" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=/2018/05/text-neural-networks/&amp;t=How%20to%20Quickly%20Train%20a%20Text-Generating%20Neural%20Network%20for%20Free" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook-f"></i></a></li><li><a href="mailto:?subject=How%20to%20Quickly%20Train%20a%20Text-Generating%20Neural%20Network%20for%20Free&amp;body=/2018/05/text-neural-networks/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=/2018/05/text-neural-networks/&amp;title=How%20to%20Quickly%20Train%20a%20Text-Generating%20Neural%20Network%20for%20Free" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://reddit.com/submit?url=/2018/05/text-neural-networks/&amp;title=How%20to%20Quickly%20Train%20a%20Text-Generating%20Neural%20Network%20for%20Free" target=_blank rel=noopener class=share-btn-reddit><i class="fab fa-reddit-alien"></i></a></li></ul></div></div></div><div class=article-container><div class=article-style itemprop=articleBody><p>One of the more interesting applications of the neural network revolution is text generation. Most popular approaches are based off of Andrej Karpathy&rsquo;s <a href=https://github.com/karpathy/char-rnn target=_blank>char-rnn architecture</a>/<a href=http://karpathy.github.io/2015/05/21/rnn-effectiveness/ target=_blank>blog post</a>, which teaches a recurrent neural network to be able to predict the next character in a sequence based on the previous <em>n</em> characters. As a result, a sufficiently trained network can theoretically reproduce its input source material, but since properly-trained neural networks aren&rsquo;t <em>perfect</em>, the output can fall into a weird-but-good uncanny valley.</p><p><img src=/img/text-neural-networks/textgenrnn_console.gif alt></p><p>Many internet tutorials for text-generation neural networks simply copy an existing char-rnn implementation while changing the input dataset. It&rsquo;s one approach, but there&rsquo;s an opportunity for improvement with modern deep learning tooling. Thanks to frameworks like <a href=https://www.tensorflow.org target=_blank>TensorFlow</a> and <a href=https://github.com/keras-team/keras target=_blank>Keras</a>, I built <a href=https://github.com/minimaxir/textgenrnn target=_blank>textgenrnn</a>, a <a href=https://pypi.org/project/textgenrnn/#description target=_blank>Python package</a> which abstracts the process of creating and training such char-rnns to a <em>few lines of code</em>, with numerous model architecture and training improvements such as <a href=http://minimaxir.com/2017/04/char-embeddings/ target=_blank>character embeddings</a>, attention-weighted averaging, and a decaying learning rate.</p><p>A neat benefit of textgenrnn is that it can be easily used to train neural networks on a GPU very quickly, <em>for free</em> using <a href=https://colab.research.google.com/notebooks/welcome.ipynb target=_blank>Google Colaboratory</a>. I&rsquo;ve <a href="https://drive.google.com/file/d/1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK/view?usp=sharing" target=_blank>created a notebook</a> which lets you train your own network and generate text whenever you want with just a few clicks!</p><h2 id=your-first-text-generating-neural-network>Your First Text-Generating Neural Network</h2><p>Colaboratory is a notebook environment similar to <a href=http://jupyter.org target=_blank>Jupyter Notebooks</a> used in other data science projects. However, Colaboratory notebooks are hosted in a short term virtual machine, with 2 vCPUs, 13GB memory, and a K80 GPU attached. For free. Normally, this configuration would <a href=https://cloud.google.com/compute/pricing target=_blank>cost</a> $0.57/hr on Google Compute Engine; it sounds low, but adds up when you need to train model(s) for hours to get good results.</p><p>First, I recommend copying the notebook to your own Drive so it&rsquo;ll always be there (and switch to using Google Chrome if you aren&rsquo;t). The Colaboratory VM contains Python 3 and common Python packages for machine learning such as TensorFlow. But you can install more packages directly in the notebook. Like textgenrnn! Just run this cell by clicking into the cell and click the &ldquo;play&rdquo; button (or use Shift + Enter) and it&rsquo;ll take care of the rest:</p><p><img src=/img/text-neural-networks/pip.png alt></p><p>When training a new model, textgenrnn allows you to specify the size and complexity of the neural network with a wide variety of parameters:</p><p><img src=/img/text-neural-networks/config.png alt></p><p>Let&rsquo;s keep these default parameters for now, so run that cell to load them into memory. Run the next cell, which prompts you to upload a file. <em>Any text file should work</em>, even large text files! For this example, we&rsquo;ll use a 1.1MB text file of Shakespeare plays also <a href=https://github.com/karpathy/char-rnn/tree/master/data/tinyshakespeare target=_blank>used in the char-rnn demos</a>.</p><p><img src=/img/text-neural-networks/upload.png alt></p><p>The next cell initializes an instance of textgenrnn and begins training a custom new text-generating neural network!</p><p><img src=/img/text-neural-networks/train.png alt></p><p>textgenrnn automatically processes the input text into character sequences ready to train the network. After every 2 epochs (a full pass through the data), the network will generate sample text at different temperatures, which represent the &ldquo;creativity&rdquo; of the text (i.e. it allows the model to make increasingly suboptimal predictions, which can cause hilarity to ensue). I typically like generating text at a temperature of 0.5, but for very well-trained models, you can go up to 1.0.</p><p>The quick model training speed comes from the VM&rsquo;s GPU, which can perform the necessary mathematical operations much faster than with a CPU. However, in the case of recurrent neural networks, Keras recently added a <a href=https://keras.io/layers/recurrent/#cudnnlstm target=_blank>CuDNN implementation of RNNs</a> like LSTMs, which can easily tap into the GPU-native code more easily and gain a <em>massive</em> speed boost (<a href=http://minimaxir.com/2017/11/benchmark-gpus/ target=_blank>about <em>7x as fast</em></a>) compared to previous implementations! In all, for this example dataset and model architecture, training on a GPU took 5-6 minutes an epoch, while on a modern CPU, training took <em>1 hour and 24 minutes</em> an epoch, a <strong>14x speedup</strong> on the GPU!</p><p>After training is complete, running the next cell will download three files: a <code>weights</code> file, a <code>vocabulary</code> file, and a <code>config</code> file that are all needed to regenerate your model elsewhere.</p><p><img src=/img/text-neural-networks/download.png alt></p><p>For example, on your own personal computer. Just install textgenrnn + TensorFlow by inputting <code>pip3 install textgenrnn tensorflow</code> into a terminal, change to the directory where the downloaded files are located, run <code>python3</code>, and load the model using:</p><pre><code class=language-python>from textgenrnn import textgenrnn
textgen = textgenrnn(weights_path='colaboratory_weights.hdf5',
                       vocab_path='colaboratory_vocab.json',
                       config_path='colaboratory_config.json')
</code></pre><p>And that&rsquo;s that! No GPU necessary if you&rsquo;re just generating text. You can generate samples (like during training) using <code>textgen.generate_samples()</code>, generate a ton of samples at any temperature you like to a file using <code>textgen.generate_to_file()</code>, or incorporate a generated text into a Python script (e.g. a Twitter bot) using <code>textgen.generate(1, return_as_list=True)[0]</code> to store a text as a variable. You can view more of textgenrnn&rsquo;s functions and capabilities in <a href=https://github.com/minimaxir/textgenrnn/blob/master/docs/textgenrnn-demo.ipynb target=_blank>this demo Jupyter Notebook</a>.</p><p>Here&rsquo;s some Shakespeare generated with a 50-minute-trained model at a temperature of 0.5:</p><pre><code>LUCENTIO:
And then shall good grave to my wife thee;
Thou would the cause the brieved to me,
And let the place and then receives:
The rest you the foren to my ways him child,
And marry that will be a parties and so set me that be deeds
And then the heart and be so shall make the most as he and stand of seat.

GLOUCESTER:
Your father and madam, or shall for the people
And dead to make the truth, or a business
As we brother to the place her great the truth;
And that which to the smaster and her father,
I am I was see the sun have to the royal true.
</code></pre><p>Not too bad, and it&rsquo;s even close to <a href=https://en.wikipedia.org/wiki/Iambic_pentameter target=_blank>iambic pentameter</a>!</p><h2 id=tweaking-the-model>Tweaking the Model</h2><p>The most important model configuration options above are <code>rnn_size</code> and <code>rnn_layers</code>: these determine the complexity of the network. Typically, you&rsquo;ll see networks in tutorials be a single 128-cell or 256-cell network. However, textgenrnn&rsquo;s architecture is slightly different as it has an attention layer which incorporates <em>all</em> the preceding model layers. As a result, it&rsquo;s much better to go deeper than wider (e.g. 4x128 is better than 1x512) unless you have a very large amount of text (&gt;10MB). <code>rnn_bidirectional</code> controls whether the recurrent neural network is bidirectional, that is, it processes the previous characters both forward <em>and</em> backward (which works great if text follows specific rules, like Shakespeare&rsquo;s character headings). <code>max_length</code> determines the maximum number of characters for the network to use to predict the next character, which should be increased to let the network learn longer sequences, or decrease for shorter sequences.</p><p>Training has a few helpful options as well. <code>num_epochs</code> determines the number of full passes of the data; this can be tweaked if you want to train the model even more. <code>batch_size</code> determines the number of model sequences to train in a step: typically, batch size for deep learning models is 32 or 128, but with a GPU, you can get a speed increase by saturating it with the given 1024 default. <code>train_size</code> determines the proportion of character samples to train; setting it <code>&lt; 1.0</code> both speeds up each epoch, and prevents the model from cheating and being able to learn sequences verbatim. (You can set <code>'validation': True</code> to run the model on the unused data after each epoch to see if the model is overfitting).</p><p>Let&rsquo;s try playing with the parameters more on a new text dataset.</p><h2 id=word-level-text-generation-with-reddit-data>Word-Level Text Generation With Reddit Data</h2><p>You might be asking &ldquo;how do you obtain text data&rdquo;? The popular text-generation use cases like lyric generation and movie scripts are copyright-protected so they&rsquo;re harder to find, and even then, it might not be enough text data to train a new model upon (you typically want atleast 100,000 characters).</p><p><a href=https://www.reddit.com target=_blank>Reddit</a>, however, has <em>millions</em> of submission titles which would be great to train for a model. I wrote a <a href=https://github.com/minimaxir/subreddit-generator target=_blank>helper script</a> to automatically download the top <em>n</em> Reddit submissions from a given subreddit over a given period of time. If you choose subreddits with similar linguistic styles in their titles, the subreddits will even blend together! Let&rsquo;s play with the Top 20,000 Submissions in 2017 from each of <a href=https://www.reddit.com/r/politics/ target=_blank>/r/politics</a> and <a href=https://www.reddit.com/r/technology/ target=_blank>/r/technology</a>, which results in a 3.3MB file: about 3x as much data as the Shakespeare plays.</p><p><img src=/img/text-neural-networks/reddit_data.png alt></p><p>One last thing that textgenrnn can do that most char-rnn implementations can&rsquo;t is generate a <em>word level</em> model (thanks to Keras&rsquo;s tokenizers), where the model uses the <em>n</em> previous words/punctuation to predict the next word/punctuation. On the plus side, using only words prevents crazy typoes and since it predicts multiple &ldquo;characters&rdquo; at a time, <code>max_length</code> can be reduced proportionally, dramatically speeding up training. There&rsquo;s two downsides with this approach; since words are all lowercase and punctuation is its own token, the generated text cannot be immediately used without manual editing. Additionally, the model weights will be substantially larger than a character-level model since the word-level model has to store an embedding for each word (up to <code>max_words</code>, which is 10,000 by default when the vocabulary size for a char-level model is 200-300).</p><p>Another advantage of the Colaboratory notebook is that you can quickly adjust model parameters, upload a new file, and immediately start training it. We&rsquo;ll set <code>'line_delimited': True</code> and <code>'rnn_bidirectional': False</code> since there aren&rsquo;t specific rules. For word level training, let&rsquo;s set <code>'word_level': True</code> and <code>'max_length': 8</code> to reflect the new training architecture. Since training length has been reduced to 1/5th, we can set <code>'num_epochs': 50</code> and <code>'gen_epoch': 10</code> to balance it out. Rerun the config cell to update parameters, upload the Reddit data file, and rerun training.</p><p>The resulting model is much more well trained than the Shakespeare model, and here&rsquo;s a few Reddit submission titles generated at a temperature of 1.0:</p><pre><code>report : 49 % of americans now believe all of the country ’ s effective

people like facebook like it ' s 650 of 1 %

uber accused of secretly - security popular service ( likely oklahoma )

equifax breach fallout : your salary is dead

sanders uses texas shooter ' s iphone sales

adobe videos will be used to sell the web

apple to hold cash for $ 500 service

fitbit just targeting solar energy

george bush ' s concept car ‘ goes for all the biggest controversy .
</code></pre><p>Those look pretty good, although they may need a little editing before posting on social media.</p><h2 id=followup>Followup</h2><p>These examples only train the model for little time as a demo of textgenrnn&rsquo;s fast learning; there&rsquo;s nothing stopping you from increasing <code>num_epochs</code> even more to further refine a model. However, from my experience, the training cell times out after <strong>4 hours</strong>; set <code>num_epochs</code> accordingly, although in my experience that&rsquo;s all you need before the network converges.</p><p>In practice, I used this Colaboratory notebook to train <em>many</em> models for <a href=https://www.reddit.com/r/SubredditNN/ target=_blank>/r/SubredditNN</a>, a Reddit subreddit where only text-generating neural network bots trained on other subreddits. And the results are very funny:</p><p><img src=/img/text-neural-networks/subredditnn.png alt></p><p>Although text generating neural networks aren&rsquo;t at the point where they can <a href=https://www.bloomberg.com/news/features/2018-05-17/i-tried-to-get-an-ai-to-write-this-story-paul-ford target=_blank>write entire articles by themselves</a>, there are still many opportunities to use it just for fun! And thanks to textgenrnn, it&rsquo;s easy, fast, and cost-effective for anyone to do! Let me know if you make any interesting neural networks with textgenrnn and this Notebook!</p><div class="alert alert-note"><div>If you liked this blog post, I have set up a <a href=https://www.patreon.com/minimaxir target=_blank>Patreon</a> to fund my machine learning/deep learning/software/hardware needs for my future crazy yet cool projects, and any monetary contributions to the Patreon are appreciated and will be put to good creative use.</div></div></div><div class=article-tags><a class="badge badge-light" href=/tags/python/>Python</a>
<a class="badge badge-light" href=/tags/tensorflow/>TensorFlow</a>
<a class="badge badge-light" href=/tags/keras/>Keras</a></div><div class="media author-card" itemscope itemtype=http://schema.org/Person><img class="portrait mr-3" src="https://s.gravatar.com/avatar/28f09e3deff62333b3f32f19d3971d46?s=200')" itemprop=image alt=Avatar><div class=media-body><h5 class=card-title itemprop=name><a href=/>Max Woolf</a></h5><h6 class=card-subtitle>Data Scientist at BuzzFeed</h6><p class=card-text itemprop=description>Ex-Apple. Carnegie Mellon graduate. Plotter of pretty charts. Former TechCrunch comment troll.</p><ul class=network-icon aria-hidden=true><li><a itemprop=sameAs href=https://twitter.com/minimaxir target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a itemprop=sameAs href=https://linkedin.com/in/minimaxir target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a itemprop=sameAs href=https:/youtube.com/minimaxir target=_blank rel=noopener><i class="fab fa-youtube"></i></a></li><li><a itemprop=sameAs href=https:/twitch.tv/minimaxir target=_blank rel=noopener><i class="fab fa-twitch"></i></a></li><li><a itemprop=sameAs href=mailto:max@minimaxir.com><i class="fas fa-envelope"></i></a></li></ul></div></div><div class=article-widget><div class=hr-light></div><h3>Related</h3><ul><li><a href=/2017/06/reddit-deep-learning/>Predicting the Success of a Reddit Submission with Deep Learning and Keras</a></li><li><a href=/2017/11/benchmark-gpus/>Benchmarking Modern GPUs for Maximum Cloud Cost Efficiency in Deep Learning</a></li><li><a href=/2017/11/magic-the-gifening/>Making Magic: the GIFening</a></li><li><a href=/2017/07/cpu-or-gpu/>Benchmarking TensorFlow on Cloud CPUs: Cheaper Deep Learning than Cloud GPUs</a></li><li><a href=/2017/06/r-notebooks/>Advantages of Using R Notebooks For Data Analysis Instead of Jupyter Notebooks</a></li></ul></div><div class=article-widget><div class=post-nav><div class=post-nav-item><div class=meta-nav>Next</div><a href=/2018/07/imdb-data-analysis/ rel=next>Analyzing IMDb Data The Intended Way, with R and ggplot2</a></div><div class=post-nav-item><div class=meta-nav>Previous</div><a href=/2018/03/basketball-shots/ rel=prev>Visualizing One Million NCAA Basketball Shots</a></div></div></div><section id=comments><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"minimaxir"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js></script><script id=dsq-count-scr src=//minimaxir.disqus.com/count.js async></script><script>hljs.initHighlightingOnLoad();</script><script src=/js/academic.min.bc1d5e4f014b8d38d75521ad1ae2ab18.js></script><div class=container><footer class=site-footer><p class=powered-by>Copyright Max Woolf &copy; 2019 &middot;
Powered by the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a>.
<span class=float-right aria-hidden=true><a href=# id=back_to_top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&times;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>