<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Web Scraping on Max Woolf&#39;s Blog</title><link>/tags/web-scraping/</link><description>Recent content in Web Scraping on Max Woolf&#39;s Blog</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright Max Woolf &amp;copy; {year}</copyright><lastBuildDate>Fri, 26 Sep 2014 10:30:00 +0000</lastBuildDate><atom:link href="/tags/web-scraping/index.xml" rel="self" type="application/rss+xml"/><item><title>The Least Effective Method For Blocking Web Scraping of a Website</title><link>/2014/09/buzzscrape/</link><pubDate>Fri, 26 Sep 2014 10:30:00 +0000</pubDate><guid>/2014/09/buzzscrape/</guid><description>
&lt;p&gt;As someone who typically plays around with data from public website APIs, I figured it would be worthwhile to learn how to get data from websites the old-fashioned way: through force, via web scraping. &lt;a href=&#34;http://en.wikipedia.org/wiki/Web_scraping&#34; target=&#34;_blank&#34;&gt;Web scraping&lt;/a&gt; is a technique where the user downloads the raw HTML of a webpage, and parses the inherent structure of the webpage to extract the necessary data.&lt;/p&gt;
&lt;p&gt;In order to understand the mechanics behind web scraping, I used a tool from new Y Combinator startup &lt;a href=&#34;https://www.kimonolabs.com/&#34; target=&#34;_blank&#34;&gt;Kimono Labs&lt;/a&gt;, which allows the user to click structured elements of any website and quickly create an API that can access all the collected data and convert it into an easy-to-access form. My first target was BuzzFeed, since their social interaction data might be useful in determining how it became so big so quickly, and maybe also determine just how effective those stupid listicles are.&lt;/p&gt;
&lt;h2 id=&#34;live-free-and-scrape-hard&#34;&gt;Live Free and Scrape Hard&lt;/h2&gt;
&lt;p&gt;For example, here&amp;rsquo;s a pair of typical BuzzFeed articles:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/buzzscrape/buzzfeed_example.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And here&amp;rsquo;s the corresponding output of &lt;a href=&#34;https://www.kimonolabs.com/apis/1x3l57k0&#34; target=&#34;_blank&#34;&gt;my Kimono Labs BuzzFeed scraper&lt;/a&gt; for those two articles:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
&amp;quot;title&amp;quot;: {
&amp;quot;text&amp;quot;: &amp;quot;Stop Tweeting Instagram Links&amp;quot;,
&amp;quot;href&amp;quot;: &amp;quot;http://www.buzzfeed.com/katienotopoulos/stop-tweeting-instagram-links&amp;quot;
},
&amp;quot;content&amp;quot;: &amp;quot;I CANNOT REMAIN SILENT ON THIS ISSUE ANY LONGER.&amp;quot;,
&amp;quot;author&amp;quot;: &amp;quot;Katie Notopoulos&amp;quot;,
&amp;quot;num_responses&amp;quot;: &amp;quot;123&amp;quot;
},
{
&amp;quot;title&amp;quot;: {
&amp;quot;text&amp;quot;: &amp;quot;28 Things Your Gchat Availability Status Really Means&amp;quot;,
&amp;quot;href&amp;quot;: &amp;quot;http://www.buzzfeed.com/katieheaney/28-things-your-gchat-availability-status-really-means&amp;quot;
},
&amp;quot;content&amp;quot;: &amp;quot;Letâ€™s chat.&amp;quot;,
&amp;quot;author&amp;quot;: &amp;quot;Katie Heaney&amp;quot;,
&amp;quot;num_responses&amp;quot;: &amp;quot;13&amp;quot;
}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This data format (&lt;a href=&#34;http://en.wikipedia.org/wiki/JSON&#34; target=&#34;_blank&#34;&gt;JSON&lt;/a&gt;) can be processed and manipulated by nearly any modern programming language.&lt;/p&gt;
&lt;p&gt;Each BuzzFeed category page has about 22 articles, but the scraper can also access successive pages to retrieve more articles. This works by finding the URL of the next page by deriving the URL from the &amp;ldquo;Older&amp;rdquo; button if present on that page. Then the scraper can find the Older button on the page after that, and so forth, until all pages on BuzzFeed are scraped.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/buzzscrape/buzzfeed_page.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;My BuzzFeed scraper, for some reason, only retrieved 10 pages maximum. So I used my QA skills and traced the exact route the scraper took, navigating through all 10 pages by clicking the Older button.&lt;/p&gt;
&lt;p&gt;What was on Page #10 shocked me. OMG. I could not believe my eyes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/buzzscrape/buzzfeed_disable.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;no-scrape-for-you&#34;&gt;No Scrape For You&lt;/h2&gt;
&lt;p&gt;The Older button is &lt;em&gt;disabled&lt;/em&gt; on page 10? &lt;strong&gt;WHY?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;BuzzFeed is a website created in the 21st century that &lt;a href=&#34;http://www.nytimes.com/2014/08/11/technology/a-move-to-go-beyond-lists-for-content-at-buzzfeed.html?_r=0&#34; target=&#34;_blank&#34;&gt;just raised $50 million&lt;/a&gt;. This is not a peculiar technical limitation due to bad coding or bad infrastructure; this is a deliberate functional aspect of the product.&lt;/p&gt;
&lt;p&gt;It is my personal mantra that if a startup has a particularly unintuitive UI/UX behavior, it&amp;rsquo;s a form of &amp;ldquo;growth hacking&amp;rdquo; that my feeble brain cannot comprehend.&lt;/p&gt;
&lt;p&gt;Could the reason that BuzzFeed disables access to pages past 10 is that they want to prevent archive browsing, thereby putting more of an emphasis on more recent and more potentially-viral articles? That wouldn&amp;rsquo;t make sense; if a person is so hooked on BuzzFeed articles that they are at Page 10, why stop them? It&amp;rsquo;s still free page views and ad revenue.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s likely that BuzzFeed has statistics on how many users actually visit article pages up to Page 10. Perhaps their data analysts noted &amp;ldquo;hey, only 0.0027% of our visitors actually read up to Page 10, anyone who actually reads that far is three standard deviations away from normal, therefore they must be a web scraper!&amp;rdquo; and recommend punitive action accordingly.&lt;/p&gt;
&lt;p&gt;I noted earlier that it definitely was not a technical limitation that kept pages greater than 10 from being accessed. If you look at the URLs of the BuzzFeed screenshots above, you&amp;rsquo;ll notice a &amp;ldquo;&lt;em&gt;p=2&lt;/em&gt;&amp;rdquo; parameter for Page 2 or a &amp;ldquo;&lt;em&gt;p=10&lt;/em&gt;&amp;rdquo; parameter for Page 10. What happens if you just change the 10 to a 11?&lt;/p&gt;
&lt;p&gt;You go to Page 11. And you can go all the way to page &lt;em&gt;200&lt;/em&gt; in some categories.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/buzzscrape/buzzfeed_200.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Incidentally, this is easier and more reliable to implement programmatically than searching for the Older button and extracting the URL.&lt;/p&gt;
&lt;p&gt;So the disabling of the button will only stop stupid web scrapers. Yes, that makes my BuzzFeed web scraper a stupid web scraper, &lt;em&gt;but that&amp;rsquo;s not the point!&lt;/em&gt; It just makes the BuzzFeed&amp;rsquo;s motive behind disabling the button at Page 10 even more baffling.&lt;/p&gt;
&lt;p&gt;What did I do now that my quick-and-dirty BuzzFeed scraper couldn&amp;rsquo;t parse a statistically significant amount of BuzzFeed articles? I did the only logical thing.&lt;/p&gt;
&lt;p&gt;I spent a weekend learning how to use &lt;a href=&#34;http://www.crummy.com/software/BeautifulSoup/&#34; target=&#34;_blank&#34;&gt;BeautifulSoup&lt;/a&gt; in Python and parsed BuzzFeed the hard way, while changing the page number in the URL to pagenate through the pages. And I even was able to add new features to the scraper, such as simultaneously scraping the number of Facebook Shares a given BuzzFeed article generates.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/buzzscrape/buzzfeed_listicles.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hey, there &lt;em&gt;is&lt;/em&gt; a relationship between listicle size and social media engagement!&lt;/p&gt;</description></item></channel></rss>