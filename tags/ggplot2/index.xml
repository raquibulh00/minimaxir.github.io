<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ggplot2 on Max Woolf&#39;s Blog</title><link>https://minimaxir.com/tags/ggplot2/</link><description>Recent content in ggplot2 on Max Woolf&#39;s Blog</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright Max Woolf &amp;copy; {year}</copyright><lastBuildDate>Wed, 23 Oct 2019 09:00:00 -0700</lastBuildDate><atom:link href="https://minimaxir.com/tags/ggplot2/index.xml" rel="self" type="application/rss+xml"/><item><title>Visualizing Airline Flight Characteristics Between SFO and JFK</title><link>https://minimaxir.com/2019/10/sfo-jfk-flights/</link><pubDate>Wed, 23 Oct 2019 09:00:00 -0700</pubDate><guid>https://minimaxir.com/2019/10/sfo-jfk-flights/</guid><description>
&lt;p&gt;In March, &lt;a href=&#34;https://cloud.google.com&#34; target=&#34;_blank&#34;&gt;Google Compute Platform&lt;/a&gt; developer advocate &lt;a href=&#34;https://twitter.com/felipehoffa&#34; target=&#34;_blank&#34;&gt;Felipe Hoffa&lt;/a&gt; made a tweet about airline flight data from San Francisco International Airport (SFO) to Seattle-Tacoma International Airport (SEA):&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;The time to fly from San Francisco to Seattle (SFO-&amp;gt;SEA) keeps getting longer throughout the years - with a huge increase in how much time the airplane spends taxiing around SeaTac .&lt;br&gt;&lt;br&gt;Playing with US flights data in BigQuery: &lt;a href=&#34;https://t.co/eD9unaokWx&#34;&gt;https://t.co/eD9unaokWx&lt;/a&gt; &lt;a href=&#34;https://t.co/3vfnBhiJv4&#34;&gt;pic.twitter.com/3vfnBhiJv4&lt;/a&gt;&lt;/p&gt;&amp;mdash; Felipe Hoffa (@felipehoffa) &lt;a href=&#34;https://twitter.com/felipehoffa/status/1111050585120206848?ref_src=twsrc%5Etfw&#34;&gt;March 27, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;Particularly, his visualization of total elapsed times by airline caught my eye.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pbs.twimg.com/media/D2s9oFtX4AEK6nD?format=jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The overall time for flights from SFO to SEA goes up drastically starting in 2015, and this increase occurs across multiple airlines, implying that it&amp;rsquo;s not an airline-specific problem. But what could intuitively cause that?&lt;/p&gt;
&lt;p&gt;U.S. domestic airline data is &lt;a href=&#34;https://www.transtats.bts.gov/Tables.asp?DB_ID=120&#34; target=&#34;_blank&#34;&gt;freely distributed&lt;/a&gt; by the United States Department of Transportation. Normally it&amp;rsquo;s a pain to work with as it&amp;rsquo;s very large with millions of rows, but BigQuery makes playing with such data relatively easy, fun, and free. What other interesting factoids can be found?&lt;/p&gt;
&lt;h2 id=&#34;expanding-on-sfo-sea&#34;&gt;Expanding on SFO → SEA&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/bigquery/&#34; target=&#34;_blank&#34;&gt;BigQuery&lt;/a&gt; is a big data warehousing tool that allows you to query massive amounts of data. The table Hoffa created from the airline data (&lt;code&gt;fh-bigquery.flights.ontime_201903&lt;/code&gt;) is 83.37 GB and 184 &lt;em&gt;million&lt;/em&gt; rows. You can query 1 TB of data from it for free, but since BQ will only query against the fields you request, the queries in this post only consume about 2 GB each, allowing you to run them well within that quota.&lt;/p&gt;
&lt;p&gt;Hoffa&amp;rsquo;s query that runs on BigQuery looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT FlightDate_year, Reporting_Airline
, AVG(ActualElapsedTime) ActualElapsedTime
, AVG(TaxiOut) TaxiOut
, AVG(TaxiIn) TaxiIn
, AVG(AirTime) AirTime
, COUNT(*) c
FROM `fh-bigquery.flights.ontime_201903`
WHERE Origin = &#39;SFO&#39;
AND Dest = &#39;SEA&#39;
AND FlightDate_year &amp;gt; &#39;2010-01-01&#39;
GROUP BY 1,2
ORDER BY 1 DESC, 3 DESC
LIMIT 1000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For each year and airline after 2010, the query calculates the average metrics specified for flights on the SFO → SEA route.&lt;/p&gt;
&lt;p&gt;I made a few query and data visualization tweaks to what Hoffa did above, and here&amp;rsquo;s the result showing the increase in elapsed airline flight time, over time for that route:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/sfo-jfk-flights/sfo_sea_flight_duration.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s explain what&amp;rsquo;s going on here.&lt;/p&gt;
&lt;p&gt;A common trend in statistics is avoiding using &lt;a href=&#34;https://en.wikipedia.org/wiki/Average&#34; target=&#34;_blank&#34;&gt;averages&lt;/a&gt; as a summary statistic whenever possible, as averages can be overly affected by strong outliers (and with airline flights, there are definitely strong outliers!). The solution is to use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Median&#34; target=&#34;_blank&#34;&gt;median&lt;/a&gt; instead, but one problem: medians are hard and &lt;a href=&#34;https://www.periscopedata.com/blog/medians-in-sql&#34; target=&#34;_blank&#34;&gt;computationally complex&lt;/a&gt; to calculate compared to simple averages. Despite the rise of &amp;ldquo;big data&amp;rdquo;, most databases and BI tools don&amp;rsquo;t have a &lt;code&gt;MEDIAN&lt;/code&gt; function that&amp;rsquo;s as easy to use as an &lt;code&gt;AVG&lt;/code&gt; function. But BigQuery has an uncommon &lt;a href=&#34;https://cloud.google.com/bigquery/docs/reference/standard-sql/approximate_aggregate_functions#approx_quantiles&#34; target=&#34;_blank&#34;&gt;APPROX_QUANTILES&lt;/a&gt; function, which calculates the specified amount of quantiles; for example, if you call &lt;code&gt;APPROX_QUANTILES(ActualElapsedTime, 100)&lt;/code&gt;, it will return an array with the 100 quantiles, where the median will be the 50th quantile. BigQuery &lt;a href=&#34;https://cloud.google.com/bigquery/docs/reference/standard-sql/approximate-aggregation&#34; target=&#34;_blank&#34;&gt;uses&lt;/a&gt; an algorithmic trick called &lt;a href=&#34;https://en.wikipedia.org/wiki/HyperLogLog&#34; target=&#34;_blank&#34;&gt;HyperLogLog++&lt;/a&gt; to calculate these quantiles efficiently even with millions of data points. But since we get other quantiles like the 5th, 25th, 75th, and 95th quantiles for free with that approach, we can visualize the &lt;em&gt;spread&lt;/em&gt; of the data.&lt;/p&gt;
&lt;p&gt;We can aggregate the data by month for more granular trends and calculate the &lt;code&gt;APPROX_QUANTILES&lt;/code&gt; in a subquery so it only has to be computed once. Hoffa also uploaded a more recent table (&lt;code&gt;fh-bigquery.flights.ontime_201908&lt;/code&gt;) with a few additional months of data. To make things more simple, we&amp;rsquo;ll ignore aggregating by airlines since the metrics do not vary strongly between them. The final query ends up looking like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;#standardSQL
SELECT Year, Month, num_flights,
time_q[OFFSET(5)] AS q_5,
time_q[OFFSET(25)] AS q_25,
time_q[OFFSET(50)] AS q_50,
time_q[OFFSET(75)] AS q_75,
time_q[OFFSET(95)] AS q_95
FROM (
SELECT Year, Month,
COUNT(*) as num_flights,
APPROX_QUANTILES(ActualElapsedTime, 100) AS time_q
FROM `fh-bigquery.flights.ontime_201908`
WHERE Origin = &#39;SFO&#39;
AND Dest = &#39;SEA&#39;
AND FlightDate_year &amp;gt; &#39;2010-01-01&#39;
GROUP BY Year, Month
)
ORDER BY Year, Month
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The resulting data table:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/sfo-jfk-flights/table.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In retrospect, since we&amp;rsquo;re only focusing on one route, it isn&amp;rsquo;t &lt;em&gt;big&lt;/em&gt; data (this query only returns data on 64,356 flights total), but it&amp;rsquo;s still a very useful skill if you need to analyze more of the airline data (the &lt;code&gt;APPROX_QUANTILES&lt;/code&gt; function can handle &lt;em&gt;millions&lt;/em&gt; of data points very quickly).&lt;/p&gt;
&lt;p&gt;As a professional data scientist, one of my favorite types of data visualization is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Box_plot&#34; target=&#34;_blank&#34;&gt;box plot&lt;/a&gt;, as it provides a way to visualize spread without being visually intrusive. Data visualization tools like &lt;a href=&#34;https://www.r-project.org&#34; target=&#34;_blank&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://ggplot2.tidyverse.org/index.html&#34; target=&#34;_blank&#34;&gt;ggplot2&lt;/a&gt; make constructing them &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/geom_boxplot.html&#34; target=&#34;_blank&#34;&gt;very easy to do&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/sfo-jfk-flights/geom_boxplot-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By default, for each box representing a group, the thick line in the middle of the box is the median, the lower bound of the box is the 25th quantile and the upper bound is the 75th quantile. The whiskers are normally a function of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Interquartile_range&#34; target=&#34;_blank&#34;&gt;interquartile range&lt;/a&gt; (IQR), but if there&amp;rsquo;s enough data, I prefer to use the 5th and 95th quantiles instead.&lt;/p&gt;
&lt;p&gt;If you feed ggplot2&amp;rsquo;s &lt;code&gt;geom_boxplot()&lt;/code&gt; with raw data, it will automatically calculate the corresponding metrics for visualization; however, with big data, the data may not fit into memory and as noted earlier, medians and other quantiles are computationally expensive to calculate. Because we precomputed the quantiles with the query above for every year and month, we can use those explicitly. (The minor downside is that this will not include outliers)&lt;/p&gt;
&lt;p&gt;Additionally for box plots, I like to fill in each box with a different color corresponding to the year in order to better perceive data &lt;a href=&#34;https://en.wikipedia.org/wiki/Seasonality&#34; target=&#34;_blank&#34;&gt;seasonality&lt;/a&gt;. In the case of airline flights, seasonality is more literal: weather has an intuitive impact on flight times and delays, and during winter months there are also holidays which could affect airline logistics.&lt;/p&gt;
&lt;p&gt;The resulting ggplot2 code looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plot &amp;lt;-
ggplot(df_tf,
aes(
x = date,
ymin = q_5,
lower = q_25,
middle = q_50,
upper = q_75,
ymax = q_95,
group = date,
fill = year_factor
)) +
geom_boxplot(stat = &amp;quot;identity&amp;quot;, size = 0.3) +
scale_fill_hue(l = 50, guide = F) +
scale_x_date(date_breaks = &#39;1 year&#39;, date_labels = &amp;quot;%Y&amp;quot;) +
scale_y_continuous(breaks = pretty_breaks(6)) +
labs(
title = &amp;quot;Distribution of Flight Times of Flights From SFO → SEA, by Month&amp;quot;,
subtitle = &amp;quot;via US DoT. Box bounds are 25th/75th percentiles, whiskers are 5th/95th percentiles.&amp;quot;,
y = &#39;Total Elapsed Flight Time (Minutes)&#39;,
fill = &#39;&#39;,
caption = &#39;Max Woolf — minimaxir.com&#39;
) +
theme(axis.title.x = element_blank())
ggsave(&#39;sfo_sea_flight_duration.png&#39;,
plot,
width = 6,
height = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And behold (again)!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/sfo-jfk-flights/sfo_sea_flight_duration.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can see that the boxes do indeed trend upward after 2016, although per-month medians are in flux. The spread is also increasingly slowly over time. But what&amp;rsquo;s interesting is the seasonality; pre-2016, the summer months (the &amp;ldquo;middle&amp;rdquo; of a given color) have a &lt;em&gt;very&lt;/em&gt; significant drop in total time, which doesn&amp;rsquo;t occur as strongly after 2016. Hmm.&lt;/p&gt;
&lt;h2 id=&#34;sfo-and-jfk&#34;&gt;SFO and JFK&lt;/h2&gt;
&lt;p&gt;Since I occasionally fly from San Francisco to New York City, it might be interesting (for completely selfish reasons) to track trends over time for flights between those areas. On the San Francisco side I choose SFO, and for the New York side I choose John F. Kennedy International Airport (JFK), as the data goes back very far for those routes specifically, and I only want to look at a single airport at a time (instead of including other NYC airports such as Newark Liberty International Airport [EWR] and LaGuardia Airport [LGA]) to limit potential data confounders.&lt;/p&gt;
&lt;p&gt;Fortunately, the code and query changes are minimal: in the query, change the target metric to whatever metric you want, and the &lt;code&gt;Origin&lt;/code&gt; and &lt;code&gt;Dest&lt;/code&gt; in the &lt;code&gt;WHERE&lt;/code&gt; clause to what you want, and if you want to calculate metrics other than elapsed time, change the metric in &lt;code&gt;APPROX_QUANTILES&lt;/code&gt; accordingly.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the chart of total elapsed time from SFO → JFK:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/sfo-jfk-flights/sfo_jfk_flight_duration.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And here&amp;rsquo;s the reverse, from JFK → SFO:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/sfo-jfk-flights/jfk_sfo_flight_duration.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unlike the SFO → SEA charts, both charts are relatively flat over the years. However, when looking at seasonality, SFO → JFK dips in the summer and spikes during winter, while JFK → SFO &lt;em&gt;does the complete opposite&lt;/em&gt;: dips during the winter and spikes during the summer, which is similar to the SFO → SEA route. I don&amp;rsquo;t have any guesses what would cause that behavior.&lt;/p&gt;
&lt;p&gt;How about flight speed (calculated via air time divided by distance)? Have new advances in airline technology made planes faster and/or more efficient?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/sfo-jfk-flights/sfo_jfk_flight_speed.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://minimaxir.com/img/sfo-jfk-flights/jfk_sfo_flight_speed.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The expected flight speed for a commercial airplane, &lt;a href=&#34;https://en.wikipedia.org/wiki/Cruise_(aeronautics)&#34; target=&#34;_blank&#34;&gt;per Wikipedia&lt;/a&gt;, is 547-575 mph, so the metrics from SFO pass the sanity check. The metrics from JFK indicate there&amp;rsquo;s about a 20% drop in flight speed potentially due to wind resistance, which makes sense. Month-to-month, the speed trends are inverse to the total elapsed time, which makes sense intuitively as they are strongly negatively correlated.&lt;/p&gt;
&lt;p&gt;Lastly, what about flight departure delays? Are airlines becoming more efficient, or has increased demand caused more congestion?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/sfo-jfk-flights/sfo_jfk_departure_delay.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Wait a second. In this case, massive 2-3 hour flight delays are frequent enough that even just the 95% percentile skews the entire plot. Let&amp;rsquo;s remove the whiskers in order to look at trends more clearly.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/sfo-jfk-flights/sfo_jfk_departure_delay_nowhiskers.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://minimaxir.com/img/sfo-jfk-flights/jfk_sfo_departure_delay_nowhiskers.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A negative delay implies the flight leaves early, so we can conclude on average, flights leave slightly earlier than the stated departure time. Even without the whiskers, we can see major spikes at the 75th percentile level for summer months, and said spikes were especially bad in 2017 for both airports.&lt;/p&gt;
&lt;p&gt;These box plots are only an &lt;a href=&#34;https://en.wikipedia.org/wiki/Exploratory_data_analysis&#34; target=&#34;_blank&#34;&gt;exploratory data analysis&lt;/a&gt;. Determining the &lt;em&gt;cause&lt;/em&gt; of changes in these flight metrics is difficult even for experts (I am definitely not an expert!) and many not even be possible to determine from publicly-available data.&lt;/p&gt;
&lt;p&gt;But there are still other fun things that can be done with the airline flight data, such as faceting airline trends by time and the inclusion of other airports, which is &lt;a href=&#34;https://twitter.com/minimaxir/status/1115261670153048065&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;interesting&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can view the BigQuery queries used to get the data, plus the R and ggplot2 used to create the data visualizations, in &lt;a href=&#34;http://minimaxir.com/notebooks/sfo-jfk-flights/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;. You can also view the images/code used for this post in &lt;a href=&#34;https://github.com/minimaxir/sfo-jfk-flights&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Problems with Predicting Post Performance on Reddit and Other Link Aggregators</title><link>https://minimaxir.com/2018/09/modeling-link-aggregators/</link><pubDate>Mon, 10 Sep 2018 09:15:00 -0700</pubDate><guid>https://minimaxir.com/2018/09/modeling-link-aggregators/</guid><description>
&lt;p&gt;&lt;a href=&#34;https://www.reddit.com&#34; target=&#34;_blank&#34;&gt;Reddit&lt;/a&gt;, &amp;ldquo;the front page of the internet&amp;rdquo; is a link aggregator where anyone can submit links to cool happenings. Over the years, Reddit has expanded from just being a link aggregator, to allowing image and videos, and as of recently, hosting images and videos itself.&lt;/p&gt;
&lt;p&gt;Reddit is broken down into subreddits, where each subreddit represents each own community around a particular interest, like &lt;a href=&#34;https://www.reddit.com/r/aww&#34; target=&#34;_blank&#34;&gt;/r/aww&lt;/a&gt; for pet photos and &lt;a href=&#34;https://www.reddit.com/r/politics/&#34; target=&#34;_blank&#34;&gt;/r/politics&lt;/a&gt; for U.S. politics. The posts on each subreddit are ranked by some function of both time elapsed since the submission was made, and the &lt;em&gt;score&lt;/em&gt; of the submission as determined by upvotes and downvotes from other users.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_aww.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s also an intrinsic pride in having something you&amp;rsquo;re responsible for providing to the community get lots of upvotes (the submitter also earns karma based on received upvotes, although karma is meaningless and doesn&amp;rsquo;t provide any user benefits). But the reality is that even on the largest subreddits, submissions with 1 point (the default score for new submissions) are the most prominent, with some subreddits having &lt;em&gt;over half&lt;/em&gt; of their submissions with only 1 point.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_dist_facet.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The exposure from having a submission go viral on Reddit (especially on larger subreddits) can be valuable especially if its your own original content. As a result, there has been a lot of &lt;a href=&#34;https://www.brandwatch.com/blog/how-to-get-on-the-front-page-of-reddit/&#34; target=&#34;_blank&#34;&gt;analysis&lt;/a&gt;/&lt;a href=&#34;https://www.reddit.com/r/starterpacks/comments/8rkfk9/reddit_front_page_starter_pack/&#34; target=&#34;_blank&#34;&gt;stereotypes&lt;/a&gt; on what techniques to do to help your submission make it to the top of the front page. But almost all claims of &amp;ldquo;cracking&amp;rdquo; the Reddit algorithm are &lt;a href=&#34;https://en.wikipedia.org/wiki/Post_hoc_ergo_propter_hoc&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;post hoc&lt;/em&gt; rationalizations&lt;/a&gt;, attributing success to things like submission timing and title verbiage of a single submission after the fact. The nature of algorithmic feeds inherently leads to a &lt;a href=&#34;https://en.wikipedia.org/wiki/Survivorship_bias&#34; target=&#34;_blank&#34;&gt;survivorship bias&lt;/a&gt;: although users may recognize certain types of posts that appear on the front page, there are many more which follow the same patterns but fail, which makes modeling a successful post very tricky.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve touched on analyzing Reddit post performance &lt;a href=&#34;https://minimaxir.com/2017/06/reddit-deep-learning/&#34; target=&#34;_blank&#34;&gt;before&lt;/a&gt;, but let&amp;rsquo;s give it another look and see if we can drill down on why Reddit posts do and do not do well.&lt;/p&gt;
&lt;h2 id=&#34;submission-timing&#34;&gt;Submission Timing&lt;/h2&gt;
&lt;p&gt;As with many US-based websites, the majority of Reddit users are most active during work hours (9 AM — 5 PM Eastern time weekdays). Most subreddits have submission patterns which fit accordingly.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_subreddit_prop.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But what&amp;rsquo;s interesting are the subreddits which &lt;em&gt;deviate&lt;/em&gt; from that standard. Gaming subreddits (&lt;a href=&#34;https://www.reddit.com/r/DestinyTheGame&#34; target=&#34;_blank&#34;&gt;/r/DestinyTheGame&lt;/a&gt;, &lt;a href=&#34;https://www.reddit.com/r/Overwatch&#34; target=&#34;_blank&#34;&gt;/r/Overwatch&lt;/a&gt;) have short activity after a Tuesday game update/patch, game &lt;em&gt;communication&lt;/em&gt; subreddits (&lt;a href=&#34;https://www.reddit.com/r/Fireteams&#34; target=&#34;_blank&#34;&gt;/r/Fireteams&lt;/a&gt;, &lt;a href=&#34;https://www.reddit.com/r/RocketLeagueExchange&#34; target=&#34;_blank&#34;&gt;/r/RocketLeagueExchange&lt;/a&gt;) are more active &lt;em&gt;outside&lt;/em&gt; of work hours as they assume you are playing the game at the time, and Not-Safe-For-Work subreddits (/r/dirtykikpals, /r/gonewild) are incidentally less active during work hours and more active late-night than other subreddits.&lt;/p&gt;
&lt;p&gt;Whenever you make a submission to Reddit, the submission appears in the subreddit&amp;rsquo;s &lt;code&gt;/new&lt;/code&gt; queue of the most recent submissions, where hopefully kind souls will find your submission and upvote it if it&amp;rsquo;s good.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_new.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, if it falls off the first page of the &lt;code&gt;/new&lt;/code&gt; queue, your submission might be as good as dead. As a result, there&amp;rsquo;s an element of game theory to timing your submission if you want it to not become another 1-point submission. Is it better to submit during peak hours when more users may see the submission before it falls off of &lt;code&gt;/new&lt;/code&gt;? Is it better to submit &lt;em&gt;before&lt;/em&gt; peak usage since there will be less competition, then continue the momentum once it hits the front page?&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a look at the median post performance at each given time slot for top subreddits:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_subreddit_hr_doy.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As the earlier distribution chart implied, the median score is around 1-2 for most subreddits, and that&amp;rsquo;s consistent across all time slots. Some subreddits with higher medians like /r/me_irl do appear to have a &lt;em&gt;slight&lt;/em&gt; benefit when posting before peak activity. When focusing on subreddits with high overall median scores, the difference is more explicit.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_subreddit_highmedian.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Subreddits like /r/PrequelMemes and /r/The_Donald &lt;em&gt;definitely&lt;/em&gt; have better performance on average when made before peak activity! Posting before peak usage &lt;em&gt;does&lt;/em&gt; appear to be a viable strategy, however for the majority of subreddits it doesn&amp;rsquo;t make much of a difference.&lt;/p&gt;
&lt;h2 id=&#34;submission-titles&#34;&gt;Submission Titles&lt;/h2&gt;
&lt;p&gt;Each Reddit subreddit has their own vocabulary and topics of discussion. Let&amp;rsquo;s break down text by subreddit by looking at the 75th percentile for score on posts containing a given two-word phrase:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/reddit_subreddit_topbigrams.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The one trend consistent across all subreddits is the effectiveness of first-person pronouns (&lt;em&gt;I/my&lt;/em&gt;) and original content (&lt;em&gt;fan art&lt;/em&gt;). Other than that, the vocabulary and sentiment for successful posts is very specific to the subreddit and culture is represents; no universal guaranteed-success memes.&lt;/p&gt;
&lt;h2 id=&#34;can-deep-learning-predict-post-performance&#34;&gt;Can Deep Learning Predict Post Performance?&lt;/h2&gt;
&lt;p&gt;Some might think &amp;ldquo;oh hey, this is an arbitrary statistical problem, you can just build an AI to solve it!&amp;rdquo; So, for the sake of argument, I did.&lt;/p&gt;
&lt;p&gt;Instead of using Reddit data for building a deep learning model, we&amp;rsquo;ll use data from &lt;a href=&#34;https://news.ycombinator.com&#34; target=&#34;_blank&#34;&gt;Hacker News&lt;/a&gt;, another link aggregator similar to Reddit with a strong focus on technology and startup entrepreneurship. The distribution of scores on posts, submission timings, upvoting, and front page ranking systems are all the same as on Reddit.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/hn.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The titles on Hacker News submissions are also shorter (80 characters max vs. Reddit&amp;rsquo;s 300 character max) and in concise English (no memes/shitposts allowed), which should help the model learn the title syntax and identify high-impact keywords easier. Like Reddit, the score data is super-skewed with most HN submissions at 1-2 points, and typical model training will quickly converge but try to predict that &lt;em&gt;every&lt;/em&gt; submission has a score of 1, which isn&amp;rsquo;t helpful!&lt;/p&gt;
&lt;p&gt;By constructing a model employing &lt;em&gt;many&lt;/em&gt; deep learning tricks with &lt;a href=&#34;https://keras.io&#34; target=&#34;_blank&#34;&gt;Keras&lt;/a&gt;/&lt;a href=&#34;https://www.tensorflow.org&#34; target=&#34;_blank&#34;&gt;TensorFlow&lt;/a&gt; to prevent model cheating and training on &lt;em&gt;hundreds of thousands&lt;/em&gt; of HN submissions (using post title, day-of-week, hour, and link domain like &lt;code&gt;github.com&lt;/code&gt; as model features), the model does converge and finds some signal among the noise (training R&lt;sup&gt;2&lt;/sup&gt; ~ 0.55 when trained for 50 epochs). However, it fails to offer any valuable predictions on new, unseen posts (test R&lt;sup&gt;2&lt;/sup&gt; &lt;em&gt;&amp;lt; 0.00&lt;/em&gt;) because it falls into the same exact human biases regarding titles: it saw submissions with titles that did very well during training, but can&amp;rsquo;t isolate the random chance why X and Y submissions are similar but X goes viral while Y does not.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/hn_test.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve made the Keras/TensorFlow model training code available in &lt;a href=&#34;https://www.kaggle.com/minimaxir/hacker-news-submission-score-predictor/notebook&#34; target=&#34;_blank&#34;&gt;this Kaggle Notebook&lt;/a&gt; if you want to fork it and try to improve the model.&lt;/p&gt;
&lt;h2 id=&#34;other-potential-modeling-factors&#34;&gt;Other Potential Modeling Factors&lt;/h2&gt;
&lt;p&gt;The deep learning model above makes optimistic assumptions about the underlying data, including that each post behaves independently, and the included features are the sole features which determine the score. These assumptions are questionable.&lt;/p&gt;
&lt;p&gt;The simple model forgoes the content of the submission itself, which is hard to retrieve for hundreds of thousands of data points. On Hacker News that&amp;rsquo;s mostly OK since most submissions are links/articles which accurately correlate to the content, although occasionally there are idiosyncratic short titles which do the opposite. On Reddit, obviously looking at content is necessary for image/video-oriented subreddits, which is hard to gather and analyze at scale.&lt;/p&gt;
&lt;p&gt;A very important concept of post performance is &lt;em&gt;momentum&lt;/em&gt;. A post having a high score is a positive signal in itself, which begets more votes (a famous Reddit problem is brigading from /r/all which can cause submission scores to skyrocket). If the front page of a subreddit has a large number of high-performing posts, they might also suppress posts coming out of the &lt;code&gt;/new&lt;/code&gt; queue because the score threshold is much higher. A simple model may not be able to capture these impacts; the model would need to incorporate the &lt;em&gt;state of the front page&lt;/em&gt; at the time of posting.&lt;/p&gt;
&lt;p&gt;Some also try to manipulate upvotes. Reddit became famous for adding the rule &amp;ldquo;asking for upvotes is a violation of intergalactic law&amp;rdquo; to their &lt;a href=&#34;https://www.reddithelp.com/en/categories/rules-reporting/account-and-community-restrictions/what-constitutes-vote-cheating-or&#34; target=&#34;_blank&#34;&gt;Content Policy&lt;/a&gt;, although some subreddits do it anyway &lt;a href=&#34;https://www.reddit.com/r/TheoryOfReddit/comments/5qqrod/for_years_reddit_told_us_that_saying_upvote_this/&#34; target=&#34;_blank&#34;&gt;without consequence&lt;/a&gt;. On Reddit, obvious spam posts can be downvoted to immediately counteract illicit upvotes. Hacker News has a &lt;a href=&#34;https://news.ycombinator.com/newsfaq.html&#34; target=&#34;_blank&#34;&gt;similar don&amp;rsquo;t-upvote rule&lt;/a&gt;, although there aren&amp;rsquo;t downvotes, just a flagging mechanism which quickly neutralizes spam/misleading posts. In general, there&amp;rsquo;s no &lt;em&gt;legitimate&lt;/em&gt; reason to highlight your own submission immediately after its posted (except for Reddit&amp;rsquo;s AMAs). Fortunately, gaming the system is less impactful on Reddit and Hacker News due to their sheer size and countermeasures, but it&amp;rsquo;s a good example of potential user behavior that makes modeling post performance difficult, and hopefully link aggregators of the future aren&amp;rsquo;t susceptible to such shenanigans.&lt;/p&gt;
&lt;h2 id=&#34;do-we-really-to-predict-post-score&#34;&gt;Do We Really to Predict Post Score?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s say you are submitting original content to Reddit or your own tech project to Hacker News. More points means a higher ranking means more exposure for your link, right? Not exactly. As noted from Reddit/HN screenshots above, the scores of popular submissions are all over the place ranking-wise, having been affected by age penalties.&lt;/p&gt;
&lt;p&gt;In practical terms, from my own purely anecdotal experience, submissions at a top ranking receive &lt;em&gt;substantially&lt;/em&gt; more clickthroughs despite being spatially close on the page to others.&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&amp;hellip;and now traffic at #3.&lt;br&gt;&lt;br&gt;Placement is absurdly important for search engines/social media sites. Difference between #1 and #3 is dramatic. &lt;a href=&#34;https://t.co/nGjWJBx6dU&#34;&gt;pic.twitter.com/nGjWJBx6dU&lt;/a&gt;&lt;/p&gt;&amp;mdash; Max Woolf (@minimaxir) &lt;a href=&#34;https://twitter.com/minimaxir/status/877219784907149316?ref_src=twsrc%5Etfw&#34;&gt;June 20, 2017&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://twitter.com/minimaxir/status/877219784907149316&#34; target=&#34;_blank&#34;&gt;that case&lt;/a&gt;, falling from #1 to #3 &lt;em&gt;immediately halved&lt;/em&gt; the referral traffic coming from Hacker News.&lt;/p&gt;
&lt;p&gt;Therefore, an ideal link aggregator predictive model to maximize clicks should try to predict the &lt;em&gt;rank&lt;/em&gt; of a submission (max rank, average rank over &lt;em&gt;n&lt;/em&gt; period, etc.), not necessarily the score it receives. You could theoretically create a model by making a snapshot of a Reddit subreddit/front page of Hacker News every minute or so which includes the post position at the time of the snapshot. As mentioned earlier, the snapshots can also be used as a model feature to identify whether the front page is active or stale. Unfortunately, snapshots can&amp;rsquo;t be retrieved retroactively, and both storing, processing, and analyzing snapshots at scale is a difficult and &lt;em&gt;expensive&lt;/em&gt; feat of data engineering.&lt;/p&gt;
&lt;p&gt;Presumably Reddit&amp;rsquo;s data scientists would be incorporating submission position as a part of their data analytics and modeling, but after inspecting what&amp;rsquo;s sent to Reddit&amp;rsquo;s servers when you perform an action like upvoting, I wasn&amp;rsquo;t able to find a sent position value when upvoting from the feed: only the post score and post upvote percentage at the time of the action were sent.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/modeling-link-aggregators/chrome.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example, I upvoted the &lt;code&gt;Fact are facts&lt;/code&gt; submission at position #5: we&amp;rsquo;d expect a value between &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt; be sent with the post metadata within the analytics payload, but that&amp;rsquo;s not the case.&lt;/p&gt;
&lt;p&gt;Optimizing ranking instead of a tangible metric or classification accuracy is a relatively underdiscussed field of modern data science (besides &lt;a href=&#34;https://en.wikipedia.org/wiki/Search_engine_optimization&#34; target=&#34;_blank&#34;&gt;SEO&lt;/a&gt; for getting the top spot on a Google search), and it would be interesting to dive deeper into it for other applications.&lt;/p&gt;
&lt;h2 id=&#34;in-the-future&#34;&gt;In the future&lt;/h2&gt;
&lt;p&gt;The moral of this post is that you should not take it personally if a submission fails to hit the front page. It doesn&amp;rsquo;t necessarily mean it&amp;rsquo;s bad. Conversely, if a post does well, don’t assume that similar posts will do just as well. There&amp;rsquo;s a lot of quality content that falls through the cracks due to dumb luck. Fortunately, both Reddit and Hacker News allow reposts, which helps alleviate this particular problem.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s still a lot that can be done to more deterministically predict the behavior of these algorithmic feeds. There&amp;rsquo;s also room to help make these link aggregators more &lt;em&gt;fair&lt;/em&gt;. Unfortunately, there&amp;rsquo;s even more undiscovered ways to game these algorithms, and we&amp;rsquo;ll see how things play out.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can view the BigQuery queries used to get the Reddit and Hacker News data, plus the R and ggplot2 used to create the data visualizations, in &lt;a href=&#34;http://minimaxir.com/notebooks/modeling-link-aggregators/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;. You can also view the images/code used for this post in &lt;a href=&#34;https://github.com/minimaxir/modeling-link-aggregators&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Analyzing IMDb Data The Intended Way, with R and ggplot2</title><link>https://minimaxir.com/2018/07/imdb-data-analysis/</link><pubDate>Mon, 16 Jul 2018 09:45:00 +0000</pubDate><guid>https://minimaxir.com/2018/07/imdb-data-analysis/</guid><description>
&lt;p&gt;&lt;a href=&#34;https://www.imdb.com&#34; target=&#34;_blank&#34;&gt;IMDb&lt;/a&gt;, the Internet Movie Database, has been a popular source for data analysis and visualizations over the years. The combination of user ratings for movies and detailed movie metadata have always been fun to &lt;a href=&#34;http://minimaxir.com/2016/01/movie-revenue-ratings/&#34; target=&#34;_blank&#34;&gt;play with&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/movie-revenue-ratings/box-office-rating-5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are a number of tools to help get IMDb data, such as &lt;a href=&#34;https://github.com/alberanid/imdbpy&#34; target=&#34;_blank&#34;&gt;IMDbPY&lt;/a&gt;, which makes it easy to programmatically scrape IMDb by pretending it&amp;rsquo;s a website user and extracting the relevant data from the page&amp;rsquo;s HTML output. While it &lt;em&gt;works&lt;/em&gt;, web scraping public data is a gray area in terms of legality; many large websites have a Terms of Service which forbids scraping, and can potentially send a DMCA take-down notice to websites redistributing scraped data.&lt;/p&gt;
&lt;p&gt;IMDb has &lt;a href=&#34;https://help.imdb.com/article/imdb/general-information/can-i-use-imdb-data-in-my-software/G5JTRESSHJBBHTGX#&#34; target=&#34;_blank&#34;&gt;data licensing terms&lt;/a&gt; which forbid scraping and require an attribution in the form of a &lt;strong&gt;Information courtesy of IMDb (&lt;a href=&#34;http://www.imdb.com&#34; target=&#34;_blank&#34;&gt;http://www.imdb.com&lt;/a&gt;). Used with permission.&lt;/strong&gt; statement, and has also &lt;a href=&#34;https://www.kaggle.com/tmdb/tmdb-movie-metadata/home&#34; target=&#34;_blank&#34;&gt;DMCAed a Kaggle IMDb dataset&lt;/a&gt; to hone the point.&lt;/p&gt;
&lt;p&gt;However, there is good news! IMDb publishes an &lt;a href=&#34;https://www.imdb.com/interfaces/&#34; target=&#34;_blank&#34;&gt;official dataset&lt;/a&gt; for casual data analysis! And it&amp;rsquo;s now very accessible, just choose a dataset and download (now with no hoops to jump through), and the files are in the standard &lt;a href=&#34;https://en.wikipedia.org/wiki/Tab-separated_values&#34; target=&#34;_blank&#34;&gt;TSV format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/datasets.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The uncompressed files are pretty large; not &amp;ldquo;big data&amp;rdquo; large (it fits into computer memory), but Excel will explode if you try to open them in it. You have to play with the data &lt;em&gt;smartly&lt;/em&gt;, and both &lt;a href=&#34;https://www.r-project.org&#34; target=&#34;_blank&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/index.html&#34; target=&#34;_blank&#34;&gt;ggplot2&lt;/a&gt; have neat tricks to do just that.&lt;/p&gt;
&lt;h2 id=&#34;first-steps&#34;&gt;First Steps&lt;/h2&gt;
&lt;p&gt;R is a popular programming language for statistical analysis. One of the most popular series of external packages is the &lt;code&gt;tidyverse&lt;/code&gt; package, which automatically imports the &lt;code&gt;ggplot2&lt;/code&gt; data visualization library and other useful packages which we&amp;rsquo;ll get to one-by-one. We&amp;rsquo;ll also use &lt;code&gt;scales&lt;/code&gt; which we&amp;rsquo;ll use later for prettier number formatting. First we&amp;rsquo;ll load these packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
library(scales)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we can load a TSV downloaded from IMDb using the &lt;code&gt;read_tsv&lt;/code&gt; function from &lt;code&gt;readr&lt;/code&gt; (a tidyverse package), which does what the name implies, at a much faster speed than base R (+ a couple other parameters to handle data encoding). Let&amp;rsquo;s start with the &lt;code&gt;ratings&lt;/code&gt; file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;df_ratings &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; read_tsv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;title.ratings.tsv&amp;#39;&lt;/span&gt;, na &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;\\N&amp;#34;&lt;/span&gt;, quote &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can preview what&amp;rsquo;s in the loaded data using &lt;code&gt;dplyr&lt;/code&gt; (a tidyverse package), which is what we&amp;rsquo;ll be using to manipulate data for this analysis. dplyr allows you to pipe commands, making it easy to create a sequence of manipulation commands. For now, we&amp;rsquo;ll use &lt;code&gt;head()&lt;/code&gt;, which displays the top few rows of the data frame.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_ratings %&amp;gt;% head()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/ratings.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each of the &lt;strong&gt;873k rows&lt;/strong&gt; corresponds to a single movie, an ID for the movie, its average rating (from 1 to 10), and the number of votes which contribute to that average. Since we have two numeric variables, why not test out ggplot2 by creating a scatterplot mapping them? ggplot2 takes in a data frame and names of columns as aesthetics, then you specify what type of shape to plot (a &amp;ldquo;geom&amp;rdquo;). Passing the plot to &lt;code&gt;ggsave&lt;/code&gt; saves it as a standalone, high-quality data visualization.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot &amp;lt;- ggplot(df_ratings, aes(x = numVotes, y = averageRating)) +
geom_point()
ggsave(&amp;quot;imdb-0.png&amp;quot;, plot, width = 4, height = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-0.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here is nearly &lt;em&gt;1 million&lt;/em&gt; points on a single chart; definitely don&amp;rsquo;t try to do that in Excel! However, it&amp;rsquo;s not a &lt;em&gt;useful&lt;/em&gt; chart since all the points are opaque and we&amp;rsquo;re not sure what the spatial density of points is. One approach to fix this issue is to create a heat map of points, which ggplot can do natively with &lt;code&gt;geom_bin2d&lt;/code&gt;. We can color the heat map with the &lt;a href=&#34;https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html&#34; target=&#34;_blank&#34;&gt;viridis&lt;/a&gt; colorblind-friendly palettes &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/scale_viridis.html&#34; target=&#34;_blank&#34;&gt;just introduced&lt;/a&gt; into ggplot2. We should also tweak the axes; the x-axis should be scaled logarithmically with &lt;code&gt;scale_x_log10&lt;/code&gt; since there are many movies with high numbers of votes and we can format those numbers with the &lt;code&gt;comma&lt;/code&gt; function from the &lt;code&gt;scales&lt;/code&gt; package (we can format the scale with &lt;code&gt;comma&lt;/code&gt; too). For the y-axis, we can add explicit number breaks for each rating; R can do this neatly by setting the breaks to &lt;code&gt;1:10&lt;/code&gt;. Putting it all together:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot &amp;lt;- ggplot(df_ratings, aes(x = numVotes, y = averageRating)) +
geom_bin2d() +
scale_x_log10(labels = comma) +
scale_y_continuous(breaks = 1:10) +
scale_fill_viridis_c(labels = comma)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not bad, although it unfortunately confirms that IMDb follows a &lt;a href=&#34;https://tvtropes.org/pmwiki/pmwiki.php/Main/FourPointScale&#34; target=&#34;_blank&#34;&gt;Four Point Scale&lt;/a&gt; where average ratings tend to fall between 6 — 9.&lt;/p&gt;
&lt;h2 id=&#34;mapping-movies-to-ratings&#34;&gt;Mapping Movies to Ratings&lt;/h2&gt;
&lt;p&gt;You may be asking &amp;ldquo;which ratings correspond to which movies?&amp;rdquo; That&amp;rsquo;s what the &lt;code&gt;tconst&lt;/code&gt; field is for. But first, let&amp;rsquo;s load the title data from &lt;code&gt;title.basics.tsv&lt;/code&gt; into &lt;code&gt;df_basics&lt;/code&gt; and take a look as before.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_basics &amp;lt;- read_tsv(&#39;title.basics.tsv&#39;, na = &amp;quot;\\N&amp;quot;, quote = &#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/basics1.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/basics2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have some neat movie metadata. Notably, this table has a &lt;code&gt;tconst&lt;/code&gt; field as well. Therefore, we can &lt;em&gt;join&lt;/em&gt; the two tables together, adding the movie information to the corresponding row in the rating table (in this case, a left join is more appropriate than an inner/full join)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_ratings &amp;lt;- df_ratings %&amp;gt;% left_join(df_basics)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Runtime minutes sounds interesting. Could there be a relationship between the length of a movie and its average rating on IMDb? Let&amp;rsquo;s make a heat map plot again, but with a few tweaks. With the new metadata, we can &lt;code&gt;filter&lt;/code&gt; the table to remove bad points; let&amp;rsquo;s keep movies only (as IMDb data also contains &lt;em&gt;television show data&lt;/em&gt;), with a runtime &amp;lt; 3 hours, and which have received atleast 10 votes by users to remove extraneous movies). X-axis should be tweaked to display the minutes-values in hours. The fill viridis palette can be changed to another one in the family (I personally like &lt;code&gt;inferno&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;More importantly, let&amp;rsquo;s discuss plot theming. If you want a minimalistic theme, add a &lt;code&gt;theme_minimal&lt;/code&gt; to the plot, and you can pass a &lt;code&gt;base_family&lt;/code&gt; to change the default font on the plot and a &lt;code&gt;base_size&lt;/code&gt; to change the font size. The &lt;code&gt;labs&lt;/code&gt; function lets you add labels to the plot (which you should &lt;em&gt;always&lt;/em&gt; do); you have your &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, and &lt;code&gt;y&lt;/code&gt; parameters, but you can also add a &lt;code&gt;subtitle&lt;/code&gt;, a &lt;code&gt;caption&lt;/code&gt; for attribution, and a &lt;code&gt;color&lt;/code&gt;/&lt;code&gt;fill&lt;/code&gt; to name the scale. Putting it all together:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot &amp;lt;- ggplot(df_ratings %&amp;gt;% filter(runtimeMinutes &amp;lt; 180, titleType == &amp;quot;movie&amp;quot;, numVotes &amp;gt;= 10), aes(x = runtimeMinutes, y = averageRating)) +
geom_bin2d() +
scale_x_continuous(breaks = seq(0, 180, 60), labels = 0:3) +
scale_y_continuous(breaks = 0:10) +
scale_fill_viridis_c(option = &amp;quot;inferno&amp;quot;, labels = comma) +
theme_minimal(base_family = &amp;quot;Source Sans Pro&amp;quot;, base_size = 8) +
labs(title = &amp;quot;Relationship between Movie Runtime and Average Mobie Rating&amp;quot;,
subtitle = &amp;quot;Data from IMDb retrieved July 4th, 2018&amp;quot;,
x = &amp;quot;Runtime (Hours)&amp;quot;,
y = &amp;quot;Average User Rating&amp;quot;,
caption = &amp;quot;Max Woolf — minimaxir.com&amp;quot;,
fill = &amp;quot;# Movies&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-2b.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that&amp;rsquo;s pretty nice-looking for only a few lines of code! Albeit unhelpful, as there doesn&amp;rsquo;t appear to be a correlation.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Note: for the rest of this post, the theming/labels code will be omitted for convenience)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;How about movie ratings vs. the year the movie was made? It&amp;rsquo;s a similar plot code-wise to the one above (one perk about &lt;code&gt;ggplot2&lt;/code&gt; is that there&amp;rsquo;s no shame in reusing chart code!), but we can add a &lt;code&gt;geom_smooth&lt;/code&gt;, which adds a nonparametric trendline with confidence bands for the trend; since we have a large amount of data, the bands are very tight. We can also fix the problem of &amp;ldquo;empty&amp;rdquo; bins by setting the color fill scale to logarithmic scaling. And since we&amp;rsquo;re adding a black trendline, let&amp;rsquo;s change the viridis palette to &lt;code&gt;plasma&lt;/code&gt; for better contrast.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot &amp;lt;- ggplot(df_ratings %&amp;gt;% filter(titleType == &amp;quot;movie&amp;quot;, numVotes &amp;gt;= 10), aes(x = startYear, y = averageRating)) +
geom_bin2d() +
geom_smooth(color=&amp;quot;black&amp;quot;) +
scale_x_continuous() +
scale_y_continuous(breaks = 1:10) +
scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;, labels = comma, trans = &#39;log10&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, this trend hasn&amp;rsquo;t changed much either, although the presence of average ratings outside the Four Point Scale has increased over time.&lt;/p&gt;
&lt;h2 id=&#34;mapping-lead-actors-to-movies&#34;&gt;Mapping Lead Actors to Movies&lt;/h2&gt;
&lt;p&gt;Now that we have a handle on working with the IMDb data, let&amp;rsquo;s try playing with the larger datasets. Since they take up a lot of computer memory, we only want to persist data we actually might use. After looking at the schema provided with the official datasets, the only really useful metadata about the actors is their birth year, so let&amp;rsquo;s load that, but only keep both actors/actresses (using the fast &lt;code&gt;str_detect&lt;/code&gt; function from &lt;code&gt;stringr&lt;/code&gt;, another tidyverse package) and the relevant fields.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_actors &amp;lt;- read_tsv(&#39;name.basics.tsv&#39;, na = &amp;quot;\\N&amp;quot;, quote = &#39;&#39;) %&amp;gt;%
filter(str_detect(primaryProfession, &amp;quot;actor|actress&amp;quot;)) %&amp;gt;%
select(nconst, primaryName, birthYear)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/actor.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The principals dataset, the large 1.28GB TSV, is the most interesting. It&amp;rsquo;s an unnested list of the credited persons in each movie, with an &lt;code&gt;ordering&lt;/code&gt; indicating their rank (where &lt;code&gt;1&lt;/code&gt; means first, &lt;code&gt;2&lt;/code&gt; means second, etc.).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/principals.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For this analysis, let&amp;rsquo;s only look at the &lt;strong&gt;lead actors/actresses&lt;/strong&gt;; specifically, for each movie (identified by the &lt;code&gt;tconst&lt;/code&gt; value), filter the dataset to where the &lt;code&gt;ordering&lt;/code&gt; value is the lowest (in this case, the person at rank &lt;code&gt;1&lt;/code&gt; may not necessarily be an actor/actress).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_principals &amp;lt;- read_tsv(&#39;title.principals.tsv&#39;, na = &amp;quot;\\N&amp;quot;, quote = &#39;&#39;) %&amp;gt;%
filter(str_detect(category, &amp;quot;actor|actress&amp;quot;)) %&amp;gt;%
select(tconst, ordering, nconst, category) %&amp;gt;%
group_by(tconst) %&amp;gt;%
filter(ordering == min(ordering))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both datasets have a &lt;code&gt;nconst&lt;/code&gt; field, so let&amp;rsquo;s join them together. And then join &lt;em&gt;that&lt;/em&gt; to the ratings table earlier via &lt;code&gt;tconst&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_principals &amp;lt;- df_principals %&amp;gt;% left_join(df_actors)
df_ratings &amp;lt;- df_ratings %&amp;gt;% left_join(df_principals)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a fully denormalized dataset in &lt;code&gt;df_ratings&lt;/code&gt;. Since we now have the movie release year and the birth year of the lead actor, we can now infer &lt;em&gt;the age of the lead actor at the movie release&lt;/em&gt;. With that goal, filter out the data on the criteria we&amp;rsquo;ve used for earlier data visualizations, plus only keeping rows which have an actor&amp;rsquo;s birth year.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_ratings_movies &amp;lt;- df_ratings %&amp;gt;%
filter(titleType == &amp;quot;movie&amp;quot;, !is.na(birthYear), numVotes &amp;gt;= 10) %&amp;gt;%
mutate(age_lead = startYear - birthYear)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/denorm1.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/denorm2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;plotting-ages&#34;&gt;Plotting Ages&lt;/h2&gt;
&lt;p&gt;Age discrimination in movie casting has been a recurring issue in Hollywood; in fact, in 2017 &lt;a href=&#34;https://www.hollywoodreporter.com/thr-esq/judge-pauses-enforcement-imdb-age-censorship-law-978797&#34; target=&#34;_blank&#34;&gt;a law was signed&lt;/a&gt; to force IMDb to remove an actor&amp;rsquo;s age upon request, which in February 2018 was &lt;a href=&#34;https://www.hollywoodreporter.com/thr-esq/californias-imdb-age-censorship-law-declared-unconstitutional-1086540&#34; target=&#34;_blank&#34;&gt;ruled to be unconstitutional&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Have the ages of movie leads changed over time? For this example, we&amp;rsquo;ll use a &lt;a href=&#34;https://ggplot2.tidyverse.org/reference/geom_ribbon.html&#34; target=&#34;_blank&#34;&gt;ribbon plot&lt;/a&gt; to plot the ranges of ages of movie leads. A simple way to do that is, for each year, calculate the 25th &lt;a href=&#34;https://en.wikipedia.org/wiki/Percentile&#34; target=&#34;_blank&#34;&gt;percentile&lt;/a&gt; of the ages, the 50th percentile (i.e. the median), and the 75th percentile, where the 25th and 75th percentiles are the ribbon bounds and the line represents the median.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_actor_ages &amp;lt;- df_ratings_movies %&amp;gt;%
group_by(startYear) %&amp;gt;%
summarize(low_age = quantile(age_lead, 0.25, na.rm=T),
med_age = quantile(age_lead, 0.50, na.rm=T),
high_age = quantile(age_lead, 0.75, na.rm=T))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting it with ggplot2 is surprisingly simple, although you need to use different y aesthetics for the ribbon and the overlapping line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot &amp;lt;- ggplot(df_actor_ages %&amp;gt;% filter(startYear &amp;gt;= 1920) , aes(x = startYear)) +
geom_ribbon(aes(ymin = low_age, ymax = high_age), alpha = 0.2) +
geom_line(aes(y = med_age))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-8.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Turns out that in the 2000&amp;rsquo;s, the median age of lead actors started to &lt;em&gt;increase&lt;/em&gt;? Both the upper and lower bounds increased too. That doesn&amp;rsquo;t coalesce with the age discrimination complaints.&lt;/p&gt;
&lt;p&gt;Another aspect of these complaints is gender, as female actresses tend to be younger than male actors. Thanks to the magic of ggplot2 and dplyr, separating actors/actresses is relatively simple: add gender (encoded in &lt;code&gt;category&lt;/code&gt;) as a grouping variable, add it as a color/fill aesthetic in ggplot, and set colors appropriately (I recommend the &lt;a href=&#34;http://colorbrewer2.org/&#34; target=&#34;_blank&#34;&gt;ColorBrewer&lt;/a&gt; qualitative palettes for categorical variables).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_actor_ages_lead &amp;lt;- df_ratings_movies %&amp;gt;%
group_by(startYear, category) %&amp;gt;%
summarize(low_age = quantile(age_lead, 0.25, na.rm = T),
med_age = quantile(age_lead, 0.50, na.rm = T),
high_age = quantile(age_lead, 0.75, na.rm = T))
plot &amp;lt;- ggplot(df_actor_ages_lead %&amp;gt;% filter(startYear &amp;gt;= 1920), aes(x = startYear, fill = category, color = category)) +
geom_ribbon(aes(ymin = low_age, ymax = high_age), alpha = 0.2) +
geom_line(aes(y = med_age)) +
scale_fill_brewer(palette = &amp;quot;Set1&amp;quot;) +
scale_color_brewer(palette = &amp;quot;Set1&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-9.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s about a 10-year gap between the ages of male and female leads, and the gap doesn&amp;rsquo;t change overtime. But both start to rise at the same time.&lt;/p&gt;
&lt;p&gt;One possible explanation for this behavior is actor reuse: if Hollywood keeps casting the same actor/actresses, by construction the ages of the leads will start to steadily increase. Let&amp;rsquo;s verify that: with our list of movies and their lead actors, for each lead actor, order all their movies by release year, and add a ranking for the #th time that actor has been a lead actor. This is possible through the use of &lt;code&gt;row_number&lt;/code&gt; in dplyr, and &lt;a href=&#34;https://cran.r-project.org/web/packages/dplyr/vignettes/window-functions.html&#34; target=&#34;_blank&#34;&gt;window functions&lt;/a&gt; like &lt;code&gt;row_number&lt;/code&gt; are data science&amp;rsquo;s most useful secret.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_ratings_movies_nth &amp;lt;- df_ratings_movies %&amp;gt;%
group_by(nconst) %&amp;gt;%
arrange(startYear) %&amp;gt;%
mutate(nth_lead = row_number())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/row_number.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One more ribbon plot later (w/ same code as above + custom y-axis breaks):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/imdb-data-analysis/imdb-12.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Huh. The median and upper-bound #th time has &lt;em&gt;dropped&lt;/em&gt; over time? Hollywood has been promoting more newcomers as leads? That&amp;rsquo;s not what I expected!&lt;/p&gt;
&lt;p&gt;More work definitely needs to be done in this area. In the meantime, the official IMDb datasets are a lot more robust than I thought they would be! And I only used a fraction of the datasets; the rest tie into TV shows, which are a bit messier. Hopefully you&amp;rsquo;ve seen a good taste of the power of R and ggplot2 for playing with big-but-not-big data!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can view the R and ggplot used to create the data visualizations in &lt;a href=&#34;http://minimaxir.com/notebooks/imdb-data-analysis/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;, which includes many visualizations not used in this post. You can also view the images/code used for this post in &lt;a href=&#34;https://github.com/minimaxir/imdb-data-analysis&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Visualizing One Million NCAA Basketball Shots</title><link>https://minimaxir.com/2018/03/basketball-shots/</link><pubDate>Mon, 19 Mar 2018 09:20:00 -0700</pubDate><guid>https://minimaxir.com/2018/03/basketball-shots/</guid><description>
&lt;p&gt;So &lt;a href=&#34;https://www.ncaa.com/march-madness&#34; target=&#34;_blank&#34;&gt;March Madness&lt;/a&gt; is happing right now. In celebration, &lt;a href=&#34;https://www.google.com&#34; target=&#34;_blank&#34;&gt;Google&lt;/a&gt; uploaded &lt;a href=&#34;https://console.cloud.google.com/launcher/details/ncaa-bb-public/ncaa-basketball&#34; target=&#34;_blank&#34;&gt;massive basketball datasets&lt;/a&gt; from the &lt;a href=&#34;https://www.ncaa.com&#34; target=&#34;_blank&#34;&gt;NCAA&lt;/a&gt; and &lt;a href=&#34;https://www.sportradar.com/&#34; target=&#34;_blank&#34;&gt;Sportradar&lt;/a&gt; to &lt;a href=&#34;https://cloud.google.com/bigquery/&#34; target=&#34;_blank&#34;&gt;BigQuery&lt;/a&gt; for anyone to query and experiment. After learning that the &lt;a href=&#34;https://www.reddit.com/r/bigquery/comments/82nz17/dataset_statistics_for_ncaa_mens_and_womens/&#34; target=&#34;_blank&#34;&gt;dataset had location data&lt;/a&gt; on where basketball shots were made on the court, I played with it and a couple hours later, I created a decent heat map data visualization. The next day, I &lt;a href=&#34;https://www.reddit.com/r/dataisbeautiful/comments/837qnu/heat_map_of_1058383_basketball_shots_from_ncaa/&#34; target=&#34;_blank&#34;&gt;posted it&lt;/a&gt; to Reddit&amp;rsquo;s &lt;a href=&#34;https://www.reddit.com/r/dataisbeautiful&#34; target=&#34;_blank&#34;&gt;/r/dataisbeautiful subreddit&lt;/a&gt; where it earned about &lt;strong&gt;40,000 upvotes&lt;/strong&gt;. (!?)&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s dig a little deeper. Although visualizing basketball shots has been &lt;a href=&#34;http://www.slate.com/blogs/browbeat/2012/03/06/mapping_the_nba_how_geography_can_teach_players_where_to_shoot.html&#34; target=&#34;_blank&#34;&gt;done&lt;/a&gt; &lt;a href=&#34;http://toddwschneider.com/posts/ballr-interactive-nba-shot-charts-with-r-and-shiny/&#34; target=&#34;_blank&#34;&gt;before&lt;/a&gt;, this time we have access to an order of magnitude more public data to do some really cool stuff.&lt;/p&gt;
&lt;h2 id=&#34;full-court&#34;&gt;Full Court&lt;/h2&gt;
&lt;p&gt;The Sportradar play-by-play table on BigQuery &lt;code&gt;mbb_pbp_sr&lt;/code&gt; has more than 1 million NCAA men&amp;rsquo;s basketball shots since the 2013-2014 season, with more being added now during March Madness. Here&amp;rsquo;s a heat map of the locations where those shots were made on the full basketball court:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_attempts_unlog.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can clearly see at a glance that the majority of shots are made right in front of the basket. For 3-point shots, the center and the corners have higher numbers of shot attempts than the other areas. But not much else since the data is so spatially skewed: setting the bin color scale to logarithmic makes trends more apparent and helps things go viral on Reddit.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_attempts.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now there&amp;rsquo;s more going on here: shot behavior is clearly symmetric on each side of the court, and there&amp;rsquo;s a small gap between the 3-point line and where 3-pt shots are typically made, likely to ensure that it it&amp;rsquo;s not accidentally ruled as a 2-pt shot.&lt;/p&gt;
&lt;p&gt;How likely is it to score a shot from a given spot? Are certain spots better than others?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_perc_success.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Surprisingly, shot accuracy is about &lt;em&gt;equal&lt;/em&gt; from anywhere within typical shooting distance, except directly in front of the basket where it&amp;rsquo;s much higher. What is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Expected_value&#34; target=&#34;_blank&#34;&gt;expected value&lt;/a&gt; of a shot at a given position: that is, how many points on average will they earn for their team?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_avg_points.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The average points earned for 3-pt shots is about 1.5x higher than many 2-pt shot locations in the inner court due to the equal accuracy, but locations next to the basket have an even higher expected value. Perhaps the accuracy of shots close to the basket is higher (&amp;gt;1.5x) than 3-pt shots and outweighs the lower point value?&lt;/p&gt;
&lt;p&gt;Since both sides of the court are indeed the same, we can combine the two sides and just plot a half-court instead. (Cross-court shots, which many Redditors &lt;a href=&#34;https://www.reddit.com/r/dataisugly/comments/839rax/basketball_heat_map_shows_an_impressive_number_of/&#34; target=&#34;_blank&#34;&gt;argued&lt;/a&gt; that they invalidated my visualizations above, constitute only &lt;em&gt;0.16%&lt;/em&gt; of the basketball shots in the dataset, so they can be safely removed as outliers).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_attempts_half_log.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are still a few oddities, such as shots being made &lt;em&gt;behind&lt;/em&gt; the basket. Let&amp;rsquo;s drill down a bit.&lt;/p&gt;
&lt;h2 id=&#34;focusing-on-basketball-shot-type&#34;&gt;Focusing on Basketball Shot Type&lt;/h2&gt;
&lt;p&gt;The Sportradar dataset classifies a shot as one of 5 major types: a &lt;strong&gt;jump shot&lt;/strong&gt; where the player jumps-and-throws the basketball, a &lt;strong&gt;layup&lt;/strong&gt; where the player runs down the field toward the basket and throws a one-handed shot, a &lt;strong&gt;dunk&lt;/strong&gt; where the player slams the ball into the basket (looking cool in the process), a &lt;strong&gt;hook shot&lt;/strong&gt; where the player close to the basket throws the ball with a hook motion, and a &lt;strong&gt;tip shot&lt;/strong&gt; where the player intercepts a basket rebound at the tip of the basket and pushes it in.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_prop_attempts.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, the most frequent types of shots are the less flashy, more practical jump shots and layups. But is a certain type of shot &amp;ldquo;better?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_perc.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Layups are safer than jump shots, but dunks are the most accurate of all the types (however, players likely wouldn&amp;rsquo;t attempt a dunk unless they knew it would be successful). The accuracy of layups and other close-to-basket shots is indeed more than 1.5x better than the jump shots of 3-pt shots, which explains the expected value behavior above.&lt;/p&gt;
&lt;p&gt;Plotting the heat maps for each type of shot offers more insight into how they work:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_attempts_half_types_log.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;They&amp;rsquo;re wildly different heat maps which match the shot type descriptions above, but show we&amp;rsquo;ll need to separate data visualizations by type to accurately see trends.&lt;/p&gt;
&lt;h2 id=&#34;impact-of-game-elapsed-time-at-time-of-shot&#34;&gt;Impact of Game Elapsed Time At Time of Shot&lt;/h2&gt;
&lt;p&gt;A NCAA basketball game lasts for 40 minutes total (2 halves of 20 minutes each), with the possibility of overtime. The &lt;a href=&#34;https://bigquery.cloud.google.com/savedquery/4194148158:3359d86507814fb19a5997a770456baa&#34; target=&#34;_blank&#34;&gt;example BigQuery&lt;/a&gt; for the NCAA-provided data compares the percentage of 3-point shots made during the first 35 minutes of the game versus the last 5 minutes: at the end of the game, accuracy was lower by 4 percentage points (31.2% vs. 35.1%). It might be interesting to facet these visualizations by the elapsed time of the game to see if there are any behavioral changes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_prop_type_elapsed.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There isn&amp;rsquo;t much difference between the proportions within a given half, but there is a difference between the first half and the second half, where the second half has fewer jump shots and more aggressive layups and dunks. After looking at shot success percentage:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_perc_success_type_elapsed.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The jump shot accuracy loss at the end of the game with Sportradar data is similar to that of the NCAA data, which is a good sanity check (but it&amp;rsquo;s odd that the accuracy drop only happens in the last 5 minutes and not elsewhere in the 2nd half). Layup accuracy increases in the second half with the number of layups.&lt;/p&gt;
&lt;p&gt;We can also visualize heat maps for each combo of shot type with time elapsed bucket, but given the results above, the changes in behavior over time may not be very perceptible.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_count_attempts_half_interval_log.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;impact-of-winning-losing-before-shot&#34;&gt;Impact of Winning/Losing Before Shot&lt;/h2&gt;
&lt;p&gt;Another theory worth exploring is determining if there is any difference whether a team is winning or losing when they make their shot (technically, when the delta between the team score and the other team score is positive for winning teams, negative for losing teams, or 0 if tied). Are players more relaxed when they have a lead? Are players more prone to making mistakes when losing?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_prop_type_score.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Layups are the same across all buckets, but for teams that are winning, there are fewer jump shots and &lt;strong&gt;more dunkin&amp;rsquo; action&lt;/strong&gt; (nearly double the dunks!). However, the accuracy chart illustrates an issue:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/basketball-shots/ncaa_types_perc_success_type_score.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Accuracy for most types of shots is much better for teams that are winning&amp;hellip;which may be the &lt;em&gt;reason&lt;/em&gt; they&amp;rsquo;re winning. More research can be done in this area.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I fully admit I am not a basketball expert. But playing around with this data was a fun way to get a new perspective on how collegiate basketball games work. There&amp;rsquo;s a lot more work that can be done with big basketball data and game strategy; the NCAA-provided data doesn&amp;rsquo;t have location data, but it does have &lt;strong&gt;6x more shots&lt;/strong&gt;, which will be very helpful for further fun in this area.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can view the R code, ggplot2 code, and BigQueries used to create the data visualizations in &lt;a href=&#34;http://minimaxir.com/notebooks/basketball-shots/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;. You can also view the images/code used for this post in &lt;a href=&#34;https://github.com/minimaxir/ncaa-basketball&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Special thanks to Ewen Gallic for his implementation of a &lt;a href=&#34;http://egallic.fr/en/drawing-a-basketball-court-with-r/&#34; target=&#34;_blank&#34;&gt;basketball court in ggplot2&lt;/a&gt;, which saved me a lot of time!&lt;/em&gt;&lt;/p&gt;</description></item><item><title>A Visual Overview of Stack Overflow&#39;s Question Tags</title><link>https://minimaxir.com/2018/02/stack-overflow-questions/</link><pubDate>Fri, 09 Feb 2018 09:00:00 -0700</pubDate><guid>https://minimaxir.com/2018/02/stack-overflow-questions/</guid><description>
&lt;p&gt;&lt;a href=&#34;https://stackoverflow.com&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt; is the most popular contemporary knowledge base for programming questions. But most interact with the site by Googling a programming question and getting a top result that links to SO. There isn&amp;rsquo;t as much discussion about actually &lt;em&gt;asking&lt;/em&gt; questions on the site.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/python_last_list.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I &lt;em&gt;could&lt;/em&gt; use &lt;a href=&#34;https://stackoverflow.com/users/9314418/minimaxir?tab=profile&#34; target=&#34;_blank&#34;&gt;my Stack Overflow account&lt;/a&gt; and test out the process of creating a question, but &lt;del&gt;I already know everything about programming&lt;/del&gt; there may be another way to learn how SO works. Stack Overflow &lt;a href=&#34;https://archive.org/details/stackexchange&#34; target=&#34;_blank&#34;&gt;releases an archive&lt;/a&gt; of all questions on the site every 3 months, and this archive is &lt;a href=&#34;https://cloud.google.com/bigquery/public-data/stackoverflow&#34; target=&#34;_blank&#34;&gt;syndicated to BigQuery&lt;/a&gt;, making it trivial to retrieve and analyze the millions of SO questions over the years. Even though (now-former) Stack Overflow data scientist &lt;a href=&#34;https://twitter.com/drob&#34; target=&#34;_blank&#34;&gt;David Robinson&lt;/a&gt; has written &lt;a href=&#34;https://stackoverflow.blog/2017/09/06/incredible-growth-python/&#34; target=&#34;_blank&#34;&gt;many&lt;/a&gt; &lt;a href=&#34;https://stackoverflow.blog/2017/04/19/programming-languages-used-late-night/&#34; target=&#34;_blank&#34;&gt;interesting&lt;/a&gt; blog posts for Stack Overflow with their data, I figured why not give it a try.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/python_last_list_answer.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Unlike social media sites like &lt;a href=&#34;https://twitter.com&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt; and &lt;a href=&#34;https://www.reddit.com&#34; target=&#34;_blank&#34;&gt;Reddit&lt;/a&gt; where the majority of traffic is driven within the first days after something is posted, posts on evergreen content sources like Stack Overflow are still relevant many years later. In fact, the traffic to Stack Overflow for most of 2017 (derived by finding the difference between question view counts from archive snapshots) is approximately uniform across question age, with a slight bias toward older content.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/so_overview.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In 2017, Stack Overflow received about 40k-50k new questions each week, an impressive feat:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/weekly_count.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For the rest of this post, we&amp;rsquo;ll only look at questions made in 2017 (until December; about 2.3 million questions total) in order to get a sense of the current development landscape, and what&amp;rsquo;s to come in the future. But what types of questions are they?&lt;/p&gt;
&lt;h2 id=&#34;tag-breakdown&#34;&gt;Tag Breakdown&lt;/h2&gt;
&lt;p&gt;All questions on Stack Overflow are required to have atleast 1 tag indicating the programming language/technologies involved with the question, and can have up to 5 tags. In the example &amp;ldquo;how do you get the last element of a list in Python&amp;rdquo; &lt;a href=&#34;https://stackoverflow.com/questions/930397/getting-the-last-element-of-a-list-in-python&#34; target=&#34;_blank&#34;&gt;question&lt;/a&gt; above, the tags are &lt;code&gt;python&lt;/code&gt;, &lt;code&gt;list&lt;/code&gt;, and &lt;code&gt;indexing&lt;/code&gt;. In 2017, most of new questions had 2-3 tags. (i.e. people aren&amp;rsquo;t &lt;a href=&#34;http://minimaxir.com/2014/03/hashtag-tag/&#34; target=&#34;_blank&#34;&gt;tag spamming&lt;/a&gt; like on &lt;a href=&#34;https://www.instagram.com/?hl=en&#34; target=&#34;_blank&#34;&gt;Instagram&lt;/a&gt; for maximum exposure).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/so_tag_breakdown.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In theory, tag spamming might make a question more likely to be answered; however for all tag counts, the proportion of questions with accepted answer (the green checkmark) is &lt;strong&gt;36-39%&lt;/strong&gt;, so there&amp;rsquo;s not much practical benefit from minmaxing tag counts. Which types of tagged questions are most likely to be answered?&lt;/p&gt;
&lt;p&gt;First, here&amp;rsquo;s the breakdown of the top 40 tags on Stack Overflow, by the number of new questions containing that tag for each month throughout 2017. This can give a sense of each technology&amp;rsquo;s growth/decline throughout the year.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/monthly_count_tag.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both new web development technologies like &lt;code&gt;reactjs&lt;/code&gt; and &lt;code&gt;typescript&lt;/code&gt; and data science tools like &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;r&lt;/code&gt; are trending upward.&lt;/p&gt;
&lt;p&gt;For the Top 1,000 tags, here are the top 30 tags by the proportion of questions which received an acceptable answer:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/acceptable_answer_top_30.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In contrast, here are the bottom 30 out of the Top 1,000:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/acceptable_answer_bottom_30.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The top tags are newer, sexier technologies like &lt;code&gt;rust&lt;/code&gt; and &lt;code&gt;dart&lt;/code&gt;, with another strong hint of data science tooling with &lt;code&gt;dplyr&lt;/code&gt; (which I used to aggregate the data for this post!) and &lt;code&gt;data.table&lt;/code&gt;. In contrast, the bottom tags are less sexy and more corporate like &lt;code&gt;salesforce&lt;/code&gt;, &lt;code&gt;drupal&lt;/code&gt;, and &lt;code&gt;sharepoint-2013&lt;/code&gt; (that&amp;rsquo;s why consultants who specialize in these technologies can get paid very well!).&lt;/p&gt;
&lt;p&gt;It should be noted these two charts do not necessarily imply that one technology is &amp;ldquo;better&amp;rdquo; than another, and the difference in answer rates may be due to question difficulty and the number of people skilled in the tech available that can answer it effectively.&lt;/p&gt;
&lt;p&gt;The timing when questions are asked might vary by tag. Per &lt;a href=&#34;https://stackoverflow.blog/2017/04/19/programming-languages-used-late-night/&#34; target=&#34;_blank&#34;&gt;a Stack Overflow analysis&lt;/a&gt;, people typically ask questions during the 9 AM - 5 PM work hours (although in my case, I cannot easily adjust for the time zone of the asker). How does this data fare?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/monthly_count_hr_doy.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This visualization is a bit weird. I adjusted the times to the Eastern time since internet activity for U.S.-based websites tends to revolve around that time zone. But for most technologies, the peak question-asking times are well before 9 AM to 5 PM: do those technologies correspond more to greater use in Europe and Asia? (In contrast, data-oriented technologies like &lt;code&gt;r&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;excel&lt;/code&gt; &lt;em&gt;do&lt;/em&gt; peak during the 9-5 block).&lt;/p&gt;
&lt;h2 id=&#34;how-easy-is-it-to-get-an-answer-by-tag&#34;&gt;How easy is it to get an answer by tag?&lt;/h2&gt;
&lt;p&gt;Stack Overflow caters the homepage toward the logged-in user&amp;rsquo;s recommended tags. Therefore, it&amp;rsquo;s not a surprise that the distribution of view counts on 2017 questions for each tag are very similar, although there is a slight edge toward the new &amp;ldquo;hip&amp;rdquo; technologies like &lt;code&gt;typescript&lt;/code&gt;, &lt;code&gt;spring&lt;/code&gt;, and &lt;code&gt;swift&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/views_boxplot_tag.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At the least, the distribution ensures that atleast 10 people see your question for these popular topics, which is nifty when you consider posts on Twitter and Reddit can die without any visibility at all. But will they provide an acceptable answer?&lt;/p&gt;
&lt;p&gt;The time it takes to get an acceptable answer also varies significantly by tag:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/acceptable_answer_density.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A median time of &lt;em&gt;15 minutes&lt;/em&gt; for tags like &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;arrays&lt;/code&gt; is pretty impressive! And even in the worst case scenario for these popular tags, the median is only a couple hours, much lower than I thought it would be.&lt;/p&gt;
&lt;h2 id=&#34;the-relationship-between-tags&#34;&gt;The Relationship Between Tags&lt;/h2&gt;
&lt;p&gt;As one would expect, the types of questions asked for each tag are much different. Here&amp;rsquo;s a wordcloud for each of the tags, quantifying the words most frequently used in the questions on those tags:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/so_tag_wordcloud.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notably, each word cloud is significantly different from reach other, even when technologies are related (also surprisingly true in the case of &lt;code&gt;angular&lt;/code&gt; and &lt;code&gt;angularjs&lt;/code&gt;!).&lt;/p&gt;
&lt;p&gt;How are the tags related anyways? We can calculate an &lt;a href=&#34;https://en.wikipedia.org/wiki/Adjacency_matrix&#34; target=&#34;_blank&#34;&gt;adjacency matrix&lt;/a&gt; of the tag pairs in the questions to see which tags are related:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/so_tag_adjacency.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking down a given row/column, you can see which technologies have a lot of questions in common with another (for example, &lt;code&gt;javascript&lt;/code&gt; and &lt;code&gt;json&lt;/code&gt; are frequently asked in conjunction with other tags).&lt;/p&gt;
&lt;p&gt;Going back earlier to talking about tag abuse, do the presence of certain pairs of tags lead to notably different answer rates?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/stack-overflow-tags/so_tag_adjacency_percent.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Tag pairs which don&amp;rsquo;t make much sense (e.g. &lt;code&gt;ios&lt;/code&gt;+&lt;code&gt;android&lt;/code&gt;, &lt;code&gt;ios&lt;/code&gt;+&lt;code&gt;javascript&lt;/code&gt;, &lt;code&gt;android&lt;/code&gt;+&lt;code&gt;php&lt;/code&gt;) tend to have very low answer rates (20%-30%). But tags with already high answer rates like &lt;code&gt;regex&lt;/code&gt; don&amp;rsquo;t get much higher or much lower at a given pair.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;There&amp;rsquo;s a lot more than can be done looking at question tags on Stack Overflow. I was surprised to see that all types of programming languages have quick answer times and a high probability of receiving an acceptable answer! I&amp;rsquo;ll definitely keep an eye on the SO archives as they are released, and I&amp;rsquo;m excited to see how trends change in the future.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can view the R and ggplot2 code used to create the data visualizations in &lt;a href=&#34;http://minimaxir.com/notebooks/stack-overflow-questions/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;. You can also view the images/data used for this post in &lt;a href=&#34;https://github.com/minimaxir/stack-overflow-questions&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>