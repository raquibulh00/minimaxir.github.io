<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>TensorFlow on Max Woolf&#39;s Blog</title><link>/tags/tensorflow/</link><description>Recent content in TensorFlow on Max Woolf&#39;s Blog</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright Max Woolf &amp;copy; {year}</copyright><lastBuildDate>Fri, 18 May 2018 09:00:00 +0000</lastBuildDate><atom:link href="/tags/tensorflow/index.xml" rel="self" type="application/rss+xml"/><item><title>How to Quickly Train a Text-Generating Neural Network for Free</title><link>/2018/05/text-neural-networks/</link><pubDate>Fri, 18 May 2018 09:00:00 +0000</pubDate><guid>/2018/05/text-neural-networks/</guid><description>
&lt;p&gt;One of the more interesting applications of the neural network revolution is text generation. Most popular approaches are based off of Andrej Karpathy&amp;rsquo;s &lt;a href=&#34;https://github.com/karpathy/char-rnn&#34; target=&#34;_blank&#34;&gt;char-rnn architecture&lt;/a&gt;/&lt;a href=&#34;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt;, which teaches a recurrent neural network to be able to predict the next character in a sequence based on the previous &lt;em&gt;n&lt;/em&gt; characters. As a result, a sufficiently trained network can theoretically reproduce its input source material, but since properly-trained neural networks aren&amp;rsquo;t &lt;em&gt;perfect&lt;/em&gt;, the output can fall into a weird-but-good uncanny valley.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/text-neural-networks/textgenrnn_console.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Many internet tutorials for text-generation neural networks simply copy an existing char-rnn implementation while changing the input dataset. It&amp;rsquo;s one approach, but there&amp;rsquo;s an opportunity for improvement with modern deep learning tooling. Thanks to frameworks like &lt;a href=&#34;https://www.tensorflow.org&#34; target=&#34;_blank&#34;&gt;TensorFlow&lt;/a&gt; and &lt;a href=&#34;https://github.com/keras-team/keras&#34; target=&#34;_blank&#34;&gt;Keras&lt;/a&gt;, I built &lt;a href=&#34;https://github.com/minimaxir/textgenrnn&#34; target=&#34;_blank&#34;&gt;textgenrnn&lt;/a&gt;, a &lt;a href=&#34;https://pypi.org/project/textgenrnn/#description&#34; target=&#34;_blank&#34;&gt;Python package&lt;/a&gt; which abstracts the process of creating and training such char-rnns to a &lt;em&gt;few lines of code&lt;/em&gt;, with numerous model architecture and training improvements such as &lt;a href=&#34;http://minimaxir.com/2017/04/char-embeddings/&#34; target=&#34;_blank&#34;&gt;character embeddings&lt;/a&gt;, attention-weighted averaging, and a decaying learning rate.&lt;/p&gt;
&lt;p&gt;A neat benefit of textgenrnn is that it can be easily used to train neural networks on a GPU very quickly, &lt;em&gt;for free&lt;/em&gt; using &lt;a href=&#34;https://colab.research.google.com/notebooks/welcome.ipynb&#34; target=&#34;_blank&#34;&gt;Google Colaboratory&lt;/a&gt;. I&amp;rsquo;ve &lt;a href=&#34;https://drive.google.com/file/d/1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;created a notebook&lt;/a&gt; which lets you train your own network and generate text whenever you want with just a few clicks!&lt;/p&gt;
&lt;h2 id=&#34;your-first-text-generating-neural-network&#34;&gt;Your First Text-Generating Neural Network&lt;/h2&gt;
&lt;p&gt;Colaboratory is a notebook environment similar to &lt;a href=&#34;http://jupyter.org&#34; target=&#34;_blank&#34;&gt;Jupyter Notebooks&lt;/a&gt; used in other data science projects. However, Colaboratory notebooks are hosted in a short term virtual machine, with 2 vCPUs, 13GB memory, and a K80 GPU attached. For free. Normally, this configuration would &lt;a href=&#34;https://cloud.google.com/compute/pricing&#34; target=&#34;_blank&#34;&gt;cost&lt;/a&gt; $0.57/hr on Google Compute Engine; it sounds low, but adds up when you need to train model(s) for hours to get good results.&lt;/p&gt;
&lt;p&gt;First, I recommend copying the notebook to your own Drive so it&amp;rsquo;ll always be there (and switch to using Google Chrome if you aren&amp;rsquo;t). The Colaboratory VM contains Python 3 and common Python packages for machine learning such as TensorFlow. But you can install more packages directly in the notebook. Like textgenrnn! Just run this cell by clicking into the cell and click the &amp;ldquo;play&amp;rdquo; button (or use Shift + Enter) and it&amp;rsquo;ll take care of the rest:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/text-neural-networks/pip.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When training a new model, textgenrnn allows you to specify the size and complexity of the neural network with a wide variety of parameters:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/text-neural-networks/config.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s keep these default parameters for now, so run that cell to load them into memory. Run the next cell, which prompts you to upload a file. &lt;em&gt;Any text file should work&lt;/em&gt;, even large text files! For this example, we&amp;rsquo;ll use a 1.1MB text file of Shakespeare plays also &lt;a href=&#34;https://github.com/karpathy/char-rnn/tree/master/data/tinyshakespeare&#34; target=&#34;_blank&#34;&gt;used in the char-rnn demos&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/text-neural-networks/upload.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The next cell initializes an instance of textgenrnn and begins training a custom new text-generating neural network!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/text-neural-networks/train.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;textgenrnn automatically processes the input text into character sequences ready to train the network. After every 2 epochs (a full pass through the data), the network will generate sample text at different temperatures, which represent the &amp;ldquo;creativity&amp;rdquo; of the text (i.e. it allows the model to make increasingly suboptimal predictions, which can cause hilarity to ensue). I typically like generating text at a temperature of 0.5, but for very well-trained models, you can go up to 1.0.&lt;/p&gt;
&lt;p&gt;The quick model training speed comes from the VM&amp;rsquo;s GPU, which can perform the necessary mathematical operations much faster than with a CPU. However, in the case of recurrent neural networks, Keras recently added a &lt;a href=&#34;https://keras.io/layers/recurrent/#cudnnlstm&#34; target=&#34;_blank&#34;&gt;CuDNN implementation of RNNs&lt;/a&gt; like LSTMs, which can easily tap into the GPU-native code more easily and gain a &lt;em&gt;massive&lt;/em&gt; speed boost (&lt;a href=&#34;http://minimaxir.com/2017/11/benchmark-gpus/&#34; target=&#34;_blank&#34;&gt;about &lt;em&gt;7x as fast&lt;/em&gt;&lt;/a&gt;) compared to previous implementations! In all, for this example dataset and model architecture, training on a GPU took 5-6 minutes an epoch, while on a modern CPU, training took &lt;em&gt;1 hour and 24 minutes&lt;/em&gt; an epoch, a &lt;strong&gt;14x speedup&lt;/strong&gt; on the GPU!&lt;/p&gt;
&lt;p&gt;After training is complete, running the next cell will download three files: a &lt;code&gt;weights&lt;/code&gt; file, a &lt;code&gt;vocabulary&lt;/code&gt; file, and a &lt;code&gt;config&lt;/code&gt; file that are all needed to regenerate your model elsewhere.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/text-neural-networks/download.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For example, on your own personal computer. Just install textgenrnn + TensorFlow by inputting &lt;code&gt;pip3 install textgenrnn tensorflow&lt;/code&gt; into a terminal, change to the directory where the downloaded files are located, run &lt;code&gt;python3&lt;/code&gt;, and load the model using:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from textgenrnn import textgenrnn
textgen = textgenrnn(weights_path=&#39;colaboratory_weights.hdf5&#39;,
vocab_path=&#39;colaboratory_vocab.json&#39;,
config_path=&#39;colaboratory_config.json&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s that! No GPU necessary if you&amp;rsquo;re just generating text. You can generate samples (like during training) using &lt;code&gt;textgen.generate_samples()&lt;/code&gt;, generate a ton of samples at any temperature you like to a file using &lt;code&gt;textgen.generate_to_file()&lt;/code&gt;, or incorporate a generated text into a Python script (e.g. a Twitter bot) using &lt;code&gt;textgen.generate(1, return_as_list=True)[0]&lt;/code&gt; to store a text as a variable. You can view more of textgenrnn&amp;rsquo;s functions and capabilities in &lt;a href=&#34;https://github.com/minimaxir/textgenrnn/blob/master/docs/textgenrnn-demo.ipynb&#34; target=&#34;_blank&#34;&gt;this demo Jupyter Notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s some Shakespeare generated with a 50-minute-trained model at a temperature of 0.5:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;LUCENTIO:
And then shall good grave to my wife thee;
Thou would the cause the brieved to me,
And let the place and then receives:
The rest you the foren to my ways him child,
And marry that will be a parties and so set me that be deeds
And then the heart and be so shall make the most as he and stand of seat.
GLOUCESTER:
Your father and madam, or shall for the people
And dead to make the truth, or a business
As we brother to the place her great the truth;
And that which to the smaster and her father,
I am I was see the sun have to the royal true.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not too bad, and it&amp;rsquo;s even close to &lt;a href=&#34;https://en.wikipedia.org/wiki/Iambic_pentameter&#34; target=&#34;_blank&#34;&gt;iambic pentameter&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id=&#34;tweaking-the-model&#34;&gt;Tweaking the Model&lt;/h2&gt;
&lt;p&gt;The most important model configuration options above are &lt;code&gt;rnn_size&lt;/code&gt; and &lt;code&gt;rnn_layers&lt;/code&gt;: these determine the complexity of the network. Typically, you&amp;rsquo;ll see networks in tutorials be a single 128-cell or 256-cell network. However, textgenrnn&amp;rsquo;s architecture is slightly different as it has an attention layer which incorporates &lt;em&gt;all&lt;/em&gt; the preceding model layers. As a result, it&amp;rsquo;s much better to go deeper than wider (e.g. 4x128 is better than 1x512) unless you have a very large amount of text (&amp;gt;10MB). &lt;code&gt;rnn_bidirectional&lt;/code&gt; controls whether the recurrent neural network is bidirectional, that is, it processes the previous characters both forward &lt;em&gt;and&lt;/em&gt; backward (which works great if text follows specific rules, like Shakespeare&amp;rsquo;s character headings). &lt;code&gt;max_length&lt;/code&gt; determines the maximum number of characters for the network to use to predict the next character, which should be increased to let the network learn longer sequences, or decrease for shorter sequences.&lt;/p&gt;
&lt;p&gt;Training has a few helpful options as well. &lt;code&gt;num_epochs&lt;/code&gt; determines the number of full passes of the data; this can be tweaked if you want to train the model even more. &lt;code&gt;batch_size&lt;/code&gt; determines the number of model sequences to train in a step: typically, batch size for deep learning models is 32 or 128, but with a GPU, you can get a speed increase by saturating it with the given 1024 default. &lt;code&gt;train_size&lt;/code&gt; determines the proportion of character samples to train; setting it &lt;code&gt;&amp;lt; 1.0&lt;/code&gt; both speeds up each epoch, and prevents the model from cheating and being able to learn sequences verbatim. (You can set &lt;code&gt;&#39;validation&#39;: True&lt;/code&gt; to run the model on the unused data after each epoch to see if the model is overfitting).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try playing with the parameters more on a new text dataset.&lt;/p&gt;
&lt;h2 id=&#34;word-level-text-generation-with-reddit-data&#34;&gt;Word-Level Text Generation With Reddit Data&lt;/h2&gt;
&lt;p&gt;You might be asking &amp;ldquo;how do you obtain text data&amp;rdquo;? The popular text-generation use cases like lyric generation and movie scripts are copyright-protected so they&amp;rsquo;re harder to find, and even then, it might not be enough text data to train a new model upon (you typically want atleast 100,000 characters).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.reddit.com&#34; target=&#34;_blank&#34;&gt;Reddit&lt;/a&gt;, however, has &lt;em&gt;millions&lt;/em&gt; of submission titles which would be great to train for a model. I wrote a &lt;a href=&#34;https://github.com/minimaxir/subreddit-generator&#34; target=&#34;_blank&#34;&gt;helper script&lt;/a&gt; to automatically download the top &lt;em&gt;n&lt;/em&gt; Reddit submissions from a given subreddit over a given period of time. If you choose subreddits with similar linguistic styles in their titles, the subreddits will even blend together! Let&amp;rsquo;s play with the Top 20,000 Submissions in 2017 from each of &lt;a href=&#34;https://www.reddit.com/r/politics/&#34; target=&#34;_blank&#34;&gt;/r/politics&lt;/a&gt; and &lt;a href=&#34;https://www.reddit.com/r/technology/&#34; target=&#34;_blank&#34;&gt;/r/technology&lt;/a&gt;, which results in a 3.3MB file: about 3x as much data as the Shakespeare plays.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/text-neural-networks/reddit_data.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One last thing that textgenrnn can do that most char-rnn implementations can&amp;rsquo;t is generate a &lt;em&gt;word level&lt;/em&gt; model (thanks to Keras&amp;rsquo;s tokenizers), where the model uses the &lt;em&gt;n&lt;/em&gt; previous words/punctuation to predict the next word/punctuation. On the plus side, using only words prevents crazy typoes and since it predicts multiple &amp;ldquo;characters&amp;rdquo; at a time, &lt;code&gt;max_length&lt;/code&gt; can be reduced proportionally, dramatically speeding up training. There&amp;rsquo;s two downsides with this approach; since words are all lowercase and punctuation is its own token, the generated text cannot be immediately used without manual editing. Additionally, the model weights will be substantially larger than a character-level model since the word-level model has to store an embedding for each word (up to &lt;code&gt;max_words&lt;/code&gt;, which is 10,000 by default when the vocabulary size for a char-level model is 200-300).&lt;/p&gt;
&lt;p&gt;Another advantage of the Colaboratory notebook is that you can quickly adjust model parameters, upload a new file, and immediately start training it. We&amp;rsquo;ll set &lt;code&gt;&#39;line_delimited&#39;: True&lt;/code&gt; and &lt;code&gt;&#39;rnn_bidirectional&#39;: False&lt;/code&gt; since there aren&amp;rsquo;t specific rules. For word level training, let&amp;rsquo;s set &lt;code&gt;&#39;word_level&#39;: True&lt;/code&gt; and &lt;code&gt;&#39;max_length&#39;: 8&lt;/code&gt; to reflect the new training architecture. Since training length has been reduced to 1/5th, we can set &lt;code&gt;&#39;num_epochs&#39;: 50&lt;/code&gt; and &lt;code&gt;&#39;gen_epoch&#39;: 10&lt;/code&gt; to balance it out. Rerun the config cell to update parameters, upload the Reddit data file, and rerun training.&lt;/p&gt;
&lt;p&gt;The resulting model is much more well trained than the Shakespeare model, and here&amp;rsquo;s a few Reddit submission titles generated at a temperature of 1.0:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;report : 49 % of americans now believe all of the country ’ s effective
people like facebook like it &#39; s 650 of 1 %
uber accused of secretly - security popular service ( likely oklahoma )
equifax breach fallout : your salary is dead
sanders uses texas shooter &#39; s iphone sales
adobe videos will be used to sell the web
apple to hold cash for $ 500 service
fitbit just targeting solar energy
george bush &#39; s concept car ‘ goes for all the biggest controversy .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those look pretty good, although they may need a little editing before posting on social media.&lt;/p&gt;
&lt;h2 id=&#34;followup&#34;&gt;Followup&lt;/h2&gt;
&lt;p&gt;These examples only train the model for little time as a demo of textgenrnn&amp;rsquo;s fast learning; there&amp;rsquo;s nothing stopping you from increasing &lt;code&gt;num_epochs&lt;/code&gt; even more to further refine a model. However, from my experience, the training cell times out after &lt;strong&gt;4 hours&lt;/strong&gt;; set &lt;code&gt;num_epochs&lt;/code&gt; accordingly, although in my experience that&amp;rsquo;s all you need before the network converges.&lt;/p&gt;
&lt;p&gt;In practice, I used this Colaboratory notebook to train &lt;em&gt;many&lt;/em&gt; models for &lt;a href=&#34;https://www.reddit.com/r/SubredditNN/&#34; target=&#34;_blank&#34;&gt;/r/SubredditNN&lt;/a&gt;, a Reddit subreddit where only text-generating neural network bots trained on other subreddits. And the results are very funny:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/text-neural-networks/subredditnn.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although text generating neural networks aren&amp;rsquo;t at the point where they can &lt;a href=&#34;https://www.bloomberg.com/news/features/2018-05-17/i-tried-to-get-an-ai-to-write-this-story-paul-ford&#34; target=&#34;_blank&#34;&gt;write entire articles by themselves&lt;/a&gt;, there are still many opportunities to use it just for fun! And thanks to textgenrnn, it&amp;rsquo;s easy, fast, and cost-effective for anyone to do! Let me know if you make any interesting neural networks with textgenrnn and this Notebook!&lt;/p&gt;</description></item><item><title>Benchmarking TensorFlow on Cloud CPUs: Cheaper Deep Learning than Cloud GPUs</title><link>/2017/07/cpu-or-gpu/</link><pubDate>Wed, 05 Jul 2017 09:00:00 +0000</pubDate><guid>/2017/07/cpu-or-gpu/</guid><description>
&lt;p&gt;I&amp;rsquo;ve been working on a few personal deep learning projects with &lt;a href=&#34;https://github.com/fchollet/keras&#34; target=&#34;_blank&#34;&gt;Keras&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org&#34; target=&#34;_blank&#34;&gt;TensorFlow&lt;/a&gt;. However, training models for deep learning with cloud services such as &lt;a href=&#34;https://aws.amazon.com/ec2/&#34; target=&#34;_blank&#34;&gt;Amazon EC2&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/compute/&#34; target=&#34;_blank&#34;&gt;Google Compute Engine&lt;/a&gt; isn&amp;rsquo;t free, and as someone who is currently unemployed, I have to keep an eye on extraneous spending and be as cost-efficient as possible (please support my work on &lt;a href=&#34;https://www.patreon.com/minimaxir&#34; target=&#34;_blank&#34;&gt;Patreon&lt;/a&gt;!). I tried deep learning on the cheaper CPU instances instead of GPU instances to save money, and to my surprise, my model training was only slightly slower. As a result, I took a deeper look at the pricing mechanisms of these two types of instances to see if CPUs are more useful for my needs.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/compute/pricing#gpus&#34; target=&#34;_blank&#34;&gt;pricing of GPU instances&lt;/a&gt; on Google Compute Engine starts at &lt;strong&gt;$0.745/hr&lt;/strong&gt; (by attaching a $0.700/hr GPU die to a $0.045/hr n1-standard-1 instance). A couple months ago, Google &lt;a href=&#34;https://cloudplatform.googleblog.com/2017/05/Compute-Engine-machine-types-with-up-to-64-vCPUs-now-ready-for-your-production-workloads.html&#34; target=&#34;_blank&#34;&gt;announced&lt;/a&gt; CPU instances with up to 64 vCPUs on the modern Intel &lt;a href=&#34;https://en.wikipedia.org/wiki/Skylake_(microarchitecture)&#34; target=&#34;_blank&#34;&gt;Skylake&lt;/a&gt; CPU architecture. More importantly, they can also be used in &lt;a href=&#34;https://cloud.google.com/compute/docs/instances/preemptible&#34; target=&#34;_blank&#34;&gt;preemptible CPU instances&lt;/a&gt;, which live at most for 24 hours on GCE and can be terminated at any time (very rarely), but cost about &lt;em&gt;20%&lt;/em&gt; of the price of a standard instance. A preemptible n1-highcpu-64 instance with 64 vCPUs and 57.6GB RAM plus the premium for using Skylake CPUs is &lt;strong&gt;$0.509/hr&lt;/strong&gt;, about 2/3rds of the cost of the GPU instance.&lt;/p&gt;
&lt;p&gt;If the model training speed of 64 vCPUs is comparable to that of a GPU (or even slightly slower), it would be more cost-effective to use the CPUs instead. But that&amp;rsquo;s assuming the deep learning software and the GCE platform hardware operate at 100% efficiency; if they don&amp;rsquo;t (and they likely don&amp;rsquo;t), there may be &lt;em&gt;even more savings&lt;/em&gt; by scaling down the number of vCPUs and cost accordingly (a 32 vCPU instance with same parameters is half the price at &lt;strong&gt;$0.254/hr&lt;/strong&gt;, 16 vCPU at &lt;strong&gt;$0.127/hr&lt;/strong&gt;, etc).&lt;/p&gt;
&lt;p&gt;There aren&amp;rsquo;t any benchmarks for deep learning libraries with tons and tons of CPUs since there&amp;rsquo;s no demand, as GPUs are the &lt;a href=&#34;https://en.wikipedia.org/wiki/Occam%27s_razor&#34; target=&#34;_blank&#34;&gt;Occam&amp;rsquo;s razor&lt;/a&gt; solution to deep learning hardware. But what might make counterintuitive but economical sense is to use CPUs instead of GPUs for deep learning training because of the massive cost differential afforded by preemptible instances, thanks to Google&amp;rsquo;s &lt;a href=&#34;https://en.wikipedia.org/wiki/Economies_of_scale&#34; target=&#34;_blank&#34;&gt;economies of scale&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;I already have &lt;a href=&#34;https://github.com/minimaxir/deep-learning-cpu-gpu-benchmark&#34; target=&#34;_blank&#34;&gt;benchmarking scripts&lt;/a&gt; of real-world deep learning use cases, &lt;a href=&#34;https://github.com/minimaxir/keras-cntk-docker&#34; target=&#34;_blank&#34;&gt;Docker container environments&lt;/a&gt;, and results logging from my &lt;a href=&#34;http://minimaxir.com/2017/06/keras-cntk/&#34; target=&#34;_blank&#34;&gt;TensorFlow vs. CNTK article&lt;/a&gt;. A few minor tweaks allow the scripts to be utilized for both CPU and GPU instances by setting CLI arguments. I also rebuilt &lt;a href=&#34;https://github.com/minimaxir/keras-cntk-docker/blob/master/Dockerfile&#34; target=&#34;_blank&#34;&gt;the Docker container&lt;/a&gt; to support the latest version of TensorFlow (1.2.1), and created a &lt;a href=&#34;https://github.com/minimaxir/keras-cntk-docker/blob/master/Dockerfile-cpu&#34; target=&#34;_blank&#34;&gt;CPU version&lt;/a&gt; of the container which installs the CPU-appropriate TensorFlow library instead.&lt;/p&gt;
&lt;p&gt;There is a notable CPU-specific TensorFlow behavior; if you install from &lt;code&gt;pip&lt;/code&gt; (as the&lt;a href=&#34;https://www.tensorflow.org/install/&#34; target=&#34;_blank&#34;&gt; official instructions&lt;/a&gt; and tutorials recommend) and begin training a model in TensorFlow, you&amp;rsquo;ll see these warnings in the console:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cpu-or-gpu/tensorflow-console.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In order to fix the warnings and benefit from these &lt;a href=&#34;https://en.wikipedia.org/wiki/SSE4#SSE4.2&#34; target=&#34;_blank&#34;&gt;SSE4.2&lt;/a&gt;/&lt;a href=&#34;https://en.wikipedia.org/wiki/Advanced_Vector_Extensions&#34; target=&#34;_blank&#34;&gt;AVX&lt;/a&gt;/&lt;a href=&#34;https://en.wikipedia.org/wiki/FMA_instruction_set&#34; target=&#34;_blank&#34;&gt;FMA&lt;/a&gt; optimizations, we &lt;a href=&#34;https://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions&#34; target=&#34;_blank&#34;&gt;compile TensorFlow from source&lt;/a&gt;, and I created a &lt;a href=&#34;https://github.com/minimaxir/keras-cntk-docker/blob/master/Dockerfile-cpu-compiled&#34; target=&#34;_blank&#34;&gt;third Docker container&lt;/a&gt; to do just that. When training models in the new container, &lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues/10689&#34; target=&#34;_blank&#34;&gt;most&lt;/a&gt; of the warnings no longer show, and (spoiler alert) there is indeed a speed boost in training time.&lt;/p&gt;
&lt;p&gt;Therefore, we can test three major cases with Google Compute Engine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Tesla K80 GPU instance.&lt;/li&gt;
&lt;li&gt;A 64 Skylake vCPU instance where TensorFlow is installed via &lt;code&gt;pip&lt;/code&gt; (along with testings at 8/16/32 vCPUs).&lt;/li&gt;
&lt;li&gt;A 64 Skylake vCPU instance where TensorFlow is compiled (&lt;code&gt;cmp&lt;/code&gt;) with CPU instructions (+ 8/16/32 vCPUs).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;For each model architecture and software/hardware configuration, I calculate the &lt;strong&gt;total training time relative to the GPU instance training&lt;/strong&gt; for running the model training for the provided test script. In all cases, the GPU &lt;em&gt;should&lt;/em&gt; be the fastest training configuration, and systems with more processors should train faster than those with fewer processors.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start using the &lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34; target=&#34;_blank&#34;&gt;MNIST dataset&lt;/a&gt; of handwritten digits plus the common multilayer perceptron (MLP) architecture, with dense fully-connected layers. Lower training time is better. All configurations below the horizontal dotted line are better than GPUs; all configurations above the dotted line are worse than GPUs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cpu-or-gpu/dl-cpu-gpu-5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, the GPU is the fastest out of all the platform configurations, but there are other curious trends: the performance between 32 vCPUs and 64 vCPUs is similar, and the compiled TensorFlow library is indeed a significant improvement in training speed &lt;em&gt;but only for 8 and 16 vCPUs&lt;/em&gt;. Perhaps there are overheads negotiating information between vCPUs that eliminate the performance advantages of more vCPUs, and perhaps these overheads are &lt;em&gt;different&lt;/em&gt; with the CPU instructions of the compiled TensorFlow. In the end, it&amp;rsquo;s a &lt;a href=&#34;https://en.wikipedia.org/wiki/Black_box&#34; target=&#34;_blank&#34;&gt;black box&lt;/a&gt;, which is why I prefer black box benchmarking all configurations of hardware instead of theorycrafting.&lt;/p&gt;
&lt;p&gt;Since the difference between training speeds of different vCPU counts is minimal, there is definitely an advantage by scaling down. For each model architecture and configuration, I calculate a &lt;strong&gt;normalized training cost relative to the cost of GPU instance training&lt;/strong&gt;. Because GCE instance costs are prorated (unlike Amazon EC2), we can simply calculate experiment cost by multiplying the total number of seconds the experiment runs by the cost of the instance (per second). Ideally, we want to &lt;em&gt;minimize&lt;/em&gt; cost.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cpu-or-gpu/dl-cpu-gpu-6.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lower CPU counts are &lt;em&gt;much&lt;/em&gt; more cost-effective for this problem, when going as low as possible is better.&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s look at the same dataset with a convolutional neural network (CNN) approach for digit classification:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cpu-or-gpu/dl-cpu-gpu-7.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cpu-or-gpu/dl-cpu-gpu-8.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;GPUs are unsurprisingly more than twice as fast as any CPU approach at CNNs, but cost structures are still the same, except that 64 vCPUs are &lt;em&gt;worse&lt;/em&gt; than GPUs cost-wise, with 32 vCPUs training even faster than with 64 vCPUs.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s go deeper with CNNs and look at the &lt;a href=&#34;https://www.cs.toronto.edu/%7Ekriz/cifar.html&#34; target=&#34;_blank&#34;&gt;CIFAR-10&lt;/a&gt; image classification dataset, and a model which utilizes a deep covnet + a multilayer perceptron and ideal for image classification (similar to the &lt;a href=&#34;https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3&#34; target=&#34;_blank&#34;&gt;VGG-16&lt;/a&gt; architecture).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cpu-or-gpu/dl-cpu-gpu-9.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cpu-or-gpu/dl-cpu-gpu-10.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similar behaviors as in the simple CNN case, although in this instance all CPUs perform better with the compiled TensorFlow library.&lt;/p&gt;
&lt;p&gt;The fasttext algorithm, used here on the &lt;a href=&#34;http://ai.stanford.edu/%7Eamaas/data/sentiment/&#34; target=&#34;_blank&#34;&gt;IMDb reviews dataset&lt;/a&gt; to determine whether a review is positive or negative, classifies text extremely quickly relative to other methods.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cpu-or-gpu/dl-cpu-gpu-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cpu-or-gpu/dl-cpu-gpu-4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case, GPUs are much, much faster than CPUs. The benefit of lower numbers of CPU isn&amp;rsquo;t as dramatic; although as an aside, the &lt;a href=&#34;https://github.com/facebookresearch/fastText&#34; target=&#34;_blank&#34;&gt;official fasttext implementation&lt;/a&gt; is &lt;em&gt;designed&lt;/em&gt; for large amounts of CPUs and handles parallelization much better.&lt;/p&gt;
&lt;p&gt;The Bidirectional long-short-term memory (LSTM) architecture is great for working with text data like IMDb reviews, but after my previous benchmark article, &lt;a href=&#34;https://news.ycombinator.com/item?id=14538086&#34; target=&#34;_blank&#34;&gt;commenters on Hacker News&lt;/a&gt; noted that TensorFlow uses an inefficient implementation of the LSTM on the GPU, so perhaps the difference will be more notable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cpu-or-gpu/dl-cpu-gpu-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cpu-or-gpu/dl-cpu-gpu-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Wait, what? GPU training of Bidirectional LSTMs is &lt;em&gt;twice as slow&lt;/em&gt; as any CPU configuration? Wow. (In fairness, the benchmark uses the Keras LSTM default of &lt;code&gt;implementation=0&lt;/code&gt; which is better on CPUs while &lt;code&gt;implementation=2&lt;/code&gt; is better on GPUs, but it shouldn&amp;rsquo;t result in that much of a differential)&lt;/p&gt;
&lt;p&gt;Lastly, LSTM text generation of &lt;a href=&#34;https://en.wikipedia.org/wiki/Friedrich_Nietzsche&#34; target=&#34;_blank&#34;&gt;Nietzsche&amp;rsquo;s&lt;/a&gt; &lt;a href=&#34;https://s3.amazonaws.com/text-datasets/nietzsche.txt&#34; target=&#34;_blank&#34;&gt;writings&lt;/a&gt; follows similar patterns to the other architectures, but without the drastic hit to the GPU.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cpu-or-gpu/dl-cpu-gpu-11.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/cpu-or-gpu/dl-cpu-gpu-12.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;As it turns out, using 64 vCPUs is &lt;em&gt;bad&lt;/em&gt; for deep learning as current software/hardware architectures can&amp;rsquo;t fully utilize all of them, and it often results in the exact same performance (or &lt;em&gt;worse&lt;/em&gt;) than with 32 vCPUs. In terms balancing both training speed and cost, training models with &lt;strong&gt;16 vCPUs + compiled TensorFlow&lt;/strong&gt; seems like the winner. The 30%-40% speed boost of the compiled TensorFlow library was an unexpected surprise, and I&amp;rsquo;m shocked Google doesn&amp;rsquo;t offer a precompiled version of TensorFlow with these CPU speedups since the gains are nontrivial.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s worth nothing that the cost advantages shown here are &lt;em&gt;only&lt;/em&gt; possible with preemptible instances; regular high-CPU instances on Google Compute Engine are about 5x as expensive, and as a result eliminate the cost benefits completely. Hooray for economies of scale!&lt;/p&gt;
&lt;p&gt;A major implicit assumption with the cloud CPU training approach is that you don&amp;rsquo;t need a trained model ASAP. In professional use cases, time may be too valuable to waste, but in personal use cases where someone can just leave a model training overnight, it&amp;rsquo;s a very, very good and cost-effective option, and one that I&amp;rsquo;ll now utilize.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;All scripts for running the benchmark are available in &lt;a href=&#34;https://github.com/minimaxir/deep-learning-cpu-gpu-benchmark&#34; target=&#34;_blank&#34;&gt;this GitHub repo&lt;/a&gt;. You can view the R/ggplot2 code used to process the logs and create the visualizations in &lt;a href=&#34;http://minimaxir.com/notebooks/deep-learning-cpu-gpu/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Predicting the Success of a Reddit Submission with Deep Learning and Keras</title><link>/2017/06/reddit-deep-learning/</link><pubDate>Mon, 26 Jun 2017 09:00:00 +0000</pubDate><guid>/2017/06/reddit-deep-learning/</guid><description>
&lt;p&gt;I&amp;rsquo;ve been trying to figure out what makes a &lt;a href=&#34;https://www.reddit.com&#34; target=&#34;_blank&#34;&gt;Reddit&lt;/a&gt; submission &amp;ldquo;good&amp;rdquo; for years. If we assume the number of upvotes on a submission is a fair proxy for submission quality, optimizing a statistical model for Reddit data with submission score as a response variable might lead to interesting (and profitable) insights when transferred into other domains, such as Facebook Likes and Twitter Favorites.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/reddit-deep-learning/reddit-example.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An important part of a Reddit submission is the submission &lt;strong&gt;title&lt;/strong&gt;. Like news headlines, a catchy title will make a user &lt;a href=&#34;http://minimaxir.com/2015/10/reddit-topwords/&#34; target=&#34;_blank&#34;&gt;more inclined&lt;/a&gt; to engage with a submission and potentially upvote.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/reddit-topwords/mean-054-Fitness.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Additionally, the &lt;strong&gt;time when the submission is made&lt;/strong&gt; is &lt;a href=&#34;http://minimaxir.com/2015/10/reddit-bigquery/&#34; target=&#34;_blank&#34;&gt;important&lt;/a&gt;; submitting when user activity is the highest tends to lead to better results if you are trying to maximize exposure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/reddit-bigquery/reddit-bigquery-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The actual &lt;strong&gt;content&lt;/strong&gt; of the Reddit submission such as images/links to a website is likewise important, but good content is relatively difficult to optimize.&lt;/p&gt;
&lt;p&gt;Can the magic of deep learning reconcile these concepts and create a model which can predict if a submission is a good submission? Thanks to &lt;a href=&#34;https://github.com/fchollet/keras&#34; target=&#34;_blank&#34;&gt;Keras&lt;/a&gt;, performing deep learning on a very large number of Reddit submissions is actually pretty easy. Performing it &lt;em&gt;well&lt;/em&gt; is a different story.&lt;/p&gt;
&lt;h2 id=&#34;getting-the-data-feature-engineering&#34;&gt;Getting the Data + Feature Engineering&lt;/h2&gt;
&lt;p&gt;It&amp;rsquo;s difficult to retrieve the content of millions of Reddit submissions at scale (ethically), so let&amp;rsquo;s initially start by building a model using submissions on &lt;a href=&#34;https://www.reddit.com/r/AskReddit/&#34; target=&#34;_blank&#34;&gt;/r/AskReddit&lt;/a&gt;: Reddit&amp;rsquo;s largest subreddit which receives 8,000+ submissions each day. /r/AskReddit is a self-post only subreddit with no external links, allowing us to focus on only the submission title and timing.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://minimaxir.com/2015/10/reddit-bigquery/&#34; target=&#34;_blank&#34;&gt;As always&lt;/a&gt;, we can collect large amounts of Reddit data from the public Reddit dataset on &lt;a href=&#34;https://cloud.google.com/bigquery/&#34; target=&#34;_blank&#34;&gt;BigQuery&lt;/a&gt;. The submission &lt;code&gt;title&lt;/code&gt; is available by default. The raw timestamp of the submission is also present, allowing us to extract the &lt;code&gt;hour&lt;/code&gt; of submission (adjusted to Eastern Standard Time) and &lt;code&gt;dayofweek&lt;/code&gt;, as used in the heatmap above. But why stop there? Since /r/AskReddit receives hundreds of submissions &lt;em&gt;every hour&lt;/em&gt; on average, we should look at the &lt;code&gt;minute&lt;/code&gt; level to see if there are any deeper trends (e.g. there are only 30 slots available on the first page of /new and since there is so much submission activity, it might be more advantageous to submit during off-peak times). Lastly, to account for potential changes in behavior as the year progresses, we should add a &lt;code&gt;dayofyear&lt;/code&gt; feature, where January 1st = 1, January 2nd = 2, etc which can also account for variance due to atypical days like holidays.&lt;/p&gt;
&lt;p&gt;Instead of predicting the raw number on upvotes of the Reddit submission (as the distribution of submission scores is heavily skewed), we should predict &lt;strong&gt;whether or not the submission is good&lt;/strong&gt;, shaping the problem as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34;&gt;logistic regression&lt;/a&gt;. In this case, let&amp;rsquo;s define a &amp;ldquo;good submission&amp;rdquo; as one whose score is equal to or above the &lt;strong&gt;50th percentile (median) of all submissions&lt;/strong&gt; in /r/AskReddit. Unfortunately, the median score ends up being &lt;strong&gt;2 points&lt;/strong&gt;; although &amp;ldquo;one upvote&amp;rdquo; might be a low threshold for a &amp;ldquo;good&amp;rdquo; submission, it splits the dataset into 64% bad submissions, 36% good submissions, and setting the percentile threshold higher will result in a very unbalanced dataset for model training (a score of 2+ also implies that the submission did not get downvoted to death, which is useful).&lt;/p&gt;
&lt;p&gt;Gathering all &lt;strong&gt;976,538 /r/AskReddit submissions&lt;/strong&gt; from January 2017 to April 2017 should be enough data for this project. Here&amp;rsquo;s the final BigQuery:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;#standardSQL
SELECT id, title,
CAST(FORMAT_TIMESTAMP(&#39;%H&#39;, TIMESTAMP_SECONDS(created_utc), &#39;America/New_York&#39;) AS INT64) AS hour,
CAST(FORMAT_TIMESTAMP(&#39;%M&#39;, TIMESTAMP_SECONDS(created_utc), &#39;America/New_York&#39;) AS INT64) AS minute,
CAST(FORMAT_TIMESTAMP(&#39;%w&#39;, TIMESTAMP_SECONDS(created_utc), &#39;America/New_York&#39;) AS INT64) AS dayofweek,
CAST(FORMAT_TIMESTAMP(&#39;%j&#39;, TIMESTAMP_SECONDS(created_utc), &#39;America/New_York&#39;) AS INT64) AS dayofyear,
IF(PERCENT_RANK() OVER (ORDER BY score ASC) &amp;gt;= 0.50, 1, 0) as is_top_submission
FROM `fh-bigquery.reddit_posts.*`
WHERE (_TABLE_SUFFIX BETWEEN &#39;2017_01&#39; AND &#39;2017_04&#39;)
AND subreddit = &#39;AskReddit&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/reddit-deep-learning/bigquery.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;model-architecture&#34;&gt;Model Architecture&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;If you want to see the detailed data transformations and Keras code examples/outputs for this post, you can view &lt;a href=&#34;https://github.com/minimaxir/predict-reddit-submission-success/blob/master/predict_askreddit_submission_success_timing.ipynb&#34; target=&#34;_blank&#34;&gt;this Jupyter Notebook&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Text processing is a good use case for deep learning, as it can identify relationships between words where older methods like &lt;a href=&#34;https://en.wikipedia.org/wiki/Tf–idf&#34; target=&#34;_blank&#34;&gt;tf-idf&lt;/a&gt; can&amp;rsquo;t. Keras, a high level deep-learning framework on top of lower frameworks like &lt;a href=&#34;https://www.tensorflow.org&#34; target=&#34;_blank&#34;&gt;TensorFlow&lt;/a&gt;, can easily convert a list of texts to a &lt;a href=&#34;https://keras.io/preprocessing/sequence/&#34; target=&#34;_blank&#34;&gt;padded sequence&lt;/a&gt; of &lt;a href=&#34;https://keras.io/preprocessing/text/&#34; target=&#34;_blank&#34;&gt;index tokens&lt;/a&gt; that can interact with deep learning models, along with many other benefits. Data scientists often use &lt;a href=&#34;https://en.wikipedia.org/wiki/Recurrent_neural_network&#34; target=&#34;_blank&#34;&gt;recurrent neural networks&lt;/a&gt; that can &amp;ldquo;learn&amp;rdquo; for classifying text. However &lt;a href=&#34;https://github.com/facebookresearch/fastText&#34; target=&#34;_blank&#34;&gt;fasttext&lt;/a&gt;, a newer algorithm from researchers at Facebook, can perform classification tasks at an &lt;a href=&#34;http://minimaxir.com/2017/06/keras-cntk/&#34; target=&#34;_blank&#34;&gt;order of magnitude faster&lt;/a&gt; training time than RNNs, with similar predictive performance.&lt;/p&gt;
&lt;p&gt;fasttext works by &lt;a href=&#34;https://arxiv.org/abs/1607.01759&#34; target=&#34;_blank&#34;&gt;averaging word vectors&lt;/a&gt;. In this Reddit model architecture inspired by the &lt;a href=&#34;https://github.com/fchollet/keras/blob/master/examples/imdb_fasttext.py&#34; target=&#34;_blank&#34;&gt;official Keras fasttext example&lt;/a&gt;, each word in a Reddit submission title (up to 20) is mapped to a 50-dimensional vector from an Embeddings layer of up to 40,000 words. The Embeddings layer is &lt;a href=&#34;https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html&#34; target=&#34;_blank&#34;&gt;initialized&lt;/a&gt; with &lt;a href=&#34;https://nlp.stanford.edu/projects/glove/&#34; target=&#34;_blank&#34;&gt;GloVe word embeddings&lt;/a&gt; pre-trained on billions of words to give the model a good start. All the word vectors for a given Reddit submission title are averaged together, and then a Dense fully-connected layer outputs a probability the given text is a good submission. The gradients then backpropagate and improve the word embeddings for future batches during training.&lt;/p&gt;
&lt;p&gt;Keras has a &lt;a href=&#34;https://keras.io/visualization/&#34; target=&#34;_blank&#34;&gt;convenient utility&lt;/a&gt; to visualize deep learning models:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/reddit-deep-learning/model_shapes-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, the first output above is the &lt;em&gt;auxiliary output&lt;/em&gt; for &lt;a href=&#34;https://en.wikipedia.org/wiki/Regularization_(mathematics)&#34; target=&#34;_blank&#34;&gt;regularizing&lt;/a&gt; the word embeddings; we still have to incorporate the submission timing data into the model.&lt;/p&gt;
&lt;p&gt;Each of the four timing features (hour, minute, day of week, day of year) receives its own Embeddings layer, outputting a 64D vector. This allows the features to learn latent characteristics which may be missed using traditional &lt;a href=&#34;http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html&#34; target=&#34;_blank&#34;&gt;one-hot encoding&lt;/a&gt; for categorical data in machine learning problems.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/reddit-deep-learning/model_shapes-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The 50D word average vector is concatenated with the four vectors above, resulting in a 306D vector. This combined vector is connected to another fully-connected layer which can account for hidden interactions between all five input features (plus &lt;a href=&#34;https://keras.io/layers/normalization/&#34; target=&#34;_blank&#34;&gt;batch normalization&lt;/a&gt;, which improves training speed for Dense layers). Then the model outputs a final probability prediction: the &lt;em&gt;main output&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/reddit-deep-learning/model_shapes-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The final model:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/reddit-deep-learning/model.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;All of this sounds difficult to implement, but Keras&amp;rsquo;s &lt;a href=&#34;https://keras.io/getting-started/functional-api-guide/&#34; target=&#34;_blank&#34;&gt;functional API&lt;/a&gt; ensures that adding each layer and linking them together can be done in a single line of code each.&lt;/p&gt;
&lt;h2 id=&#34;training-results&#34;&gt;Training Results&lt;/h2&gt;
&lt;p&gt;Because the model uses no recurrent layers, it trains fast enough on a CPU despite the large dataset size.&lt;/p&gt;
&lt;p&gt;We split the full dataset into 80%/20% training/test datasets, training the model on the former and testing the model against the latter. Keras trains a model with a simple &lt;code&gt;fit&lt;/code&gt; command and trains for 20 epochs, where one epoch represents an entire pass of the training set.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/reddit-deep-learning/fit.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s a lot happening in the console output due to the architecture, but the main metrics of interest are the &lt;code&gt;main_out_acc&lt;/code&gt;, the accuracy of the training set through the main output, and &lt;code&gt;val_main_out_acc&lt;/code&gt;, the accuracy of the test set. Ideally, the accuracy of both should increase as training progresses. However, the test accuracy &lt;em&gt;must&lt;/em&gt; be better than the 64% baseline (if we just say all /r/AskReddit submissions are bad), otherwise this model is unhelpful.&lt;/p&gt;
&lt;p&gt;Keras&amp;rsquo;s &lt;a href=&#34;https://keras.io/callbacks/#csvlogger&#34; target=&#34;_blank&#34;&gt;CSVLogger&lt;/a&gt; trivially logs all these metrics to a CSV file. Plotting the results of the 20 epochs:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/reddit-deep-learning/predict-reddit-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The test accuracy does indeed beat the 64% baseline; however, test accuracy &lt;em&gt;decreases&lt;/em&gt; as training progresses. This is a sign of &lt;a href=&#34;https://en.wikipedia.org/wiki/Overfitting&#34; target=&#34;_blank&#34;&gt;overfitting&lt;/a&gt;, possibly due to the potential disparity between texts in the training and test sets. In deep learning, you can account for overfitting by adding &lt;a href=&#34;https://keras.io/layers/core/#dropout&#34; target=&#34;_blank&#34;&gt;Dropout&lt;/a&gt; to relevant layers, but in my testing it did not help.&lt;/p&gt;
&lt;h2 id=&#34;using-the-model-to-optimize-reddit-submissions&#34;&gt;Using The Model To Optimize Reddit Submissions&lt;/h2&gt;
&lt;p&gt;At the least, we now have a model that understands the latent characteristics of an /r/AskReddit submission. But how do you apply the model &lt;em&gt;in practical, real-world situations&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take a random /r/AskReddit submission: &lt;a href=&#34;https://www.reddit.com/r/AskReddit/comments/5odcpd/which_movies_plot_would_drastically_change_if_you/&#34; target=&#34;_blank&#34;&gt;Which movie&amp;rsquo;s plot would drastically change if you removed a letter from its title?&lt;/a&gt;, submitted Monday, January 16th at 3:46 PM EST and receiving 4 upvotes (a &amp;ldquo;good&amp;rdquo; submission in context of this model). Plugging those input variables into the trained model results in a &lt;strong&gt;0.669&lt;/strong&gt; probability of it being considered a good submission, which is consistent with the true results.&lt;/p&gt;
&lt;p&gt;But what if we made &lt;em&gt;minor, iterative changes&lt;/em&gt; to the title while keeping the time submitted unchanged? Can we improve this probability?&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Drastically&amp;rdquo; is a silly adjective; removing it and using the title &lt;strong&gt;Which movie&amp;rsquo;s plot would change if you removed a letter from its title?&lt;/strong&gt; results in a greater probability of &lt;strong&gt;0.682&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Removed&amp;rdquo; is &lt;a href=&#34;http://www.ef.edu/english-resources/english-grammar/conditional/&#34; target=&#34;_blank&#34;&gt;grammatically incorrect&lt;/a&gt;; fixing the issue and using the title &lt;strong&gt;Which movie&amp;rsquo;s plot would change if you remove a letter from its title?&lt;/strong&gt; results in a greater probability of &lt;strong&gt;0.692&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Which&amp;rdquo; is also &lt;a href=&#34;https://www.englishclub.com/vocabulary/wh-question-words.htm&#34; target=&#34;_blank&#34;&gt;grammatically incorrect&lt;/a&gt;; fixing the issue and using the title &lt;strong&gt;What movie&amp;rsquo;s plot would change if you remove a letter from its title?&lt;/strong&gt; results in a greater probability of &lt;strong&gt;0.732&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Although adjectives are sometimes redundant, they can add an intriguing emphasis; adding a &amp;ldquo;single&amp;rdquo; and using the title &lt;strong&gt;What movie&amp;rsquo;s plot would change if you remove a single letter from its title?&lt;/strong&gt; results in a greater probability of &lt;strong&gt;0.753&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Not bad for a little workshopping!&lt;/p&gt;
&lt;p&gt;Now that we have an improved title, we can find an optimal time to make the submission through brute force by calculating the probabilities for all combinations of hour, minute, and day of week (and offsetting the day of year appropriately). After doing so, I discovered that making the submission on the previous Sunday at 10:55 PM EST results in the maximum probability possible of being a good submission at &lt;strong&gt;0.841&lt;/strong&gt; (the other top submission times are at various other minutes during that hour; the best time on a different day is the following Tuesday at 4:05 AM EST with a probability of &lt;strong&gt;0.823&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;In all, this model of Reddit submission success prediction is a proof of concept; there are many, &lt;em&gt;many&lt;/em&gt; optimizations that can be done on the feature engineering side and on the data collection side (especially if we want to model subreddits other than /r/AskReddit). Predicting which submissions go viral instead of just predicting which submissions receive atleast one upvote is another, more advanced problem entirely.&lt;/p&gt;
&lt;p&gt;Thanks to the high-level abstractions and utility functions of Keras, I was able to prototype the initial model in an afternoon instead of the weeks/months required for academic papers and software applications in this area. At the least, this little experiment serves as an example of applying Keras to a real-world dataset, and the tradeoffs that result when deep learning can&amp;rsquo;t magically solve everything. But that doesn&amp;rsquo;t mean my experiments on the Reddit data were unproductive; on the contrary, I now have a few new clever ideas how to fix some of the issues discovered, which I hope to implement soon.&lt;/p&gt;
&lt;p&gt;Again, I strongly recommend reading the data transformations and Keras code examples in &lt;a href=&#34;https://github.com/minimaxir/predict-reddit-submission-success/blob/master/predict_askreddit_submission_success_timing.ipynb&#34; target=&#34;_blank&#34;&gt;this Jupyter Notebook&lt;/a&gt; for more information into the methodology, as building modern deep learning models is more intuitive and less arcane than what thought pieces on Medium imply.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;You can view the R and ggplot2 code used to visualize the model data in &lt;a href=&#34;http://minimaxir.com/notebooks/predict-reddit-submission-success/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;, including 2D projections of the Embedding layers not in this article. You can also view the images/data used for this post in &lt;a href=&#34;https://github.com/minimaxir/predict-reddit-submission-success&#34; target=&#34;_blank&#34;&gt;this GitHub repository&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are free to use the data visualizations/model architectures from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>