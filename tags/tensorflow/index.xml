<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>TensorFlow on Max Woolf&#39;s Blog</title><link>https://minimaxir.com/tags/tensorflow/</link><description>Recent content in TensorFlow on Max Woolf&#39;s Blog</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright Max Woolf &amp;copy; {year}</copyright><lastBuildDate>Thu, 16 Jan 2020 08:00:00 -0800</lastBuildDate><atom:link href="https://minimaxir.com/tags/tensorflow/index.xml" rel="self" type="application/rss+xml"/><item><title>How to Build a Twitter Text-Generating AI Bot With GPT-2</title><link>https://minimaxir.com/2020/01/twitter-gpt2-bot/</link><pubDate>Thu, 16 Jan 2020 08:00:00 -0800</pubDate><guid>https://minimaxir.com/2020/01/twitter-gpt2-bot/</guid><description>
&lt;p&gt;&lt;a href=&#34;https://openai.com/blog/better-language-models/&#34; target=&#34;_blank&#34;&gt;GPT-2&lt;/a&gt;, a text-generating neural network model made by &lt;a href=&#34;https://openai.com&#34; target=&#34;_blank&#34;&gt;OpenAI&lt;/a&gt;, has recently been in the headlines, from being able to play &lt;a href=&#34;https://www.aidungeon.io/start&#34; target=&#34;_blank&#34;&gt;AI-generated text adventures&lt;/a&gt; to playing &lt;em&gt;chess&lt;/em&gt; with an &lt;a href=&#34;https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/&#34; target=&#34;_blank&#34;&gt;AI trained on chess move notation&lt;/a&gt;. However, I initially built &lt;a href=&#34;https://github.com/minimaxir/gpt-2-simple&#34; target=&#34;_blank&#34;&gt;gpt-2-simple&lt;/a&gt;, which can be used to finetune GPT-2 on any text dataset you choose, for a less academic purpose: comedy.&lt;/p&gt;
&lt;p&gt;Over the past month, &lt;a href=&#34;https://twitter.com/&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt; account &lt;a href=&#34;https://twitter.com/dril_gpt2&#34; target=&#34;_blank&#34;&gt;@dril_gpt2&lt;/a&gt;, an AI parody by &lt;a href=&#34;https://twitter.com/kingdomakrillic&#34; target=&#34;_blank&#34;&gt;@kingdomakrillic&lt;/a&gt; of the infamous Twitter user &lt;a href=&#34;https://twitter.com/dril&#34; target=&#34;_blank&#34;&gt;@dril&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/dril_gpt2/status/1208597102181408771&#34; target=&#34;_blank&#34;&gt;used&lt;/a&gt; my &lt;a href=&#34;https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce&#34; target=&#34;_blank&#34;&gt;Colaboratory Notebook&lt;/a&gt; for finetuning GPT-2 on dril&amp;rsquo;s tweets using gpt-2-simple to generate human-curated tweets which push the limits of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Turing_test&#34; target=&#34;_blank&#34;&gt;Turing Test&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;this is what big data looks like:&lt;br&gt;a) filtering out the people with fucked-up hair&lt;br&gt;b) automatically generating news articles for you&lt;br&gt;c) fusing the world star hip hop fanfiction with the mp game net god&lt;/p&gt;&amp;mdash; wint but AI (@dril_gpt2) &lt;a href=&#34;https://twitter.com/dril_gpt2/status/1215760729095016449?ref_src=twsrc%5Etfw&#34;&gt;January 10, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;ok ladies and gentlemen of the jury, before i get any farther along in my testimony, i would like for you to take a moment to recognize the &amp;quot;jurors are beautiful&amp;quot; shirt i am wearing&lt;/p&gt;&amp;mdash; wint but AI (@dril_gpt2) &lt;a href=&#34;https://twitter.com/dril_gpt2/status/1215834913888460800?ref_src=twsrc%5Etfw&#34;&gt;January 11, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;These tweets are &lt;a href=&#34;https://twitter.com/kingdomakrillic/status/1210487045338079237&#34; target=&#34;_blank&#34;&gt;definitely made by a robot&lt;/a&gt; and not by a &lt;a href=&#34;https://twitter.com/KeatonPatti/status/1006961202998726665&#34; target=&#34;_blank&#34;&gt;human pretending to be a robot&lt;/a&gt;; @dril_gpt2 occasionally falls into some of the famous GPT-2 traps such as &lt;a href=&#34;https://twitter.com/dril_gpt2/status/1216162880023752705&#34; target=&#34;_blank&#34;&gt;incoherent lists&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/dril_gpt2/status/1212662889028431872&#34; target=&#34;_blank&#34;&gt;extended repetition loops&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s how you too can create an AI bot to parody any Twitter user, even if you&amp;rsquo;re not a coder!&lt;/p&gt;
&lt;h2 id=&#34;how-to-get-tweets-for-training-an-ai&#34;&gt;How to Get Tweets For Training An AI&lt;/h2&gt;
&lt;p&gt;Twitter&amp;rsquo;s &lt;a href=&#34;https://developer.twitter.com/en.html&#34; target=&#34;_blank&#34;&gt;API&lt;/a&gt; famously limits users to retrieving &lt;a href=&#34;https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline&#34; target=&#34;_blank&#34;&gt;only the latest 3,200 tweets&lt;/a&gt; from a given user, which is not nearly enough input data for training a good AI. Therefore, to get all tweets possible for a user, you&amp;rsquo;ll need to use another approach. The Python package &lt;a href=&#34;https://github.com/twintproject/twint&#34; target=&#34;_blank&#34;&gt;twint&lt;/a&gt; is a popular way of bypassing that API limitation.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve &lt;a href=&#34;https://github.com/minimaxir/download-tweets-ai-text-gen&#34; target=&#34;_blank&#34;&gt;open-sourced a Python 3 script on GitHub&lt;/a&gt; which leverages &lt;code&gt;twint&lt;/code&gt; to download tweets, and then the script does common preprocessing such as removing URLs, retweets, and tweet replies to make the resulting input text cleaner.&lt;/p&gt;
&lt;p&gt;First, in a terminal, install the Python script dependencies:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip3 install twint==2.1.4 fire tqdm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then download the &lt;a href=&#34;https://raw.githubusercontent.com/minimaxir/download-tweets-ai-text-gen/master/download_tweets.py&#34; target=&#34;_blank&#34;&gt;download_tweets.py script&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The script is interacted with via a command line interface. After &lt;code&gt;cd&lt;/code&gt;ing into the directory where the script is stored in a terminal, run:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 download_tweets.py &amp;lt;twitter_username&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;e.g. If you want to download all tweets (sans retweets/replies) from &lt;a href=&#34;https://twitter.com/dril_gpt2&#34; target=&#34;_blank&#34;&gt;@dril&lt;/a&gt;, run:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 download_tweets.py dril
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The tweets will be downloaded to a single-column CSV titled &lt;code&gt;&amp;lt;username&amp;gt;_tweets.csv&lt;/code&gt;, which is the ideal format for training with an AI.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/twitter-gpt2-bot/csv.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The more tweets the better: it&amp;rsquo;s recommended that you have at least 1 MB of input data, which is tens of thousands of tweets.&lt;/p&gt;
&lt;h2 id=&#34;how-to-train-a-twitter-ai-and-generate-tweets&#34;&gt;How To Train a Twitter AI And Generate Tweets&lt;/h2&gt;
&lt;p&gt;A common problem with training AI on short-form text is that the text can &amp;ldquo;leak&amp;rdquo; information; since the AI trains on about 2-3 paragraphs worth of text at a time (about 5-10 tweets), you need to explicitly state when a given tweet begins and when the tweet ends. To fix this issue, &lt;a href=&#34;https://github.com/minimaxir/gpt-2-simple&#34; target=&#34;_blank&#34;&gt;gpt-2-simple&lt;/a&gt; has a special case for single-column CSVs, where it will automatically process the text for best training and generation. (i.e. by adding &lt;code&gt;&amp;lt;|startoftext|&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt; to each tweet). This workflow will also handle multi-line tweets correctly as their own entity.&lt;/p&gt;
&lt;p&gt;You can use &lt;a href=&#34;https://colab.research.google.com/drive/1qxcQ2A1nNjFudAGN_mcMOnvV9sF_PkEb&#34; target=&#34;_blank&#34;&gt;this Colaboratory notebook&lt;/a&gt; to train the model on your downloaded tweets, and generate massive amounts of tweets from it. The notebook itself has more instructions on how to feed the CSV created above as input data to the model.&lt;/p&gt;
&lt;p&gt;Note that without a lot of tweets, the model might easily overfit and output existing tweets verbatim; if that&amp;rsquo;s the case, you may want to train for fewer &lt;code&gt;steps&lt;/code&gt; (e.g. 200-500). Additionally, I recommend only using the 124M &amp;ldquo;small&amp;rdquo; and 355M &amp;ldquo;medium&amp;rdquo; GPT-2 models; larger GPT-2 models finetune poorly on small text documents and low amounts of input data.&lt;/p&gt;
&lt;p&gt;Once the training is complete, you can generate tweets 1,000 at a time using this cell:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gen_file = &#39;gpt2_gentext_{:%Y%m%d_%H%M%S}.txt&#39;.format(datetime.utcnow())
gpt2.generate_to_file(sess,
destination_path=gen_file,
length=200,
temperature=1.0,
top_p=0.9,
prefix=&#39;&amp;lt;|startoftext|&amp;gt;&#39;,
truncate=&#39;&amp;lt;|endoftext|&amp;gt;&#39;,
include_prefix=False,
nsamples=1000,
batch_size=20
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the cell as many times as you want for more tweets, and download them from the Files tab by right-clicking them! The notebook also has more information on how to tweak the generation parameters to make the tweets more crazy or more sane.&lt;/p&gt;
&lt;p&gt;You can then open the generated &lt;code&gt;.txt&lt;/code&gt; files on your local computer in your favorite text editor (I recommend &lt;a href=&#34;https://code.visualstudio.com&#34; target=&#34;_blank&#34;&gt;Visual Studio Code&lt;/a&gt;), and start curating however you see fit! Each tweet is separated by a delimiter line, making it easier to visually parse and handle multiline tweets (compare/contrast with &lt;a href=&#34;https://pastebin.com/TmRtUX2x&#34; target=&#34;_blank&#34;&gt;raw @dril_gpt2&lt;/a&gt; output, which blends together a few tweets per delimiter).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/twitter-gpt2-bot/vscode.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A warning: you are not guaranteed to get quality generated tweets all the time. In fact, quality tweets are &lt;em&gt;rare&lt;/em&gt;: I estimate &lt;strong&gt;less than 5%&lt;/strong&gt; of AI-generated tweets are good/funny. That means if you want to curate hundreds of tweets, you&amp;rsquo;ll need to generate &lt;strong&gt;thousands&lt;/strong&gt; of tweets and sort through all of them (and double-check to make sure they&amp;rsquo;re not real tweets!). It&amp;rsquo;s not as bad as it sounds, in my opinion it&amp;rsquo;s kinda fun. But curation is its own skill, which is why human-curated tweets aren&amp;rsquo;t a stain on the &amp;ldquo;credibility&amp;rdquo; of AI bots, and also why the ~1,500 tweets so far from @dril_gpt2 is very impressive.&lt;/p&gt;
&lt;p&gt;Now, what do you do with these curated tweets?&lt;/p&gt;
&lt;h2 id=&#34;automating-the-twitter-bot&#34;&gt;Automating The Twitter Bot&lt;/h2&gt;
&lt;p&gt;If you&amp;rsquo;re not a programmer or just want to prototype a Twitter bot, I recommend creating a normal Twitter account and scheduling hand-curated Twitter posts through &lt;a href=&#34;https://tweetdeck.twitter.com&#34; target=&#34;_blank&#34;&gt;TweetDeck&lt;/a&gt;, which is owned by Twitter and has native scheduling capabilities. You can space out tweets at given times, although it may be a hassle to do that for hundreds of tweets.&lt;/p&gt;
&lt;p&gt;Otherwise, it is more efficient to write a code script to make tweets at periodic intervals for a bot account. Old tutorials around the internet recommend writing a script which posts to Twitter, sleeps for X hours, post, repeat; that method does not easily scale to multiple bots and it requires that a full computer be dedicated to it, which is not an efficient use of computing resources.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve &lt;a href=&#34;https://github.com/minimaxir/twitter-cloud-run&#34; target=&#34;_blank&#34;&gt;open-sourced an infrastructure schema on GitHub&lt;/a&gt; that leverages &lt;a href=&#34;https://cloud.google.com&#34; target=&#34;_blank&#34;&gt;Google Cloud Platform&lt;/a&gt; services to run hand-curated Twitter bots using a few modern technologies to minimize cost and computation; it&amp;rsquo;s admittingly somewhat complicated, but it should give you an idea of how to best implement a Twitter bot. The repo also has instructions on how to set up a Twitter developer account.&lt;/p&gt;
&lt;h2 id=&#34;the-ethics-of-twitter-ai-bots&#34;&gt;The Ethics of Twitter AI Bots&lt;/h2&gt;
&lt;p&gt;Lastly, let&amp;rsquo;s address the elephant in the room: is building these bots &lt;em&gt;ethical&lt;/em&gt;? Modern AI has frequently been criticized on two fronts, both in how the input training data is obtained (e.g. obtaining faces for training facial recognition software), and how AI-generated media content is used (e.g. video deepfakes).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I am not a lawyer&lt;/strong&gt;, but for these AI-generated tweets, this is how I see it:&lt;/p&gt;
&lt;p&gt;The input data is obtained from Twitter, but not through its API; it&amp;rsquo;s downloaded through external web scraping via &lt;code&gt;twint&lt;/code&gt;, and &lt;em&gt;never logs into the website&lt;/em&gt;. This kind of workflow was ruled as not an abuse by the recent &lt;a href=&#34;https://www.eff.org/deeplinks/2019/09/victory-ruling-hiq-v-linkedin-protects-scraping-public-data&#34; target=&#34;_blank&#34;&gt;hiQ v. LinkedIn decision&lt;/a&gt;, as the data is public. It&amp;rsquo;s still a gray area; I would not &lt;em&gt;redistribute/commercialize the downloaded tweet data&lt;/em&gt;; just use it as input data to the model.&lt;/p&gt;
&lt;p&gt;The actual generated tweets themself should be fine to use as you see fit. Whether AI-generated works infringe on the copyrights of its source material is an evolving area of both ethics and law, but at minimum these AI-generated tweets are both a transformative derivative work and a parody.&lt;/p&gt;
&lt;p&gt;That said, given the massive ambiguities around AI-generated content, it&amp;rsquo;s important to be completely transparent and also comply with &lt;a href=&#34;https://help.twitter.com/en/rules-and-policies/parody-account-policy&#34; target=&#34;_blank&#34;&gt;Twitter rules on parody accounts&lt;/a&gt;. For example, the Twitter bio for the bot should indicate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It&amp;rsquo;s posting AI-generated tweets, made with GPT-2.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s human-curated (or not).&lt;/li&gt;
&lt;li&gt;The Twitter account of who maintains the bot.&lt;/li&gt;
&lt;li&gt;The Twitter account(s) the bot is parodying / model is finetuned upon.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, to avoid impersonation, the full name of the Twitter account should not be a verbatim match to the person being parodied (e.g. &amp;ldquo;&lt;em&gt;X&lt;/em&gt; but AI&amp;rdquo; is fine), and the profile picture should be visually distinct from the human (e.g. my bots have a black-and-white profile picture). I would also not recommend making bots of people who are more newsworthy to avoid accusations of impersonation (e.g. do not make bots of politicians, &lt;em&gt;especially&lt;/em&gt; &lt;a href=&#34;https://twitter.com/realDonaldTrump&#34; target=&#34;_blank&#34;&gt;Donald Trump&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;There is still a lot of work that can be done in optimizing Twitter bots, both in terms of generated tweet quality and in ironing out the ethical logistics of maintaining an AI bot account. &lt;strong&gt;I do not believe that AI text-generating bot Twitter accounts will obsolete human Twitter accounts&lt;/strong&gt;. It&amp;rsquo;s a different &lt;em&gt;flavor&lt;/em&gt; of comedy; not better, not worse. But there&amp;rsquo;s still a lot that can be done to both expand and control the creativity of these Twitter bots, and I have a few active ideas in the pipeline to implement.&lt;/p&gt;</description></item><item><title>Experiments with Making Convincing AI-Generated Fake News</title><link>https://minimaxir.com/2019/09/ctrl-fake-news/</link><pubDate>Mon, 30 Sep 2019 08:00:00 -0700</pubDate><guid>https://minimaxir.com/2019/09/ctrl-fake-news/</guid><description>
&lt;p&gt;&lt;span&gt;&lt;style&gt;
blockquote {
padding-right: 1.25em !important;
}
&lt;/style&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ctrl-fake-news/ctrl_demo_ani.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When &lt;a href=&#34;https://openai.com&#34; target=&#34;_blank&#34;&gt;OpenAI&lt;/a&gt; announced &lt;a href=&#34;https://openai.com/blog/better-language-models/&#34; target=&#34;_blank&#34;&gt;GPT-2&lt;/a&gt;, a robust text-generating AI model, they explicitly only released smaller, less robust versions of the model out of fear that the large model could be used to generate fake news. However, since OpenAI described most of the technical decisions needed to create the model &lt;a href=&#34;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34; target=&#34;_blank&#34;&gt;in the corresponding paper&lt;/a&gt;, it would be possible for others to create their own text generating Transformer models, and maybe even &lt;em&gt;improve&lt;/em&gt; on GPT-2 (with a sufficient budget!).&lt;/p&gt;
&lt;p&gt;In September, the &lt;a href=&#34;https://www.salesforce.com&#34; target=&#34;_blank&#34;&gt;Salesforce&lt;/a&gt; AI team released &lt;a href=&#34;https://github.com/salesforce/ctrl&#34; target=&#34;_blank&#34;&gt;CTRL&lt;/a&gt;, a Transformer-based text generating model with a twist; the model can generate text from specified domains by passing &lt;strong&gt;control codes&lt;/strong&gt; to the model. What caught my interest was a demo of domain style transfer in the &lt;a href=&#34;https://arxiv.org/abs/1909.05858&#34; target=&#34;_blank&#34;&gt;CTRL paper&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/ctrl-fake-news/ctrl_paper.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If the model is that robust to minor URL changes, what happens when you give it URLs that blatantly do not exist? Can the CTRL model create the &amp;ldquo;fake news&amp;rdquo; OpenAI was concerned about? Let&amp;rsquo;s put it to the test.&lt;/p&gt;
&lt;h2 id=&#34;an-overview-of-ctrl&#34;&gt;An Overview of CTRL&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ve &lt;a href=&#34;https://github.com/minimaxir/ctrl-gce&#34; target=&#34;_blank&#34;&gt;written a guide + scripts&lt;/a&gt; to setting the base CTRL model as cheaply as possible on Google Compute Engine with just a few commands. Additionally, the CTRL team has released a &lt;a href=&#34;https://colab.research.google.com/drive/1hVveBQShDru1Mjnhe4C21uQv4A2eH1tV&#34; target=&#34;_blank&#34;&gt;free Colaboratory Notebook&lt;/a&gt; which sets up and runs the CTRL model; however, the model is &lt;em&gt;so large&lt;/em&gt; it won&amp;rsquo;t fit into the memory of traditional GPUs, so the notebook does a trick to shrink it a bit which may impact generation performance.&lt;/p&gt;
&lt;p&gt;Like GPT-2, CTRL has a &lt;a href=&#34;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&#34; target=&#34;_blank&#34;&gt;Transformer&lt;/a&gt; architecture based on &lt;a href=&#34;https://www.tensorflow.org&#34; target=&#34;_blank&#34;&gt;TensorFlow&lt;/a&gt; and uses &lt;a href=&#34;https://en.wikipedia.org/wiki/Byte_pair_encoding&#34; target=&#34;_blank&#34;&gt;byte pair encodings&lt;/a&gt; as its inputs and outputs, which are then decoded into readable text. CTRL has notable performance improvements as it&amp;rsquo;s trained on &lt;em&gt;three times as much data as GPT-2&lt;/em&gt;, including an &lt;a href=&#34;https://github.com/jcpeterson/openwebtext&#34; target=&#34;_blank&#34;&gt;open-sourced clone&lt;/a&gt; of GPT-2&amp;rsquo;s original dataset. And of course, it&amp;rsquo;s larger (1.6B hyperparameters) compared to the currently public GPT-2 (774M hyperparameters), which has significant effects on text quality.&lt;/p&gt;
&lt;p&gt;Most importantly, CTRL &lt;em&gt;requires&lt;/em&gt; a control code if you want to generate text, which allows for more deterministic output compared to GPT-2/&lt;a href=&#34;https://talktotransformer.com&#34; target=&#34;_blank&#34;&gt;TalkToTransformer&lt;/a&gt;. There are several fun control codes, such as &lt;code&gt;Questions&lt;/code&gt; if you want to ask the AI a question, or &lt;code&gt;Reviews&lt;/code&gt; if you want the AI to generate an &lt;a href=&#34;https://www.amazon.com&#34; target=&#34;_blank&#34;&gt;Amazon&lt;/a&gt; review. For this, we&amp;rsquo;ll only look at the &lt;code&gt;Links&lt;/code&gt; control code, which lets you provide a URL and/or a prompt for text generation.&lt;/p&gt;
&lt;p&gt;As the example from the paper shows, URLs contain a surprising amount of metadata. For example, let&amp;rsquo;s consider a &lt;a href=&#34;https://www.washingtonpost.com/powerpost/deal-reached-for-whistleblowers-testimony-house-intelligence-chairman-says/2019/09/29/01cade60-e2d1-11e9-b403-f738899982d2_story.html&#34; target=&#34;_blank&#34;&gt;random Washington Post URL&lt;/a&gt;: &lt;code&gt;https://www.washingtonpost.com/powerpost/deal-reached-for-whistleblowers-testimony-house-intelligence-chairman-says/2019/09/29/01cade60-e2d1-11e9-b403-f738899982d2_story.html&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s month/day/year information (&lt;code&gt;2019/09/29&lt;/code&gt;), a category (&lt;code&gt;powerpost&lt;/code&gt;), and a &lt;a href=&#34;https://en.wikipedia.org/wiki/Clean_URL#Slug&#34; target=&#34;_blank&#34;&gt;URL slug&lt;/a&gt; (&lt;code&gt;deal-reached-for-whistleblowers-testimony-house-intelligence-chairman-says&lt;/code&gt;), typically present for SEO reasons, but in this case provides strong hints to the underlying content. The Transformer architecture is &lt;em&gt;surprisingly&lt;/em&gt; effective at extracting all this metadata, and using it to generate appropriate text.&lt;/p&gt;
&lt;h2 id=&#34;examples-of-fake-news&#34;&gt;Examples of Fake News&lt;/h2&gt;
&lt;p&gt;All of the following examples are generated using the full model with the default settings: 256 token length and a temperature of 0 (i.e. the model always selects the next token with the highest probability). I manually truncate the output once it finishes, but other than that, &lt;strong&gt;these AI-generated articles are not edited&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s address the elephant in the room: when people think of &amp;ldquo;fake news&amp;rdquo;, they think of President Donald Trump, who effectively codified the modern usage of the term. So let&amp;rsquo;s generate a fake news story about him with an innocuous topic rarely associated with him. For example, the video gaming phenomenon &lt;a href=&#34;https://www.epicgames.com/fortnite/home&#34; target=&#34;_blank&#34;&gt;Fortnite&lt;/a&gt;. All that&amp;rsquo;s needed is to tweak the slug (and optionally the date) in the earlier WaPo URL a bit and prepend the corresponding control code, resulting in &lt;code&gt;Links https://www.washingtonpost.com/powerpost/trump-likes-fortnite/2019/09/24/&lt;/code&gt;. Feed it to the model, and behold!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Trump likes Fortnite&lt;/p&gt;
&lt;p&gt;President Trump, who has been known to play video games for hours at a time, is now playing the popular game “Fortnite” on his phone.&lt;/p&gt;
&lt;p&gt;The president was spotted watching the online battle royale game, which pits players against each other in an attempt to kill their opponents, while he sat down with House and Senate leaders Tuesday afternoon.&lt;/p&gt;
&lt;p&gt;“I like that it’s not just about shooting people but also having fun doing it,” said Trump.&lt;/p&gt;
&lt;p&gt;He then asked if anyone had played it before.&lt;/p&gt;
&lt;p&gt;“You know what, I don’t think I’ve ever seen that,” replied Rep. Steve Scalise, R-La.&lt;/p&gt;
&lt;p&gt;A White House official told reporters later Wednesday afternoon that Trump did indeed watch the game.&lt;/p&gt;
&lt;p&gt;“He watched it, yeah,” the official said.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The generated article is very realistic and follows the style of a typical WaPo article (lede, introduction, body, support, etc.), with proper descriptions of Trump and Fortnite, plus a story that matches the headline. Like GPT-2, there are redundancies in the generation (at the end), but it&amp;rsquo;s not something most readers would find suspicious in a casual read.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s tweak things a little. Instead of using &lt;code&gt;likes&lt;/code&gt; in the slug, what if we use &lt;code&gt;despises&lt;/code&gt;, so &lt;code&gt;trump-despises-fortnite&lt;/code&gt;. How well does the model adapt?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Trump despises Fortnite&lt;/p&gt;
&lt;p&gt;The president of the United States has a new favorite video game: “Fortnite.”&lt;/p&gt;
&lt;p&gt;President Trump, who is known for his love of fast food and beer, was reportedly so excited about playing this game that he called it “the greatest thing in the world.”&lt;/p&gt;
&lt;p&gt;But according to an Axios report, Trump’s enthusiasm waned after learning how much time players spend on it.&lt;/p&gt;
&lt;p&gt;“He said it’s not even close to being as good as ‘Candy Crush,’ ” one source told the news outlet.&lt;/p&gt;
&lt;p&gt;In fact, the report said, Trump had been watching videos online while eating dinner with Japanese Prime Minister Shinzo Abe at Mar-a-Lago.&lt;/p&gt;
&lt;p&gt;According to Axios, Trump also complained about the game’s difficulty level — which can be adjusted by spending real money or virtual currency — saying it wasn’t challenging enough.&lt;/p&gt;
&lt;p&gt;“I don’t know why they made it hard,” Trump said.&lt;/p&gt;
&lt;p&gt;He then went off topic when asked if he’d play again.&lt;/p&gt;
&lt;p&gt;“No, I won’t.”&lt;/p&gt;
&lt;p&gt;A White House official later clarified that Trump didn’t say he wouldn’t play, but rather that he’s “not into it.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It adapts very well to the change in tone (even with a rhetorical lede!).&lt;/p&gt;
&lt;p&gt;Fake news doesn&amp;rsquo;t have to be politically-based. Such news in the financial sector could cause just as much chaos. What if it was reported that &lt;a href=&#34;https://www.apple.com&#34; target=&#34;_blank&#34;&gt;Apple&lt;/a&gt;, one of the largest companies in the world, went bankrupt? Put in &lt;code&gt;Links https://www.theverge.com/2019/2/24/20882491/apple-has-filed-for-bankruptcy&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Apple has filed for bankruptcy.&lt;/p&gt;
&lt;p&gt;The company that made the iPhone and iPad is filing for bankruptcy, according to a report from Bloomberg.&lt;/p&gt;
&lt;p&gt;According to Bloomberg News, Apple&amp;rsquo;s board of directors voted unanimously on Friday morning in favor of an emergency plan designed to keep it afloat until its next quarterly earnings report.&lt;/p&gt;
&lt;p&gt;In order to avoid paying more than $128 million in back taxes, Apple will have to sell off assets like its Beats headphone business, which was valued at about $1 billion when it went public last year.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Apple said today that it plans to file for Chapter 11 protection under US Bankruptcy Court in Delaware this week,&amp;rdquo; Bloomberg reports.&lt;/p&gt;
&lt;p&gt;A source familiar with the matter told Reuters earlier this week that Apple had been considering selling some or all of its music streaming service iTunes Music Store as part of a restructuring plan.&lt;/p&gt;
&lt;p&gt;But the news comes just days after reports surfaced suggesting that Apple would be forced into liquidation.&lt;/p&gt;
&lt;p&gt;Last week, Bloomberg reported that Apple could soon announce layoffs if sales don&amp;rsquo;t pick up soon.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We are working hard every day to deliver great products and services to our customers around the world,&amp;rdquo; Tim Cook, CEO of Apple, wrote in a letter to employees today.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Again, the model incorporates both bankruptcy terminology and Apple-based entities. There are minor factual errors (Beats is not public, Apple is not incorporated in Delaware), but casual readers would likely scan over that.&lt;/p&gt;
&lt;p&gt;Science is also a field with lots of fake discoveries. What if AI reported that &lt;a href=&#34;https://en.wikipedia.org/wiki/P_versus_NP_problem&#34; target=&#34;_blank&#34;&gt;P versus NP&lt;/a&gt;, a very famous problem in computer science, was finally solved? Let&amp;rsquo;s try &lt;code&gt;Links https://techcrunch.com/2029/09/24/mit-proves-p-equals-np&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;MIT proves P equals NP&lt;/p&gt;
&lt;p&gt;The University of Massachusetts Amherst has proven that the problem posed by factoring large numbers is NP-complete.&lt;/p&gt;
&lt;p&gt;In a paper published in Nature, researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) have shown how to factor an integer using only two numbers, one for each digit — which means it takes just three operations on your brain.&lt;/p&gt;
&lt;p&gt;This isn’t quite as impressive as solving the problem, but it’s still pretty impressive.&lt;/p&gt;
&lt;p&gt;“Given any number n, we can find its product n times with polynomial time if there are no factors involved,” said lead author David DeWitt, who worked on this project while he was a graduate student there.&lt;/p&gt;
&lt;p&gt;But what about factorizing larger numbers?&lt;/p&gt;
&lt;p&gt;“We showed you could do it, but not without some extra effort,” DeWitt said.&lt;/p&gt;
&lt;p&gt;To prove this, they used a technique called Monte Carlo integration, where they simulated millions or billions of different inputs from thousands of computers running various versions of Java.&lt;/p&gt;
&lt;p&gt;They then ran their algorithm through all possible combinations of input values until it found the best one.&lt;/p&gt;
&lt;p&gt;“The result shows that even though our system doesn’t solve every single case, it does get close enough to be useful.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This instance is less factually correct (and it&amp;rsquo;s clickbaity!), but the topic behind the generated text is &lt;em&gt;complicated&lt;/em&gt; enough that only domain experts would be able to tell it&amp;rsquo;s not internally consistent.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s also worth pointing out now that &lt;em&gt;the majority of the text generated so far is unique&lt;/em&gt;, and CTRL is not cheating by just outputting text from the source datasets. Even though Monte Carlo integration is &lt;a href=&#34;https://en.wikipedia.org/wiki/Monte_Carlo_integration&#34; target=&#34;_blank&#34;&gt;indeed a real thing&lt;/a&gt; that&amp;rsquo;s relatively obscure, Googling that description of it fails to turn up results.&lt;/p&gt;
&lt;p&gt;Lastly, fake news doesn&amp;rsquo;t have to be depressing or sow discord. What about &lt;em&gt;wholesome&lt;/em&gt; fake news? Such as&amp;hellip;the return of beloved television series &lt;a href=&#34;https://en.wikipedia.org/wiki/Firefly_(TV_series)&#34; target=&#34;_blank&#34;&gt;Firefly&lt;/a&gt;? One &lt;code&gt;Links https://www.hollywoodreporter.com/live-feed/fox-announces-firefly-season-2-revival-original-cast-returning-12343245&lt;/code&gt; later:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fox announces Firefly Season 2 revival, original cast returning&lt;/p&gt;
&lt;p&gt;The network has announced that the second season of Joss Whedon’s cult sci-fi series will return for a new run in 2018.&lt;/p&gt;
&lt;p&gt;“Fans have been asking us to bring back Firefly, and we couldn’t be more thrilled about this opportunity,” said Chris Albrecht, president of programming at Fox. “We are excited to continue our relationship with Joss, who is one of the most creative minds working today on television, as well as his amazing cast, which includes Nathan Fillion, Summer Glau, Alan Tudyk, Adam Baldwin, Sean Maher, Adrianne Palicki, Tricia Helfer, Jewel Staite, Michael Trucco, and many more.”&lt;/p&gt;
&lt;p&gt;In addition to Whedon himself, executive producers include Damon Lindelof, Maurissa Tancharoen, Vince Gilligan, David Fury, Jeffrey Bell, Greg Plageman, Scott M. Gimple, Heather Kadin, Andrew Chambliss, John Rogers, and Ron Moore.&lt;/p&gt;
&lt;p&gt;“The show was an instant hit when it debuted over 20 years ago, but its popularity only grew after the success of ‘Serenity,’ so we’re very pleased to welcome fans into another chapter of their lives,” added Feige.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That is a &lt;em&gt;very&lt;/em&gt; stacked cast and crew, all of which (besides the original Firefly members) have acted/worked on sci-fi television series. The only major factual errors are that Chris Albrecht was at STARZ, not Fox, and Feige, presumably Kevin Feige of Marvel Studios, is not mentioned previously in the generated article.&lt;/p&gt;
&lt;p&gt;I know I&amp;rsquo;ll get criticism for highlighting a potentially dangerous application of AI text generation. My perspective is that it&amp;rsquo;s important to know what such tools are &lt;em&gt;capable&lt;/em&gt; of doing in order to more easily recognize fake news. The real problem with fake news isn&amp;rsquo;t the text itself: it&amp;rsquo;s the &lt;em&gt;distribution&lt;/em&gt; of the news on social media like &lt;a href=&#34;http://www.facebook.com&#34; target=&#34;_blank&#34;&gt;Facebook&lt;/a&gt; and &lt;a href=&#34;https://twitter.com&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt;, where the platforms not only &lt;em&gt;incentivize&lt;/em&gt; it, but also fail to sufficiently punish deliberate, repeat offenders. It&amp;rsquo;s why journalism and awareness of fake news is extremely important.&lt;/p&gt;
&lt;p&gt;Some might comment &amp;ldquo;these generated texts aren&amp;rsquo;t convincing at all!&amp;rdquo;, but keep in mind that&amp;rsquo;s because the headline says upfront that they&amp;rsquo;re fake. Would you be able to identify it as a fake if a respected source impulsively tweeted it?&lt;/p&gt;</description></item><item><title>How To Make Custom AI-Generated Text With GPT-2</title><link>https://minimaxir.com/2019/09/howto-gpt2/</link><pubDate>Wed, 04 Sep 2019 08:00:00 -0700</pubDate><guid>https://minimaxir.com/2019/09/howto-gpt2/</guid><description>
&lt;p&gt;In February 2019, &lt;a href=&#34;https://openai.com&#34; target=&#34;_blank&#34;&gt;OpenAI&lt;/a&gt; released &lt;a href=&#34;https://openai.com/blog/better-language-models/&#34; target=&#34;_blank&#34;&gt;a paper&lt;/a&gt; describing GPT-2, a AI-based text-generation model based on the &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34;&gt;Transformer architecture&lt;/a&gt; and trained on massive amounts of text all around the internet. From a text-generation perspective, the included demos were very impressive: the text is coherent over a long horizon, and grammatical syntax and punctuation are near-perfect.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/howto-gpt2/openai-demo.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At the same time, the Python code which allowed anyone to download the model (albeit smaller versions out of concern the full model can be abused to mass-generate fake news) and the &lt;a href=&#34;https://www.tensorflow.org&#34; target=&#34;_blank&#34;&gt;TensorFlow&lt;/a&gt; code to load the downloaded model and generate predictions was &lt;a href=&#34;https://github.com/openai/gpt-2&#34; target=&#34;_blank&#34;&gt;open-sourced on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Neil Shepperd created &lt;a href=&#34;https://github.com/nshepperd/gpt-2&#34; target=&#34;_blank&#34;&gt;a fork&lt;/a&gt; of OpenAI&amp;rsquo;s repo which contains additional code to allow &lt;em&gt;finetuning&lt;/em&gt; the existing OpenAI model on custom datasets. A &lt;a href=&#34;https://github.com/ak9250/gpt-2-colab&#34; target=&#34;_blank&#34;&gt;notebook&lt;/a&gt; was created soon after, which can be copied into &lt;a href=&#34;https://colab.research.google.com&#34; target=&#34;_blank&#34;&gt;Google Colaboratory&lt;/a&gt; and clones Shepperd&amp;rsquo;s repo to finetune GPT-2 backed by a free GPU. From there, the proliferation of GPT-2 generated text took off: researchers such as Gwern Branwen made &lt;a href=&#34;https://www.gwern.net/GPT-2&#34; target=&#34;_blank&#34;&gt;GPT-2 Poetry&lt;/a&gt; and Janelle Shane made &lt;a href=&#34;https://aiweirdness.com/post/183471928977/dd-character-bios-now-making-slightly-more&#34; target=&#34;_blank&#34;&gt;GPT-2 Dungeons and Dragons character bios&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I waited to see if anyone would make a tool to help streamline this finetuning and text generation workflow, a la &lt;a href=&#34;https://github.com/minimaxir/textgenrnn&#34; target=&#34;_blank&#34;&gt;textgenrnn&lt;/a&gt; which I had made for recurrent neural network-based text generation. Months later, no one did. So I did it myself. Enter &lt;a href=&#34;https://github.com/minimaxir/gpt-2-simple&#34; target=&#34;_blank&#34;&gt;gpt-2-simple&lt;/a&gt;, a Python package which wraps Shepperd&amp;rsquo;s finetuning code in a functional interface and adds &lt;em&gt;many&lt;/em&gt; utilities for model management and generation control.&lt;/p&gt;
&lt;p&gt;Thanks to gpt-2-simple and &lt;a href=&#34;https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce&#34; target=&#34;_blank&#34;&gt;this Colaboratory Notebook&lt;/a&gt;, you can easily finetune GPT-2 on your own dataset with a simple function, and generate text to your own specifications!&lt;/p&gt;
&lt;h2 id=&#34;how-gpt-2-works&#34;&gt;How GPT-2 Works&lt;/h2&gt;
&lt;p&gt;OpenAI has released three flavors of GPT-2 models to date: the &amp;ldquo;small&amp;rdquo; 124M parameter model (500MB on disk), the &amp;ldquo;medium&amp;rdquo; 355M model (1.5GB on disk), and recently the 774M model (3GB on disk). These models are &lt;em&gt;much&lt;/em&gt; larger than what you see in typical AI tutorials and are harder to wield: the &amp;ldquo;small&amp;rdquo; model hits GPU memory limits while finetuning with consumer GPUs, the &amp;ldquo;medium&amp;rdquo; model requires additional training techniques before it could be finetuned on server GPUs without going out-of-memory, and the &amp;ldquo;large&amp;rdquo; model &lt;em&gt;cannot be finetuned at all&lt;/em&gt; with current server GPUs before going OOM, even with those techniques.&lt;/p&gt;
&lt;p&gt;The actual Transformer architecture GPT-2 uses is very complicated to explain (here&amp;rsquo;s a &lt;a href=&#34;http://www.peterbloem.nl/blog/transformers&#34; target=&#34;_blank&#34;&gt;great lecture&lt;/a&gt;). For the purposes of finetuning, since we can&amp;rsquo;t modify the architecture, it&amp;rsquo;s easier to think of GPT-2 as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Black_box&#34; target=&#34;_blank&#34;&gt;black box&lt;/a&gt;, taking in inputs and providing outputs. Like &lt;a href=&#34;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&#34; target=&#34;_blank&#34;&gt;previous forms of text generators&lt;/a&gt;, the inputs are a sequence of tokens, and the outputs are the probability of the next token in the sequence, with these probabilities serving as weights for the AI to pick the next token in the sequence. In this case, both the input and output tokens are &lt;a href=&#34;https://en.wikipedia.org/wiki/Byte_pair_encoding&#34; target=&#34;_blank&#34;&gt;byte pair encodings&lt;/a&gt;, which instead of using character tokens (slower to train but includes case/formatting) or word tokens (faster to train but does not include case/formatting) like most RNN approaches, the inputs are &amp;ldquo;compressed&amp;rdquo; to the shortest combination of bytes including case/formatting, which serves as a compromise between both approaches but unfortunately adds randomness to the final generation length. The byte pair encodings are later decoded into readable text for human generation.&lt;/p&gt;
&lt;p&gt;The pretrained GPT-2 models were trained on websites linked from &lt;a href=&#34;https://www.reddit.com&#34; target=&#34;_blank&#34;&gt;Reddit&lt;/a&gt;. As a result, the model has a very strong grasp of the English language, allowing this knowledge to transfer to other datasets and perform well with only a minor amount of additional finetuning. Due to the English bias in encoder construction, languages with non-Latin characters like Russian and &lt;a href=&#34;https://en.wikipedia.org/wiki/CJK_characters&#34; target=&#34;_blank&#34;&gt;CJK&lt;/a&gt; will perform poorly in finetuning.&lt;/p&gt;
&lt;p&gt;When finetuning GPT-2, I recommend using the 124M model (the default) as it&amp;rsquo;s the best balance of speed, size, and creativity. If you have large amounts of training data (&amp;gt;10 MB), then the 355M model may work better.&lt;/p&gt;
&lt;h2 id=&#34;gpt-2-simple-and-colaboratory&#34;&gt;gpt-2-simple And Colaboratory&lt;/h2&gt;
&lt;p&gt;In order to better utilize gpt-2-simple and showcase its features, I created my &lt;a href=&#34;https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce&#34; target=&#34;_blank&#34;&gt;own Colaboratory Notebook&lt;/a&gt;, which can be copied into your own Google account. A Colaboratory Notebook is effectively a &lt;a href=&#34;https://jupyter.org&#34; target=&#34;_blank&#34;&gt;Jupyter Notebook&lt;/a&gt; running on a free (w/ a Google Account) virtual machine with an Nvidia server GPU attached (&lt;a href=&#34;https://twitter.com/BasedBlue/status/1164732922953379841&#34; target=&#34;_blank&#34;&gt;randomly&lt;/a&gt; a K80 or a T4; T4 is ideal) that normally can be cost-prohibitive.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/howto-gpt2/gpu.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Once open, the first cell (run by pressing Shift+Enter in the cell or mousing-over the cell and pressing the &amp;ldquo;Play&amp;rdquo; button) of the notebook installs gpt-2-simple and its dependencies, and loads the package.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/howto-gpt2/imports.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Later in the notebook is &lt;code&gt;gpt2.download_gpt2()&lt;/code&gt; which downloads the requested model type to the Colaboratory VM (the models are hosted on Google&amp;rsquo;s servers, so it&amp;rsquo;s a &lt;em&gt;very&lt;/em&gt; fast download).&lt;/p&gt;
&lt;p&gt;Expanding the Colaboratory sidebar reveals a UI that you can use to upload files. For example, the &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt&#34; target=&#34;_blank&#34;&gt;tinyshakespeare dataset&lt;/a&gt; (1MB) provided with the original &lt;a href=&#34;https://github.com/karpathy/char-rnn&#34; target=&#34;_blank&#34;&gt;char-rnn implementation&lt;/a&gt;. Upload a text file via the UI (you can drag and drop), run the &lt;code&gt;file_name = &#39;&amp;lt;xxx&amp;gt;&#39;&lt;/code&gt; cell with your filename changed in the cell.&lt;/p&gt;
&lt;p&gt;Now we can start finetuning! This finetuning cell loads the specified dataset and trains for the specified number of steps (the default of 1,000 steps is enough to allow distinct text to emerge and takes about 45 minutes, but you can increase the number of steps if necessary).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/howto-gpt2/finetuning.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While the model is finetuning, the average training loss is output every-so-often to the cell. The &lt;em&gt;absolute value&lt;/em&gt; of the loss is not important (the output text quality is subjective), but if the average loss stops decreasing, that&amp;rsquo;s a sign the model has converged and additional training may not help improve the model.&lt;/p&gt;
&lt;p&gt;By default, your model is saved in the &lt;code&gt;checkpoint/run1&lt;/code&gt; folder, and you&amp;rsquo;ll need to use that folder to load the model as well (you can specify the &lt;code&gt;run_name&lt;/code&gt; when using other functions categorize finetuned models). If you want to export the model from Colaboratory, it&amp;rsquo;s recommended you do so via &lt;a href=&#34;https://www.google.com/drive/&#34; target=&#34;_blank&#34;&gt;Google Drive&lt;/a&gt; (as Colaboratory does not like exporting large files). Run the &lt;code&gt;gpt2.mount_gdrive()&lt;/code&gt; cell to mount your Google Drive in the Colaboratory VM, then run the &lt;code&gt;gpt2.copy_checkpoint_to_gdrive()&lt;/code&gt; cell. You can then download the compressed model folder from Google Drive and run the model wherever you want. Likewise, you can use the &lt;code&gt;gpt2.copy_checkpoint_from_gdrive()&lt;/code&gt; cell to retrieve a stored model and generate in the notebook.&lt;/p&gt;
&lt;p&gt;Speaking of generation, once you have a finetuned model, you can now generate custom text from it! By default, the &lt;code&gt;gpt2.generate()&lt;/code&gt; function will generate as much text as possible (1,024 tokens) with a little bit of randomness. An important caveat: &lt;em&gt;you will not get good generated text 100% of the time&lt;/em&gt;, even with a properly trained model (the OpenAI demo above took &lt;em&gt;25 tries&lt;/em&gt; to get good text!).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/howto-gpt2/gen_long.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can also increase the &lt;code&gt;temperature&lt;/code&gt; to increase &amp;ldquo;creativity&amp;rdquo; by allowing the network to more likely make suboptimal predictions, provide a &lt;code&gt;prefix&lt;/code&gt; to specify how exactly you want your text to begin. There are many other useful configuration parameters, such as &lt;code&gt;top_p&lt;/code&gt; for &lt;a href=&#34;https://github.com/minimaxir/gpt-2-simple/issues/51&#34; target=&#34;_blank&#34;&gt;nucleus sampling&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/howto-gpt2/gen_long_params.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As a bonus, you can bulk-generate text with gpt-2-simple by setting &lt;code&gt;nsamples&lt;/code&gt; (number of texts to generate total) and &lt;code&gt;batch_size&lt;/code&gt; (number of texts to generate at a time); the Colaboratory GPUs can support a &lt;code&gt;batch_size&lt;/code&gt; of up to 20, and you can generate these to a text file with &lt;code&gt;gpt2.generate_to_file(file_name)&lt;/code&gt; with the same parameters as &lt;code&gt;gpt2.generate()&lt;/code&gt;. You can download the generated file locally via the sidebar, and use those to easily save and share the generated texts.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce&#34; target=&#34;_blank&#34;&gt;The notebook&lt;/a&gt; has many more functions as well, with more parameters and detailed explanations! The &lt;a href=&#34;https://github.com/minimaxir/gpt-2-simple&#34; target=&#34;_blank&#34;&gt;gpt-2-simple README&lt;/a&gt; lists additional features of gpt-2-simple if you want to use the model outside the notebook.&lt;/p&gt;
&lt;p&gt;(NB: Currently, you&amp;rsquo;ll need to reset the Notebook via Runtime → Restart Runtime to finetune a different model/dataset or load a different finetuned model.)&lt;/p&gt;
&lt;h2 id=&#34;gpt-2-for-short-texts&#34;&gt;GPT-2 For Short Texts&lt;/h2&gt;
&lt;p&gt;A weakness of GPT-2 and other out-of-the-box AI text generators is that they are built for longform content, and keep on generating text until you hit the specified length. Another reason I wanted to make gpt-2-simple was to add explicit processing tricks to the generated text to work around this issue for short texts. In this case, there are two additional parameters that can be passed to &lt;code&gt;gpt2.generate()&lt;/code&gt;: &lt;code&gt;truncate&lt;/code&gt; and &lt;code&gt;include_prefix&lt;/code&gt;. For example, if each short text begins with a &lt;code&gt;&amp;lt;|startoftext|&amp;gt;&lt;/code&gt; token and ends with a &lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt;, then setting &lt;code&gt;prefix=&#39;&amp;lt;|startoftext|&amp;gt;&#39;&lt;/code&gt;, &lt;code&gt;truncate=&amp;lt;|endoftext|&amp;gt;&#39;&lt;/code&gt;, and &lt;code&gt;include_prefix=False&lt;/code&gt;, and &lt;code&gt;length&lt;/code&gt; is sufficient, then gpt-2-simple will automatically extract the shortform texts, even when generating in batches.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s finetune a GPT-2 model on Reddit submission titles. This query, when run on &lt;a href=&#34;https://console.cloud.google.com/bigquery&#34; target=&#34;_blank&#34;&gt;BigQuery&lt;/a&gt; (for free), returns the top 16,000 titles by score between January and March 2019 for a given Reddit subreddit (in this case, &lt;a href=&#34;https://www.reddit.com/r/AskReddit/&#34; target=&#34;_blank&#34;&gt;/r/AskReddit&lt;/a&gt;) + minor text preprocessing, which can be downloaded locally as a 1.3 MB CSV (Save Results → CSV [local file]):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;#standardSQL
SELECT
REGEXP_REPLACE(REGEXP_REPLACE(REGEXP_REPLACE(REGEXP_REPLACE(title, &#39;&amp;amp;amp;&#39;, &#39;&amp;amp;&#39;), &#39;&amp;amp;lt;&#39;, &#39;&amp;lt;&#39;), &#39;&amp;amp;gt;&#39;, &#39;&amp;gt;&#39;), &#39;�&#39;, &#39;&#39;) AS title
FROM
`fh-bigquery.reddit_posts.*`
WHERE
_TABLE_SUFFIX BETWEEN &#39;2019_01&#39; AND &#39;2019_03&#39;
AND LENGTH(title) &amp;gt;= 8
AND LOWER(subreddit) = &#39;askreddit&#39;
ORDER BY
score DESC
LIMIT
16000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With gpt-2-simple, using a single-column CSV like the one generated above as the input dataset will automatically add &lt;code&gt;&amp;lt;|startoftext|&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt; tokens appropriately. Finetune a new GPT-2 model as normal, and then generate with those additional parameters mentioned above:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/howto-gpt2/gen_short.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s worth noting that despite a good amount of input data to the model, finetuned networks can easily &lt;em&gt;overfit&lt;/em&gt; on short form text: some of these example titles are very close to existing /r/AskReddit titles. Overfitting can be rectified by training for less time, or adding more input data. Make sure to double check that your generated text is unique!&lt;/p&gt;
&lt;p&gt;You can play with this Reddit-oriented variant in &lt;a href=&#34;https://colab.research.google.com/drive/1RugXCYDcMvSACYNt9j0kB6zzqRKzAbBn&#34; target=&#34;_blank&#34;&gt;this modified Colaboratory Notebook&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;making-gpt-2-apps&#34;&gt;Making GPT-2 Apps&lt;/h2&gt;
&lt;p&gt;There have already been cool, non-nefarious uses of GPT-2, such as Adam King&amp;rsquo;s &lt;a href=&#34;https://talktotransformer.com&#34; target=&#34;_blank&#34;&gt;TalkToTransformer&lt;/a&gt; which provides a UI for the 774M model (and has gone viral many times) and &lt;a href=&#34;https://tabnine.com&#34; target=&#34;_blank&#34;&gt;TabNine&lt;/a&gt;, which uses GPT-2 finetuned on GitHub code in order to create probabilistic code completion. On the &lt;a href=&#34;https://pytorch.org&#34; target=&#34;_blank&#34;&gt;PyTorch&lt;/a&gt; side, Huggingface has released a &lt;a href=&#34;https://github.com/huggingface/pytorch-transformers&#34; target=&#34;_blank&#34;&gt;Transformers client&lt;/a&gt; (w/ GPT-2 support) of their own, and also created apps such as &lt;a href=&#34;https://transformer.huggingface.co&#34; target=&#34;_blank&#34;&gt;Write With Transformer&lt;/a&gt; to serve as a text autocompleter.&lt;/p&gt;
&lt;p&gt;Many AI tutorials often show how to deploy a small model to a web service by using the &lt;a href=&#34;https://palletsprojects.com/p/flask/&#34; target=&#34;_blank&#34;&gt;Flask&lt;/a&gt; application framework. The problem with GPT-2 is that it&amp;rsquo;s such a huge model that most conventional advice won&amp;rsquo;t work well to get a performant app. And even if you do get it to run fast (e.g. by running the app on a GPU), it won&amp;rsquo;t be &lt;em&gt;cheap&lt;/em&gt;, especially if you want it to be resilient to a random surge of virality.&lt;/p&gt;
&lt;p&gt;With gpt-2-simple, the solution I came up with is &lt;a href=&#34;https://github.com/minimaxir/gpt-2-cloud-run&#34; target=&#34;_blank&#34;&gt;gpt-2-cloud-run&lt;/a&gt;; a small webapp intended to run GPT-2 via &lt;a href=&#34;https://cloud.google.com/run/&#34; target=&#34;_blank&#34;&gt;Google Cloud Run&lt;/a&gt; backed by gpt-2-simple. The advantage here is that Cloud Run only charges for compute used and can scale indefinitely if there&amp;rsquo;s a traffic surge; for casual use, it&amp;rsquo;s extremely cost effective compared to running a GPU 24/7. I&amp;rsquo;ve used Cloud Run to make a GPT-2 text generator for &lt;a href=&#34;https://minimaxir.com/apps/gpt2-reddit/&#34; target=&#34;_blank&#34;&gt;Reddit-wide submission titles&lt;/a&gt; and a GPT-2 generator for &lt;a href=&#34;https://minimaxir.com/apps/gpt2-mtg/&#34; target=&#34;_blank&#34;&gt;Magic: The Gathering cards&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/howto-gpt2/mtg.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;attributing-ai-generated-text&#34;&gt;Attributing AI-Generated Text&lt;/h2&gt;
&lt;p&gt;One of the main reasons I developed textgenrnn and gpt-2-simple is to make AI text generation more &lt;em&gt;accessible&lt;/em&gt; as you do not need a strong AI or technical background to create fun stories. However, in the case of GPT-2, I&amp;rsquo;ve noticed an elevated amount of &amp;ldquo;I trained an AI to generate text&amp;rdquo; articles/Reddit posts/YouTube videos saying they used GPT-2 to train an AI, but not &lt;em&gt;how&lt;/em&gt; they trained the AI: especially suspicious since finetuning is not an out-of-the-box feature that OpenAI provides. The fact that Keaton Patti&amp;rsquo;s &lt;a href=&#34;https://twitter.com/KeatonPatti/status/1161284670601990146&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;I forced a bot&amp;rdquo; movie scripts&lt;/a&gt; (that aren&amp;rsquo;t written by a bot) frequently go megaviral due to that particular framing doesn&amp;rsquo;t help.&lt;/p&gt;
&lt;p&gt;Although it&amp;rsquo;s not legally required, I ask that anyone who shares generated text via gpt-2-simple add a link to the repo and/or Colaboratory notebook not just for attribution, but to &lt;em&gt;spread knowledge&lt;/em&gt; about the accessibility of AI text generation. It&amp;rsquo;s a technology that should be transparent, not obfuscated for personal gain.&lt;/p&gt;
&lt;h2 id=&#34;the-future-of-gpt-2&#34;&gt;The Future of GPT-2&lt;/h2&gt;
&lt;p&gt;Hopefully, this article gave you ideas on how to finetune and generate texts creatively. There&amp;rsquo;s still a &lt;em&gt;lot&lt;/em&gt; of untapped potential, and there are still many cool applications that have been untouched, and many cool datasets that haven&amp;rsquo;t been used for AI text generation. GPT-2 will likely be used more for mass-producing &lt;a href=&#34;https://twitter.com/Fred_Delicious/status/1166783214750445573&#34; target=&#34;_blank&#34;&gt;crazy erotica&lt;/a&gt; than fake news.&lt;/p&gt;
&lt;p&gt;However, GPT-2 and the Transformer architecture aren&amp;rsquo;t the end-game of AI text generation. Not by a long shot.&lt;/p&gt;</description></item><item><title>How to Quickly Train a Text-Generating Neural Network for Free</title><link>https://minimaxir.com/2018/05/text-neural-networks/</link><pubDate>Fri, 18 May 2018 09:00:00 -0700</pubDate><guid>https://minimaxir.com/2018/05/text-neural-networks/</guid><description>
&lt;p&gt;One of the more interesting applications of the neural network revolution is text generation. Most popular approaches are based off of Andrej Karpathy&amp;rsquo;s &lt;a href=&#34;https://github.com/karpathy/char-rnn&#34; target=&#34;_blank&#34;&gt;char-rnn architecture&lt;/a&gt;/&lt;a href=&#34;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt;, which teaches a recurrent neural network to be able to predict the next character in a sequence based on the previous &lt;em&gt;n&lt;/em&gt; characters. As a result, a sufficiently trained network can theoretically reproduce its input source material, but since properly-trained neural networks aren&amp;rsquo;t &lt;em&gt;perfect&lt;/em&gt;, the output can fall into a weird-but-good uncanny valley.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/text-neural-networks/textgenrnn_console.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Many internet tutorials for text-generation neural networks simply copy an existing char-rnn implementation while changing the input dataset. It&amp;rsquo;s one approach, but there&amp;rsquo;s an opportunity for improvement with modern deep learning tooling. Thanks to frameworks like &lt;a href=&#34;https://www.tensorflow.org&#34; target=&#34;_blank&#34;&gt;TensorFlow&lt;/a&gt; and &lt;a href=&#34;https://github.com/keras-team/keras&#34; target=&#34;_blank&#34;&gt;Keras&lt;/a&gt;, I built &lt;a href=&#34;https://github.com/minimaxir/textgenrnn&#34; target=&#34;_blank&#34;&gt;textgenrnn&lt;/a&gt;, a &lt;a href=&#34;https://pypi.org/project/textgenrnn/#description&#34; target=&#34;_blank&#34;&gt;Python package&lt;/a&gt; which abstracts the process of creating and training such char-rnns to a &lt;em&gt;few lines of code&lt;/em&gt;, with numerous model architecture and training improvements such as &lt;a href=&#34;http://minimaxir.com/2017/04/char-embeddings/&#34; target=&#34;_blank&#34;&gt;character embeddings&lt;/a&gt;, attention-weighted averaging, and a decaying learning rate.&lt;/p&gt;
&lt;p&gt;A neat benefit of textgenrnn is that it can be easily used to train neural networks on a GPU very quickly, &lt;em&gt;for free&lt;/em&gt; using &lt;a href=&#34;https://colab.research.google.com/notebooks/welcome.ipynb&#34; target=&#34;_blank&#34;&gt;Google Colaboratory&lt;/a&gt;. I&amp;rsquo;ve &lt;a href=&#34;https://drive.google.com/file/d/1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;created a notebook&lt;/a&gt; which lets you train your own network and generate text whenever you want with just a few clicks!&lt;/p&gt;
&lt;h2 id=&#34;your-first-text-generating-neural-network&#34;&gt;Your First Text-Generating Neural Network&lt;/h2&gt;
&lt;p&gt;Colaboratory is a notebook environment similar to &lt;a href=&#34;http://jupyter.org&#34; target=&#34;_blank&#34;&gt;Jupyter Notebooks&lt;/a&gt; used in other data science projects. However, Colaboratory notebooks are hosted in a short term virtual machine, with 2 vCPUs, 13GB memory, and a K80 GPU attached. For free. Normally, this configuration would &lt;a href=&#34;https://cloud.google.com/compute/pricing&#34; target=&#34;_blank&#34;&gt;cost&lt;/a&gt; $0.57/hr on Google Compute Engine; it sounds low, but adds up when you need to train model(s) for hours to get good results.&lt;/p&gt;
&lt;p&gt;First, I recommend copying the notebook to your own Drive so it&amp;rsquo;ll always be there (and switch to using Google Chrome if you aren&amp;rsquo;t). The Colaboratory VM contains Python 3 and common Python packages for machine learning such as TensorFlow. But you can install more packages directly in the notebook. Like textgenrnn! Just run this cell by clicking into the cell and click the &amp;ldquo;play&amp;rdquo; button (or use Shift + Enter) and it&amp;rsquo;ll take care of the rest:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/text-neural-networks/pip.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When training a new model, textgenrnn allows you to specify the size and complexity of the neural network with a wide variety of parameters:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/text-neural-networks/config.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s keep these default parameters for now, so run that cell to load them into memory. Run the next cell, which prompts you to upload a file. &lt;em&gt;Any text file should work&lt;/em&gt;, even large text files! For this example, we&amp;rsquo;ll use a 1.1MB text file of Shakespeare plays also &lt;a href=&#34;https://github.com/karpathy/char-rnn/tree/master/data/tinyshakespeare&#34; target=&#34;_blank&#34;&gt;used in the char-rnn demos&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/text-neural-networks/upload.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The next cell initializes an instance of textgenrnn and begins training a custom new text-generating neural network!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/text-neural-networks/train.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;textgenrnn automatically processes the input text into character sequences ready to train the network. After every 2 epochs (a full pass through the data), the network will generate sample text at different temperatures, which represent the &amp;ldquo;creativity&amp;rdquo; of the text (i.e. it allows the model to make increasingly suboptimal predictions, which can cause hilarity to ensue). I typically like generating text at a temperature of 0.5, but for very well-trained models, you can go up to 1.0.&lt;/p&gt;
&lt;p&gt;The quick model training speed comes from the VM&amp;rsquo;s GPU, which can perform the necessary mathematical operations much faster than with a CPU. However, in the case of recurrent neural networks, Keras recently added a &lt;a href=&#34;https://keras.io/layers/recurrent/#cudnnlstm&#34; target=&#34;_blank&#34;&gt;CuDNN implementation of RNNs&lt;/a&gt; like LSTMs, which can easily tap into the GPU-native code more easily and gain a &lt;em&gt;massive&lt;/em&gt; speed boost (&lt;a href=&#34;http://minimaxir.com/2017/11/benchmark-gpus/&#34; target=&#34;_blank&#34;&gt;about &lt;em&gt;7x as fast&lt;/em&gt;&lt;/a&gt;) compared to previous implementations! In all, for this example dataset and model architecture, training on a GPU took 5-6 minutes an epoch, while on a modern CPU, training took &lt;em&gt;1 hour and 24 minutes&lt;/em&gt; an epoch, a &lt;strong&gt;14x speedup&lt;/strong&gt; on the GPU!&lt;/p&gt;
&lt;p&gt;After training is complete, running the next cell will download three files: a &lt;code&gt;weights&lt;/code&gt; file, a &lt;code&gt;vocabulary&lt;/code&gt; file, and a &lt;code&gt;config&lt;/code&gt; file that are all needed to regenerate your model elsewhere.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/text-neural-networks/download.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For example, on your own personal computer. Just install textgenrnn + TensorFlow by inputting &lt;code&gt;pip3 install textgenrnn tensorflow&lt;/code&gt; into a terminal, change to the directory where the downloaded files are located, run &lt;code&gt;python3&lt;/code&gt;, and load the model using:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from textgenrnn import textgenrnn
textgen = textgenrnn(weights_path=&#39;colaboratory_weights.hdf5&#39;,
vocab_path=&#39;colaboratory_vocab.json&#39;,
config_path=&#39;colaboratory_config.json&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s that! No GPU necessary if you&amp;rsquo;re just generating text. You can generate samples (like during training) using &lt;code&gt;textgen.generate_samples()&lt;/code&gt;, generate a ton of samples at any temperature you like to a file using &lt;code&gt;textgen.generate_to_file()&lt;/code&gt;, or incorporate a generated text into a Python script (e.g. a Twitter bot) using &lt;code&gt;textgen.generate(1, return_as_list=True)[0]&lt;/code&gt; to store a text as a variable. You can view more of textgenrnn&amp;rsquo;s functions and capabilities in &lt;a href=&#34;https://github.com/minimaxir/textgenrnn/blob/master/docs/textgenrnn-demo.ipynb&#34; target=&#34;_blank&#34;&gt;this demo Jupyter Notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s some Shakespeare generated with a 50-minute-trained model at a temperature of 0.5:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;LUCENTIO:
And then shall good grave to my wife thee;
Thou would the cause the brieved to me,
And let the place and then receives:
The rest you the foren to my ways him child,
And marry that will be a parties and so set me that be deeds
And then the heart and be so shall make the most as he and stand of seat.
GLOUCESTER:
Your father and madam, or shall for the people
And dead to make the truth, or a business
As we brother to the place her great the truth;
And that which to the smaster and her father,
I am I was see the sun have to the royal true.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not too bad, and it&amp;rsquo;s even close to &lt;a href=&#34;https://en.wikipedia.org/wiki/Iambic_pentameter&#34; target=&#34;_blank&#34;&gt;iambic pentameter&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id=&#34;tweaking-the-model&#34;&gt;Tweaking the Model&lt;/h2&gt;
&lt;p&gt;The most important model configuration options above are &lt;code&gt;rnn_size&lt;/code&gt; and &lt;code&gt;rnn_layers&lt;/code&gt;: these determine the complexity of the network. Typically, you&amp;rsquo;ll see networks in tutorials be a single 128-cell or 256-cell network. However, textgenrnn&amp;rsquo;s architecture is slightly different as it has an attention layer which incorporates &lt;em&gt;all&lt;/em&gt; the preceding model layers. As a result, it&amp;rsquo;s much better to go deeper than wider (e.g. 4x128 is better than 1x512) unless you have a very large amount of text (&amp;gt;10MB). &lt;code&gt;rnn_bidirectional&lt;/code&gt; controls whether the recurrent neural network is bidirectional, that is, it processes the previous characters both forward &lt;em&gt;and&lt;/em&gt; backward (which works great if text follows specific rules, like Shakespeare&amp;rsquo;s character headings). &lt;code&gt;max_length&lt;/code&gt; determines the maximum number of characters for the network to use to predict the next character, which should be increased to let the network learn longer sequences, or decrease for shorter sequences.&lt;/p&gt;
&lt;p&gt;Training has a few helpful options as well. &lt;code&gt;num_epochs&lt;/code&gt; determines the number of full passes of the data; this can be tweaked if you want to train the model even more. &lt;code&gt;batch_size&lt;/code&gt; determines the number of model sequences to train in a step: typically, batch size for deep learning models is 32 or 128, but with a GPU, you can get a speed increase by saturating it with the given 1024 default. &lt;code&gt;train_size&lt;/code&gt; determines the proportion of character samples to train; setting it &lt;code&gt;&amp;lt; 1.0&lt;/code&gt; both speeds up each epoch, and prevents the model from cheating and being able to learn sequences verbatim. (You can set &lt;code&gt;&#39;validation&#39;: True&lt;/code&gt; to run the model on the unused data after each epoch to see if the model is overfitting).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try playing with the parameters more on a new text dataset.&lt;/p&gt;
&lt;h2 id=&#34;word-level-text-generation-with-reddit-data&#34;&gt;Word-Level Text Generation With Reddit Data&lt;/h2&gt;
&lt;p&gt;You might be asking &amp;ldquo;how do you obtain text data&amp;rdquo;? The popular text-generation use cases like lyric generation and movie scripts are copyright-protected so they&amp;rsquo;re harder to find, and even then, it might not be enough text data to train a new model upon (you typically want atleast 100,000 characters).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.reddit.com&#34; target=&#34;_blank&#34;&gt;Reddit&lt;/a&gt;, however, has &lt;em&gt;millions&lt;/em&gt; of submission titles which would be great to train for a model. I wrote a &lt;a href=&#34;https://github.com/minimaxir/subreddit-generator&#34; target=&#34;_blank&#34;&gt;helper script&lt;/a&gt; to automatically download the top &lt;em&gt;n&lt;/em&gt; Reddit submissions from a given subreddit over a given period of time. If you choose subreddits with similar linguistic styles in their titles, the subreddits will even blend together! Let&amp;rsquo;s play with the Top 20,000 Submissions in 2017 from each of &lt;a href=&#34;https://www.reddit.com/r/politics/&#34; target=&#34;_blank&#34;&gt;/r/politics&lt;/a&gt; and &lt;a href=&#34;https://www.reddit.com/r/technology/&#34; target=&#34;_blank&#34;&gt;/r/technology&lt;/a&gt;, which results in a 3.3MB file: about 3x as much data as the Shakespeare plays.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/text-neural-networks/reddit_data.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One last thing that textgenrnn can do that most char-rnn implementations can&amp;rsquo;t is generate a &lt;em&gt;word level&lt;/em&gt; model (thanks to Keras&amp;rsquo;s tokenizers), where the model uses the &lt;em&gt;n&lt;/em&gt; previous words/punctuation to predict the next word/punctuation. On the plus side, using only words prevents crazy typoes and since it predicts multiple &amp;ldquo;characters&amp;rdquo; at a time, &lt;code&gt;max_length&lt;/code&gt; can be reduced proportionally, dramatically speeding up training. There&amp;rsquo;s two downsides with this approach; since words are all lowercase and punctuation is its own token, the generated text cannot be immediately used without manual editing. Additionally, the model weights will be substantially larger than a character-level model since the word-level model has to store an embedding for each word (up to &lt;code&gt;max_words&lt;/code&gt;, which is 10,000 by default when the vocabulary size for a char-level model is 200-300).&lt;/p&gt;
&lt;p&gt;Another advantage of the Colaboratory notebook is that you can quickly adjust model parameters, upload a new file, and immediately start training it. We&amp;rsquo;ll set &lt;code&gt;&#39;line_delimited&#39;: True&lt;/code&gt; and &lt;code&gt;&#39;rnn_bidirectional&#39;: False&lt;/code&gt; since there aren&amp;rsquo;t specific rules. For word level training, let&amp;rsquo;s set &lt;code&gt;&#39;word_level&#39;: True&lt;/code&gt; and &lt;code&gt;&#39;max_length&#39;: 8&lt;/code&gt; to reflect the new training architecture. Since training length has been reduced to 1/5th, we can set &lt;code&gt;&#39;num_epochs&#39;: 50&lt;/code&gt; and &lt;code&gt;&#39;gen_epoch&#39;: 10&lt;/code&gt; to balance it out. Rerun the config cell to update parameters, upload the Reddit data file, and rerun training.&lt;/p&gt;
&lt;p&gt;The resulting model is much more well trained than the Shakespeare model, and here&amp;rsquo;s a few Reddit submission titles generated at a temperature of 1.0:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;report : 49 % of americans now believe all of the country ’ s effective
people like facebook like it &#39; s 650 of 1 %
uber accused of secretly - security popular service ( likely oklahoma )
equifax breach fallout : your salary is dead
sanders uses texas shooter &#39; s iphone sales
adobe videos will be used to sell the web
apple to hold cash for $ 500 service
fitbit just targeting solar energy
george bush &#39; s concept car ‘ goes for all the biggest controversy .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those look pretty good, although they may need a little editing before posting on social media.&lt;/p&gt;
&lt;h2 id=&#34;followup&#34;&gt;Followup&lt;/h2&gt;
&lt;p&gt;These examples only train the model for little time as a demo of textgenrnn&amp;rsquo;s fast learning; there&amp;rsquo;s nothing stopping you from increasing &lt;code&gt;num_epochs&lt;/code&gt; even more to further refine a model. However, from my experience, the training cell times out after &lt;strong&gt;4 hours&lt;/strong&gt;; set &lt;code&gt;num_epochs&lt;/code&gt; accordingly, although in my experience that&amp;rsquo;s all you need before the network converges.&lt;/p&gt;
&lt;p&gt;In practice, I used this Colaboratory notebook to train &lt;em&gt;many&lt;/em&gt; models for &lt;a href=&#34;https://www.reddit.com/r/SubredditNN/&#34; target=&#34;_blank&#34;&gt;/r/SubredditNN&lt;/a&gt;, a Reddit subreddit where only text-generating neural network bots trained on other subreddits. And the results are very funny:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/text-neural-networks/subredditnn.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although text generating neural networks aren&amp;rsquo;t at the point where they can &lt;a href=&#34;https://www.bloomberg.com/news/features/2018-05-17/i-tried-to-get-an-ai-to-write-this-story-paul-ford&#34; target=&#34;_blank&#34;&gt;write entire articles by themselves&lt;/a&gt;, there are still many opportunities to use it just for fun! And thanks to textgenrnn, it&amp;rsquo;s easy, fast, and cost-effective for anyone to do! Let me know if you make any interesting neural networks with textgenrnn and this Notebook!&lt;/p&gt;</description></item><item><title>Benchmarking TensorFlow on Cloud CPUs: Cheaper Deep Learning than Cloud GPUs</title><link>https://minimaxir.com/2017/07/cpu-or-gpu/</link><pubDate>Wed, 05 Jul 2017 09:00:00 -0700</pubDate><guid>https://minimaxir.com/2017/07/cpu-or-gpu/</guid><description>
&lt;p&gt;I&amp;rsquo;ve been working on a few personal deep learning projects with &lt;a href=&#34;https://github.com/fchollet/keras&#34; target=&#34;_blank&#34;&gt;Keras&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org&#34; target=&#34;_blank&#34;&gt;TensorFlow&lt;/a&gt;. However, training models for deep learning with cloud services such as &lt;a href=&#34;https://aws.amazon.com/ec2/&#34; target=&#34;_blank&#34;&gt;Amazon EC2&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/compute/&#34; target=&#34;_blank&#34;&gt;Google Compute Engine&lt;/a&gt; isn&amp;rsquo;t free, and as someone who is currently unemployed, I have to keep an eye on extraneous spending and be as cost-efficient as possible (please support my work on &lt;a href=&#34;https://www.patreon.com/minimaxir&#34; target=&#34;_blank&#34;&gt;Patreon&lt;/a&gt;!). I tried deep learning on the cheaper CPU instances instead of GPU instances to save money, and to my surprise, my model training was only slightly slower. As a result, I took a deeper look at the pricing mechanisms of these two types of instances to see if CPUs are more useful for my needs.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/compute/pricing#gpus&#34; target=&#34;_blank&#34;&gt;pricing of GPU instances&lt;/a&gt; on Google Compute Engine starts at &lt;strong&gt;$0.745/hr&lt;/strong&gt; (by attaching a $0.700/hr GPU die to a $0.045/hr n1-standard-1 instance). A couple months ago, Google &lt;a href=&#34;https://cloudplatform.googleblog.com/2017/05/Compute-Engine-machine-types-with-up-to-64-vCPUs-now-ready-for-your-production-workloads.html&#34; target=&#34;_blank&#34;&gt;announced&lt;/a&gt; CPU instances with up to 64 vCPUs on the modern Intel &lt;a href=&#34;https://en.wikipedia.org/wiki/Skylake_(microarchitecture)&#34; target=&#34;_blank&#34;&gt;Skylake&lt;/a&gt; CPU architecture. More importantly, they can also be used in &lt;a href=&#34;https://cloud.google.com/compute/docs/instances/preemptible&#34; target=&#34;_blank&#34;&gt;preemptible CPU instances&lt;/a&gt;, which live at most for 24 hours on GCE and can be terminated at any time (very rarely), but cost about &lt;em&gt;20%&lt;/em&gt; of the price of a standard instance. A preemptible n1-highcpu-64 instance with 64 vCPUs and 57.6GB RAM plus the premium for using Skylake CPUs is &lt;strong&gt;$0.509/hr&lt;/strong&gt;, about 2/3rds of the cost of the GPU instance.&lt;/p&gt;
&lt;p&gt;If the model training speed of 64 vCPUs is comparable to that of a GPU (or even slightly slower), it would be more cost-effective to use the CPUs instead. But that&amp;rsquo;s assuming the deep learning software and the GCE platform hardware operate at 100% efficiency; if they don&amp;rsquo;t (and they likely don&amp;rsquo;t), there may be &lt;em&gt;even more savings&lt;/em&gt; by scaling down the number of vCPUs and cost accordingly (a 32 vCPU instance with same parameters is half the price at &lt;strong&gt;$0.254/hr&lt;/strong&gt;, 16 vCPU at &lt;strong&gt;$0.127/hr&lt;/strong&gt;, etc).&lt;/p&gt;
&lt;p&gt;There aren&amp;rsquo;t any benchmarks for deep learning libraries with tons and tons of CPUs since there&amp;rsquo;s no demand, as GPUs are the &lt;a href=&#34;https://en.wikipedia.org/wiki/Occam%27s_razor&#34; target=&#34;_blank&#34;&gt;Occam&amp;rsquo;s razor&lt;/a&gt; solution to deep learning hardware. But what might make counterintuitive but economical sense is to use CPUs instead of GPUs for deep learning training because of the massive cost differential afforded by preemptible instances, thanks to Google&amp;rsquo;s &lt;a href=&#34;https://en.wikipedia.org/wiki/Economies_of_scale&#34; target=&#34;_blank&#34;&gt;economies of scale&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;I already have &lt;a href=&#34;https://github.com/minimaxir/deep-learning-cpu-gpu-benchmark&#34; target=&#34;_blank&#34;&gt;benchmarking scripts&lt;/a&gt; of real-world deep learning use cases, &lt;a href=&#34;https://github.com/minimaxir/keras-cntk-docker&#34; target=&#34;_blank&#34;&gt;Docker container environments&lt;/a&gt;, and results logging from my &lt;a href=&#34;http://minimaxir.com/2017/06/keras-cntk/&#34; target=&#34;_blank&#34;&gt;TensorFlow vs. CNTK article&lt;/a&gt;. A few minor tweaks allow the scripts to be utilized for both CPU and GPU instances by setting CLI arguments. I also rebuilt &lt;a href=&#34;https://github.com/minimaxir/keras-cntk-docker/blob/master/Dockerfile&#34; target=&#34;_blank&#34;&gt;the Docker container&lt;/a&gt; to support the latest version of TensorFlow (1.2.1), and created a &lt;a href=&#34;https://github.com/minimaxir/keras-cntk-docker/blob/master/Dockerfile-cpu&#34; target=&#34;_blank&#34;&gt;CPU version&lt;/a&gt; of the container which installs the CPU-appropriate TensorFlow library instead.&lt;/p&gt;
&lt;p&gt;There is a notable CPU-specific TensorFlow behavior; if you install from &lt;code&gt;pip&lt;/code&gt; (as the&lt;a href=&#34;https://www.tensorflow.org/install/&#34; target=&#34;_blank&#34;&gt; official instructions&lt;/a&gt; and tutorials recommend) and begin training a model in TensorFlow, you&amp;rsquo;ll see these warnings in the console:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/cpu-or-gpu/tensorflow-console.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In order to fix the warnings and benefit from these &lt;a href=&#34;https://en.wikipedia.org/wiki/SSE4#SSE4.2&#34; target=&#34;_blank&#34;&gt;SSE4.2&lt;/a&gt;/&lt;a href=&#34;https://en.wikipedia.org/wiki/Advanced_Vector_Extensions&#34; target=&#34;_blank&#34;&gt;AVX&lt;/a&gt;/&lt;a href=&#34;https://en.wikipedia.org/wiki/FMA_instruction_set&#34; target=&#34;_blank&#34;&gt;FMA&lt;/a&gt; optimizations, we &lt;a href=&#34;https://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions&#34; target=&#34;_blank&#34;&gt;compile TensorFlow from source&lt;/a&gt;, and I created a &lt;a href=&#34;https://github.com/minimaxir/keras-cntk-docker/blob/master/Dockerfile-cpu-compiled&#34; target=&#34;_blank&#34;&gt;third Docker container&lt;/a&gt; to do just that. When training models in the new container, &lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues/10689&#34; target=&#34;_blank&#34;&gt;most&lt;/a&gt; of the warnings no longer show, and (spoiler alert) there is indeed a speed boost in training time.&lt;/p&gt;
&lt;p&gt;Therefore, we can test three major cases with Google Compute Engine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Tesla K80 GPU instance.&lt;/li&gt;
&lt;li&gt;A 64 Skylake vCPU instance where TensorFlow is installed via &lt;code&gt;pip&lt;/code&gt; (along with testings at 8/16/32 vCPUs).&lt;/li&gt;
&lt;li&gt;A 64 Skylake vCPU instance where TensorFlow is compiled (&lt;code&gt;cmp&lt;/code&gt;) with CPU instructions (+ 8/16/32 vCPUs).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;For each model architecture and software/hardware configuration, I calculate the &lt;strong&gt;total training time relative to the GPU instance training&lt;/strong&gt; for running the model training for the provided test script. In all cases, the GPU &lt;em&gt;should&lt;/em&gt; be the fastest training configuration, and systems with more processors should train faster than those with fewer processors.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start using the &lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34; target=&#34;_blank&#34;&gt;MNIST dataset&lt;/a&gt; of handwritten digits plus the common multilayer perceptron (MLP) architecture, with dense fully-connected layers. Lower training time is better. All configurations below the horizontal dotted line are better than GPUs; all configurations above the dotted line are worse than GPUs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/cpu-or-gpu/dl-cpu-gpu-5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, the GPU is the fastest out of all the platform configurations, but there are other curious trends: the performance between 32 vCPUs and 64 vCPUs is similar, and the compiled TensorFlow library is indeed a significant improvement in training speed &lt;em&gt;but only for 8 and 16 vCPUs&lt;/em&gt;. Perhaps there are overheads negotiating information between vCPUs that eliminate the performance advantages of more vCPUs, and perhaps these overheads are &lt;em&gt;different&lt;/em&gt; with the CPU instructions of the compiled TensorFlow. In the end, it&amp;rsquo;s a &lt;a href=&#34;https://en.wikipedia.org/wiki/Black_box&#34; target=&#34;_blank&#34;&gt;black box&lt;/a&gt;, which is why I prefer black box benchmarking all configurations of hardware instead of theorycrafting.&lt;/p&gt;
&lt;p&gt;Since the difference between training speeds of different vCPU counts is minimal, there is definitely an advantage by scaling down. For each model architecture and configuration, I calculate a &lt;strong&gt;normalized training cost relative to the cost of GPU instance training&lt;/strong&gt;. Because GCE instance costs are prorated (unlike Amazon EC2), we can simply calculate experiment cost by multiplying the total number of seconds the experiment runs by the cost of the instance (per second). Ideally, we want to &lt;em&gt;minimize&lt;/em&gt; cost.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/cpu-or-gpu/dl-cpu-gpu-6.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lower CPU counts are &lt;em&gt;much&lt;/em&gt; more cost-effective for this problem, when going as low as possible is better.&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s look at the same dataset with a convolutional neural network (CNN) approach for digit classification:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/cpu-or-gpu/dl-cpu-gpu-7.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/cpu-or-gpu/dl-cpu-gpu-8.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;GPUs are unsurprisingly more than twice as fast as any CPU approach at CNNs, but cost structures are still the same, except that 64 vCPUs are &lt;em&gt;worse&lt;/em&gt; than GPUs cost-wise, with 32 vCPUs training even faster than with 64 vCPUs.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s go deeper with CNNs and look at the &lt;a href=&#34;https://www.cs.toronto.edu/%7Ekriz/cifar.html&#34; target=&#34;_blank&#34;&gt;CIFAR-10&lt;/a&gt; image classification dataset, and a model which utilizes a deep covnet + a multilayer perceptron and ideal for image classification (similar to the &lt;a href=&#34;https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3&#34; target=&#34;_blank&#34;&gt;VGG-16&lt;/a&gt; architecture).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/cpu-or-gpu/dl-cpu-gpu-9.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/cpu-or-gpu/dl-cpu-gpu-10.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similar behaviors as in the simple CNN case, although in this instance all CPUs perform better with the compiled TensorFlow library.&lt;/p&gt;
&lt;p&gt;The fasttext algorithm, used here on the &lt;a href=&#34;http://ai.stanford.edu/%7Eamaas/data/sentiment/&#34; target=&#34;_blank&#34;&gt;IMDb reviews dataset&lt;/a&gt; to determine whether a review is positive or negative, classifies text extremely quickly relative to other methods.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/cpu-or-gpu/dl-cpu-gpu-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/cpu-or-gpu/dl-cpu-gpu-4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case, GPUs are much, much faster than CPUs. The benefit of lower numbers of CPU isn&amp;rsquo;t as dramatic; although as an aside, the &lt;a href=&#34;https://github.com/facebookresearch/fastText&#34; target=&#34;_blank&#34;&gt;official fasttext implementation&lt;/a&gt; is &lt;em&gt;designed&lt;/em&gt; for large amounts of CPUs and handles parallelization much better.&lt;/p&gt;
&lt;p&gt;The Bidirectional long-short-term memory (LSTM) architecture is great for working with text data like IMDb reviews, but after my previous benchmark article, &lt;a href=&#34;https://news.ycombinator.com/item?id=14538086&#34; target=&#34;_blank&#34;&gt;commenters on Hacker News&lt;/a&gt; noted that TensorFlow uses an inefficient implementation of the LSTM on the GPU, so perhaps the difference will be more notable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/cpu-or-gpu/dl-cpu-gpu-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/cpu-or-gpu/dl-cpu-gpu-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Wait, what? GPU training of Bidirectional LSTMs is &lt;em&gt;twice as slow&lt;/em&gt; as any CPU configuration? Wow. (In fairness, the benchmark uses the Keras LSTM default of &lt;code&gt;implementation=0&lt;/code&gt; which is better on CPUs while &lt;code&gt;implementation=2&lt;/code&gt; is better on GPUs, but it shouldn&amp;rsquo;t result in that much of a differential)&lt;/p&gt;
&lt;p&gt;Lastly, LSTM text generation of &lt;a href=&#34;https://en.wikipedia.org/wiki/Friedrich_Nietzsche&#34; target=&#34;_blank&#34;&gt;Nietzsche&amp;rsquo;s&lt;/a&gt; &lt;a href=&#34;https://s3.amazonaws.com/text-datasets/nietzsche.txt&#34; target=&#34;_blank&#34;&gt;writings&lt;/a&gt; follows similar patterns to the other architectures, but without the drastic hit to the GPU.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/cpu-or-gpu/dl-cpu-gpu-11.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://minimaxir.com/img/cpu-or-gpu/dl-cpu-gpu-12.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;As it turns out, using 64 vCPUs is &lt;em&gt;bad&lt;/em&gt; for deep learning as current software/hardware architectures can&amp;rsquo;t fully utilize all of them, and it often results in the exact same performance (or &lt;em&gt;worse&lt;/em&gt;) than with 32 vCPUs. In terms balancing both training speed and cost, training models with &lt;strong&gt;16 vCPUs + compiled TensorFlow&lt;/strong&gt; seems like the winner. The 30%-40% speed boost of the compiled TensorFlow library was an unexpected surprise, and I&amp;rsquo;m shocked Google doesn&amp;rsquo;t offer a precompiled version of TensorFlow with these CPU speedups since the gains are nontrivial.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s worth nothing that the cost advantages shown here are &lt;em&gt;only&lt;/em&gt; possible with preemptible instances; regular high-CPU instances on Google Compute Engine are about 5x as expensive, and as a result eliminate the cost benefits completely. Hooray for economies of scale!&lt;/p&gt;
&lt;p&gt;A major implicit assumption with the cloud CPU training approach is that you don&amp;rsquo;t need a trained model ASAP. In professional use cases, time may be too valuable to waste, but in personal use cases where someone can just leave a model training overnight, it&amp;rsquo;s a very, very good and cost-effective option, and one that I&amp;rsquo;ll now utilize.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;All scripts for running the benchmark are available in &lt;a href=&#34;https://github.com/minimaxir/deep-learning-cpu-gpu-benchmark&#34; target=&#34;_blank&#34;&gt;this GitHub repo&lt;/a&gt;. You can view the R/ggplot2 code used to process the logs and create the visualizations in &lt;a href=&#34;http://minimaxir.com/notebooks/deep-learning-cpu-gpu/&#34; target=&#34;_blank&#34;&gt;this R Notebook&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>