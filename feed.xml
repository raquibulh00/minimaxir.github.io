<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>minimaxir | Max Woolf's Blog</title>
    <description>A blog by Max Woolf about startups, technology, and blogging. It's so meta, even this acronym.</description>
    <link>http://minimaxir.com/</link>
    <atom:link href="http://minimaxir.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 26 Jun 2017 09:02:27 -0700</pubDate>
    <lastBuildDate>Mon, 26 Jun 2017 09:02:27 -0700</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>Predicting the Success of a Reddit Submission with Deep Learning and Keras</title>
        <description>&lt;p&gt;I&amp;rsquo;ve been trying to figure out what makes a &lt;a href=&quot;https://www.reddit.com&quot;&gt;Reddit&lt;/a&gt; submission &amp;ldquo;good&amp;rdquo; for years. If we assume the number of upvotes on a submission is a fair proxy for submission quality, optimizing a statistical model for Reddit data with submission score as a response variable might lead to interesting (and profitable) insights when transferred into other domains, such as Facebook Likes and Twitter Favorites.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-deep-learning/reddit-example.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;An important part of a Reddit submission is the submission &lt;strong&gt;title&lt;/strong&gt;. Like news headlines, a catchy title will make a user &lt;a href=&quot;http://minimaxir.com/2015/10/reddit-topwords/&quot;&gt;more inclined&lt;/a&gt; to engage with a submission and potentially upvote.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-topwords/mean-054-Fitness.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Additionally, the &lt;strong&gt;time when the submission is made&lt;/strong&gt; is &lt;a href=&quot;http://minimaxir.com/2015/10/reddit-bigquery/&quot;&gt;important&lt;/a&gt;; submitting when user activity is the highest tends to lead to better results if you are trying to maximize exposure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-bigquery/reddit-bigquery-2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The actual &lt;strong&gt;content&lt;/strong&gt; of the Reddit submission such as  images/links to a website is likewise important, but good content is relatively difficult to optimize.&lt;/p&gt;

&lt;p&gt;Can the magic of deep learning reconcile these concepts and create a model which can predict if a submission is a good submission? Thanks to &lt;a href=&quot;https://github.com/fchollet/keras&quot;&gt;Keras&lt;/a&gt;, performing deep learning on a very large number of Reddit submissions is actually pretty easy. Performing it &lt;em&gt;well&lt;/em&gt; is a different story.&lt;/p&gt;

&lt;h2&gt;Getting the Data + Feature Engineering&lt;/h2&gt;

&lt;p&gt;It&amp;rsquo;s difficult to retrieve the content of millions of Reddit submissions at scale (ethically), so let&amp;rsquo;s initially start by building a model using submissions on &lt;a href=&quot;https://www.reddit.com/r/AskReddit/&quot;&gt;/r/AskReddit&lt;/a&gt;: Reddit&amp;rsquo;s largest subreddit which receives 8,000+ submissions each day. /r/AskReddit is a self-post only subreddit with no external links, allowing us to focus on only the submission title and timing.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://minimaxir.com/2015/10/reddit-bigquery/&quot;&gt;As always&lt;/a&gt;, we can collect large amounts of Reddit data from the public Reddit dataset on &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;BigQuery&lt;/a&gt;. The submission &lt;code&gt;title&lt;/code&gt; is available by default. The raw timestamp of the submission is also present, allowing us to extract the &lt;code&gt;hour&lt;/code&gt; of submission (adjusted to Eastern Standard Time) and &lt;code&gt;dayofweek&lt;/code&gt;, as used in the heatmap above. But why stop there? Since /r/AskReddit receives hundreds of submissions &lt;em&gt;every hour&lt;/em&gt; on average, we should look at the &lt;code&gt;minute&lt;/code&gt; level to see if there are any deeper trends (e.g. there are only 30 slots available on the first page of /new and since there is so much submission activity, it might be more advantageous to submit during off-peak times). Lastly, to account for potential changes in behavior as the year progresses, we should add a &lt;code&gt;dayofyear&lt;/code&gt; feature, where January 1st = 1, January 2nd = 2, etc which can also account for variance due to atypical days like holidays.&lt;/p&gt;

&lt;p&gt;Instead of predicting the raw number on upvotes of the Reddit submission (as the distribution of submission scores is heavily skewed), we should predict &lt;strong&gt;whether or not the submission is good&lt;/strong&gt;, shaping the problem as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot;&gt;logistic regression&lt;/a&gt;. In this case, let&amp;rsquo;s define a &amp;ldquo;good submission&amp;rdquo; as one whose score is equal to or above the &lt;strong&gt;50th percentile (median) of all submissions&lt;/strong&gt; in /r/AskReddit. Unfortunately, the median score ends up being &lt;strong&gt;2 points&lt;/strong&gt;; although &amp;ldquo;one upvote&amp;rdquo; might be a low threshold for a &amp;ldquo;good&amp;rdquo; submission, it splits the dataset into 64% bad submissions, 36% good submissions, and setting the percentile threshold higher will result in a very unbalanced dataset for model training (a score of 2+ also implies that the submission did not get downvoted to death, which is useful). &lt;/p&gt;

&lt;p&gt;Gathering all &lt;strong&gt;976,538 /r/AskReddit submissions&lt;/strong&gt; from January 2017 to April 2017 should be enough data for this project. Here&amp;rsquo;s the final BigQuery:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span class=&quot;o&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;standardSQL&lt;/span&gt; 
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;CAST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FORMAT_TIMESTAMP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;%H&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TIMESTAMP_SECONDS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;created_utc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;America/New_York&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INT64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hour&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;CAST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FORMAT_TIMESTAMP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;%M&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TIMESTAMP_SECONDS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;created_utc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;America/New_York&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INT64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;minute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;CAST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FORMAT_TIMESTAMP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;%w&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TIMESTAMP_SECONDS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;created_utc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;America/New_York&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INT64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dayofweek&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;CAST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FORMAT_TIMESTAMP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;%j&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TIMESTAMP_SECONDS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;created_utc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;America/New_York&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INT64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dayofyear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;IF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PERCENT_RANK&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OVER&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ASC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_top_submission&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigquery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reddit_posts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*`&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_TABLE_SUFFIX&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BETWEEN&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;2017_01&amp;#39;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;2017_04&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subreddit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;AskReddit&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/img/reddit-deep-learning/bigquery.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;h2&gt;Model Architecture&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;If you want to see the detailed data transformations and Keras code examples/outputs for this post, you can view &lt;a href=&quot;https://github.com/minimaxir/predict-reddit-submission-success/blob/master/predict_askreddit_submission_success_timing.ipynb&quot;&gt;this Jupyter Notebook&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Text processing is a good use case for deep learning, as it can identify relationships between words where older methods like &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt;tf-idf&lt;/a&gt; can&amp;rsquo;t. Keras, a high level deep-learning framework on top of lower frameworks like &lt;a href=&quot;https://www.tensorflow.org&quot;&gt;TensorFlow&lt;/a&gt;, can easily convert a list of texts to a &lt;a href=&quot;https://keras.io/preprocessing/sequence/&quot;&gt;padded sequence&lt;/a&gt; of &lt;a href=&quot;https://keras.io/preprocessing/text/&quot;&gt;index tokens&lt;/a&gt; that can interact with deep learning models, along with many other benefits. Data scientists often use &lt;a href=&quot;https://en.wikipedia.org/wiki/Recurrent_neural_network&quot;&gt;recurrent neural networks&lt;/a&gt; that can &amp;ldquo;learn&amp;rdquo; for classifying text. However &lt;a href=&quot;https://github.com/facebookresearch/fastText&quot;&gt;fasttext&lt;/a&gt;, a newer algorithm from researchers at Facebook, can perform classification tasks at an &lt;a href=&quot;http://minimaxir.com/2017/06/keras-cntk/&quot;&gt;order of magnitude faster&lt;/a&gt; training time than RNNs, with similar predictive performance.&lt;/p&gt;

&lt;p&gt;fasttext works by &lt;a href=&quot;https://arxiv.org/abs/1607.01759&quot;&gt;averaging word vectors&lt;/a&gt;. In this Reddit model architecture inspired by the &lt;a href=&quot;https://github.com/fchollet/keras/blob/master/examples/imdb_fasttext.py&quot;&gt;official Keras fasttext example&lt;/a&gt;, each word in a Reddit submission title (up to 20) is mapped to a 50-dimensional vector from an Embeddings layer of up to 40,000 words. The Embeddings layer is &lt;a href=&quot;https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html&quot;&gt;initialized&lt;/a&gt; with &lt;a href=&quot;https://nlp.stanford.edu/projects/glove/&quot;&gt;GloVe word embeddings&lt;/a&gt; pre-trained on billions of words to give the model a good start. All the word vectors for a given Reddit submission title are averaged together, and then a Dense fully-connected layer outputs a probability the given text is a good submission. The gradients then backpropagate and improve the word embeddings for future batches during training.&lt;/p&gt;

&lt;p&gt;Keras has a &lt;a href=&quot;https://keras.io/visualization/&quot;&gt;convenient utility&lt;/a&gt; to visualize deep learning models:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-deep-learning/model_shapes-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;However, the first output above is the &lt;em&gt;auxiliary output&lt;/em&gt; for &lt;a href=&quot;https://en.wikipedia.org/wiki/Regularization_(mathematics)&quot;&gt;regularizing&lt;/a&gt; the word embeddings; we still have to incorporate the submission timing data into the model.&lt;/p&gt;

&lt;p&gt;Each of the four timing features (hour, minute, day of week, day of year) receives its own Embeddings layer, outputting a 64D vector. This allows the features to learn latent characteristics which may be missed using traditional &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html&quot;&gt;one-hot encoding&lt;/a&gt; for categorical data in machine learning problems.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-deep-learning/model_shapes-2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The 50D word average vector is concatenated with the four vectors above, resulting in a 306D vector. This combined vector is connected to another fully-connected layer which can account for hidden interactions between all five input features (plus &lt;a href=&quot;https://keras.io/layers/normalization/&quot;&gt;batch normalization&lt;/a&gt;, which improves training speed for Dense layers). Then the model outputs a final probability prediction: the &lt;em&gt;main output&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-deep-learning/model_shapes-3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The final model:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-deep-learning/model.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;All of this sounds difficult to implement, but Keras&amp;rsquo;s &lt;a href=&quot;https://keras.io/getting-started/functional-api-guide/&quot;&gt;functional API&lt;/a&gt; ensures that adding each layer and linking them together can be done in a single line of code each.&lt;/p&gt;

&lt;h2&gt;Training Results&lt;/h2&gt;

&lt;p&gt;Because the model uses no recurrent layers, it trains fast enough on a CPU despite the large dataset size.&lt;/p&gt;

&lt;p&gt;We split the full dataset into 80%/20% training/test datasets, training the model on the former and testing the model against the latter. Keras trains a model with a simple &lt;code&gt;fit&lt;/code&gt; command and trains for 20 epochs, where one epoch represents an entire pass of the training set.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-deep-learning/fit.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a lot happening in the console output due to the architecture, but the main metrics of interest are the &lt;code&gt;main_out_acc&lt;/code&gt;, the accuracy of the training set through the main output, and &lt;code&gt;val_main_out_acc&lt;/code&gt;, the accuracy of the test set. Ideally, the accuracy of both should increase as training progresses. However, the test accuracy &lt;em&gt;must&lt;/em&gt; be better than the 64% baseline (if we just say all /r/AskReddit submissions are bad), otherwise this model is unhelpful.&lt;/p&gt;

&lt;p&gt;Keras&amp;rsquo;s &lt;a href=&quot;https://keras.io/callbacks/#csvlogger&quot;&gt;CSVLogger&lt;/a&gt; trivially logs all these metrics to a CSV file. Plotting the results of the 20 epochs:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reddit-deep-learning/predict-reddit-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The test accuracy does indeed beat the 64% baseline; however, test accuracy &lt;em&gt;decreases&lt;/em&gt; as training progresses. This is a sign of &lt;a href=&quot;https://en.wikipedia.org/wiki/Overfitting&quot;&gt;overfitting&lt;/a&gt;, possibly due to the potential disparity between texts in the training and test sets. In deep learning, you can account for overfitting by adding &lt;a href=&quot;https://keras.io/layers/core/#dropout&quot;&gt;Dropout&lt;/a&gt; to relevant layers, but in my testing it did not help.&lt;/p&gt;

&lt;h2&gt;Using The Model To Optimize Reddit Submissions&lt;/h2&gt;

&lt;p&gt;At the least, we now have a model that understands the latent characteristics of an /r/AskReddit submission. But how do you apply the model &lt;em&gt;in practical, real-world situations&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s take a random /r/AskReddit submission: &lt;a href=&quot;https://www.reddit.com/r/AskReddit/comments/5odcpd/which_movies_plot_would_drastically_change_if_you/&quot;&gt;Which movie&amp;rsquo;s plot would drastically change if you removed a letter from its title?&lt;/a&gt;, submitted Monday, January 16th at 3:46 PM EST and receiving 4 upvotes (a &amp;ldquo;good&amp;rdquo; submission in context of this model). Plugging those input variables into the trained model results in a &lt;strong&gt;0.669&lt;/strong&gt; probability of it being considered a good submission, which is consistent with the true results.&lt;/p&gt;

&lt;p&gt;But what if we made &lt;em&gt;minor, iterative changes&lt;/em&gt; to the title  while keeping the time submitted unchanged? Can we improve this probability?&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Drastically&amp;rdquo; is a silly adjective; removing it and using the title &lt;strong&gt;Which movie&amp;rsquo;s plot would change if you removed a letter from its title?&lt;/strong&gt; results in a greater probability of &lt;strong&gt;0.682&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Removed&amp;rdquo; is &lt;a href=&quot;http://www.ef.edu/english-resources/english-grammar/conditional/&quot;&gt;grammatically incorrect&lt;/a&gt;; fixing the issue and using the title &lt;strong&gt;Which movie&amp;rsquo;s plot would change if you remove a letter from its title?&lt;/strong&gt; results in a greater probability of &lt;strong&gt;0.692&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Which&amp;rdquo; is also &lt;a href=&quot;https://www.englishclub.com/vocabulary/wh-question-words.htm&quot;&gt;grammatically incorrect&lt;/a&gt;; fixing the issue and using the title &lt;strong&gt;What movie&amp;rsquo;s plot would change if you remove a letter from its title?&lt;/strong&gt; results in a greater probability of &lt;strong&gt;0.732&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Although adjectives are sometimes redundant, they can add an intriguing emphasis; adding a &amp;ldquo;single&amp;rdquo; and using the title &lt;strong&gt;What movie&amp;rsquo;s plot would change if you remove a single letter from its title?&lt;/strong&gt; results in a greater probability of &lt;strong&gt;0.753&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Not bad for a little workshopping!&lt;/p&gt;

&lt;p&gt;Now that we have an improved title, we can find an optimal time to make the submission through brute force by calculating the probabilities for all combinations of hour, minute, and day of week (and offsetting the day of year appropriately). After doing so, I discovered that making the submission on the previous Sunday at 10:55 PM EST results in the maximum probability possible of being a good submission at &lt;strong&gt;0.841&lt;/strong&gt; (the other top submission times are at various other minutes during that hour; the best time on a different day is the following Tuesday at 4:05 AM EST with a probability of &lt;strong&gt;0.823&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;In all, this model of Reddit submission success prediction is a proof of concept; there are many, &lt;em&gt;many&lt;/em&gt; optimizations that can be done on the feature engineering side and on the data collection side (especially if we want to model subreddits other than /r/AskReddit). Predicting which submissions go viral instead of just predicting which submissions receive atleast one upvote is another, more advanced problem entirely.&lt;/p&gt;

&lt;p&gt;Thanks to the high-level abstractions and utility functions of Keras, I was able to prototype the initial model in an afternoon instead of the weeks/months required for academic papers and software applications in this area. At the least, this little experiment serves as an example of applying Keras to a real-world dataset, and the tradeoffs that result when deep learning can&amp;rsquo;t magically solve everything. But that doesn&amp;rsquo;t mean my experiments on the Reddit data were unproductive; on the contrary, I now have a few new clever ideas how to fix some of the issues discovered, which I hope to implement soon.&lt;/p&gt;

&lt;p&gt;Again, I strongly recommend reading the data transformations and Keras code examples in &lt;a href=&quot;https://github.com/minimaxir/predict-reddit-submission-success/blob/master/predict_askreddit_submission_success_timing.ipynb&quot;&gt;this Jupyter Notebook&lt;/a&gt; for more information into the methodology, as building modern deep learning models is more intuitive and less arcane than what thought pieces on Medium imply.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;You can view the R and ggplot2 code used to visualize the model data in &lt;a href=&quot;http://minimaxir.com/notebooks/predict-reddit-submission-success/&quot;&gt;this R Notebook&lt;/a&gt;, including 2D projections of the Embedding layers not in this article. You can also view the images/data used for this post in &lt;a href=&quot;https://github.com/minimaxir/predict-reddit-submission-success&quot;&gt;this GitHub repository&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You are free to use the data visualizations/model architectures from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 26 Jun 2017 09:00:00 -0700</pubDate>
        <link>http://minimaxir.com/2017/06/reddit-deep-learning/</link>
        <guid isPermaLink="true">http://minimaxir.com/2017/06/reddit-deep-learning/</guid>
        
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>The Decline of Imgur on Reddit and the Rise of Reddit's Native Image Hosting</title>
        <description>&lt;p&gt;Last week, Bloomberg &lt;a href=&quot;https://www.bloomberg.com/news/articles/2017-06-17/reddit-said-to-be-raising-funds-valuing-startup-at-1-7-billion&quot;&gt;reported&lt;/a&gt; that Reddit was raising about $150 Million in venture capital at a valuation of $1.7 billion. Since Reddit&amp;rsquo;s data is &lt;a href=&quot;http://minimaxir.com/2015/10/reddit-bigquery/&quot;&gt;public on BigQuery&lt;/a&gt;, I quickly checked if there were any recent user engagement growth spurts which could justify such a high worth. Here&amp;rsquo;s an example BigQuery which aggregates the total number of Reddit submissions made for each month until the end of April 2017:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span class=&quot;o&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;standardSQL&lt;/span&gt; 
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATE_TRUNC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TIMESTAMP_SECONDS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;created_utc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;MONTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;COUNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_submissions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bigquery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reddit_posts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*`&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_TABLE_SUFFIX&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BETWEEN&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;2016_01&amp;#39;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;2017_04&amp;#39;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;OR&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_TABLE_SUFFIX&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;full_corpus_201512&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mon&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mon&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/img/imgur-decline/reddit-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;As it turns out, Reddit did indeed get a large boost in activity toward the end of 2016, likely due to the &lt;em&gt;heated&lt;/em&gt; discussions and events around the &lt;a href=&quot;https://en.wikipedia.org/wiki/United_States_presidential_election,_2016&quot;&gt;U.S. Presidential Election&lt;/a&gt;. But Reddit has maintained the growth rate since then, which is very appealing to potential investors.&lt;/p&gt;

&lt;p&gt;How are other sites benefiting from Reddit&amp;rsquo;s growth? &lt;a href=&quot;http://imgur.com&quot;&gt;Imgur&lt;/a&gt;, an image-host developed to be the &lt;em&gt;de facto&lt;/em&gt; image hosting service for Reddit, shared in Reddit&amp;rsquo;s continual growth&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/imgur-decline/reddit-2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;until mid-2016, when Imgur submission activity abruptly dropped. What happened?&lt;/p&gt;

&lt;p&gt;Coincidentally in mid-2016, Reddit &lt;a href=&quot;https://techcrunch.com/2016/05/25/reddit-image-uploads/&quot;&gt;made itself&lt;/a&gt; an image host for submissions to the site. Initially limited to uploads via the iOS/Android apps, Reddit then allowed desktop users to upload images through a &lt;a href=&quot;https://www.reddit.com/r/changelog/comments/4kuk2j/reddit_change_introducing_image_uploading_beta/&quot;&gt;beta rollout&lt;/a&gt; starting May 24th, and a full &lt;a href=&quot;https://www.reddit.com/r/announcements/comments/4p5dm9/image_hosting_on_reddit/&quot;&gt;sitewide release&lt;/a&gt; on June 21st.&lt;/p&gt;

&lt;p&gt;How many Reddit-hosted image submissions are there compared to the number of Imgur submissions?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/imgur-decline/reddit-3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Wow, native Reddit images caught on.&lt;/p&gt;

&lt;h2&gt;Market Share&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/img/imgur-decline/pics.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Did the rise of Reddit-hosted images cause the decline of Imgur on Reddit? Let&amp;rsquo;s look at the daily number of Imgur submissions and Reddit-hosted Image submissions from December 2015 to April 2017, normalized by the total number of sitewide submissions on that day. This gives us a Reddit &amp;ldquo;market share&amp;rdquo; metric for both services. &lt;/p&gt;

&lt;p&gt;Additionally, we can plot vertical lines representing the dates when Reddit-hosted images rolled out in the limited beta release and the full sitewide release to see if there is a link between those events and submission behavior.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/imgur-decline/reddit-4.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Before Reddit added native image hosting, Imgur accounted for 15% of all submissions to Reddit. Now it&amp;rsquo;s below 9%. More Reddit-hosted images are being shared on Reddit than images from Imgur.&lt;/p&gt;

&lt;p&gt;Instead of looking at all of Reddit, where spam subreddits could skew the results, we can also look at the largest image-only subreddits: &lt;a href=&quot;https://www.reddit.com/r/pics/&quot;&gt;/r/pics&lt;/a&gt; and &lt;a href=&quot;https://www.reddit.com/r/gifs/&quot;&gt;/r/gifs&lt;/a&gt;, both of which were a part of the beta rollout.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/imgur-decline/reddit-5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Here, the impact of the two rollouts is much noticeable, with immediate increases in Reddit-hosted image market share after each rollout, and proportional decreases in Imgur market share. The growth rate after the beta release is flat for both services, but when Reddit image hosting becomes sitewide, the market shares of Reddit-hosted/Imgur images increase/decrease linearly over time once users officially learn that the native image upload functionality exists. And these trends do not appear to be slowing down.&lt;/p&gt;

&lt;h2&gt;A Silver Lining?&lt;/h2&gt;

&lt;p&gt;Obviously Imgur does not like losing a &lt;em&gt;large&lt;/em&gt; chunk of traffic, but there&amp;rsquo;s a possibility that this outcome will be better for the business than what&amp;rsquo;s implied from the charts above.&lt;/p&gt;

&lt;p&gt;Hosting images on the internet isn&amp;rsquo;t free, and bandwidth costs are the primary reason dedicated image hosts have died off over the years. Direct image links which show the user only the image and nothing else are convenient, but they are pure loss for the service. That&amp;rsquo;s why image hosts encourage linking to the image on a landing page of the website, filled with ads which generate an expected revenue greater than the cost of serving the image.&lt;/p&gt;

&lt;p&gt;After a user uploads an image to Imgur on the desktop, the user is given two share links that can be submitted to sites like Reddit: an image link that goes to the image + ads, and a direct link to the image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/imgur-decline/imgur_direct.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Recently, Imgur has &lt;a href=&quot;https://www.reddit.com/r/assholedesign/comments/5gs96k/just_show_me_the_fucking_image_imgur/&quot;&gt;pushed app downloads&lt;/a&gt; when visiting the site on an iOS/Android device, including &lt;a href=&quot;https://www.reddit.com/r/assholedesign/comments/695efj/upload_image_on_imgur_mobile_has_been_replaced_by/&quot;&gt;disabling uploads&lt;/a&gt; in the mobile browser. When sharing an image from the Imgur app, the &lt;em&gt;only&lt;/em&gt; way to share an image is through the image link, which could lead to an increase in the proportion of ad-filled Imgur image links on Reddit. Said increase could counteract the decrease in total Imgur submissions, and Imgur could actually come out ahead.&lt;/p&gt;

&lt;p&gt;With BigQuery, we can check the percentage of all Imgur submissions to Reddit which are direct links and the percentage which are indirect/lead to a landing page, and see if the ratio changes along the same time horizon used above:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/imgur-decline/reddit-6.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Welp. No significant change in the ratio over time, eliminating that possible silver lining.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Note that the decline of Imgur on Reddit says nothing about Imgur as a business; it&amp;rsquo;s entirely possible that Imgur&amp;rsquo;s traffic on the main site itself is sufficient for growth. But the loss of Reddit traffic certainly can&amp;rsquo;t be ignored, and it&amp;rsquo;s interesting to visualize how quickly a service can be replaced when there&amp;rsquo;s an equivalent native feature.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s worth nothing that new competitors in the image space such as &lt;a href=&quot;https://giphy.com&quot;&gt;Giphy&lt;/a&gt; utilize image hosting as a &lt;em&gt;secondary&lt;/em&gt; service. Instead, they focus on building a repository of images which can be licensed and accessed programmatically by other services like Slack, Facebook, and Twitter. And Giphy has raised &lt;a href=&quot;https://www.crunchbase.com/organization/giphy#/entity&quot;&gt;$150 Million&lt;/a&gt; total with this approach, so perhaps the image hosting market itself has indeed changed.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;You can view the R, ggplot2 code, and BigQueries used to visualize the Reddit data in &lt;a href=&quot;http://minimaxir.com/notebooks/imgur-decline/&quot;&gt;this R Notebook&lt;/a&gt;. You can also view the images/data used for this post in &lt;a href=&quot;https://github.com/minimaxir/imgur-decline&quot;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 20 Jun 2017 08:00:00 -0700</pubDate>
        <link>http://minimaxir.com/2017/06/imgur-decline/</link>
        <guid isPermaLink="true">http://minimaxir.com/2017/06/imgur-decline/</guid>
        
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Benchmarking CNTK on Keras: is it Better at Deep Learning than TensorFlow?</title>
        <description>&lt;style&gt;
div#htmlwidget_container {
  margin-bottom: 20px;
}
&lt;/style&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/fchollet/keras&quot;&gt;Keras&lt;/a&gt; is a high-level open-source framework for deep learning, maintained by Fran√ßois Chollet, that abstracts the massive amounts of configuration and matrix algebra needed to build production-quality deep learning models. The Keras API abstracts a lower-level deep learning framework like &lt;a href=&quot;https://github.com/Theano/Theano&quot;&gt;Theano&lt;/a&gt; or Google&amp;rsquo;s &lt;a href=&quot;https://www.tensorflow.org&quot;&gt;TensorFlow&lt;/a&gt; framework. Switching between these backends is only a matter of &lt;a href=&quot;https://keras.io/backend/&quot;&gt;setting flags&lt;/a&gt;; no front-end code changes necessary.&lt;/p&gt;

&lt;p&gt;But while Google has received a lot of publicity with TensorFlow, Microsoft has been quietly releasing their own machine learning frameworks open-source. There is &lt;a href=&quot;https://github.com/Microsoft/LightGBM&quot;&gt;LightGBM&lt;/a&gt;, presented as an alternative to the extremely famous &lt;a href=&quot;https://github.com/dmlc/xgboost&quot;&gt;xgboost&lt;/a&gt; framework. Now, there is &lt;a href=&quot;https://github.com/Microsoft/CNTK&quot;&gt;CNTK&lt;/a&gt; (Microsoft Cognitive Toolkit), released at v2.0 a &lt;a href=&quot;https://docs.microsoft.com/en-us/cognitive-toolkit/ReleaseNotes/CNTK_2_0_Release_Notes&quot;&gt;couple weeks ago&lt;/a&gt;, which markets strong performance in both accuracy and speed even when &lt;a href=&quot;https://docs.microsoft.com/en-us/cognitive-toolkit/reasons-to-switch-from-tensorflow-to-cntk&quot;&gt;compared&lt;/a&gt; to TensorFlow.&lt;/p&gt;

&lt;p&gt;CNTK v2.0 also has a key feature: Keras compatibility. And just last week, support for the CNTK backend &lt;a href=&quot;https://github.com/fchollet/keras/pull/6800&quot;&gt;was merged&lt;/a&gt; into the official Keras repository.&lt;/p&gt;

&lt;p&gt;Microsoft employees &lt;a href=&quot;https://news.ycombinator.com/item?id=14470967&quot;&gt;commented on Hacker News&lt;/a&gt; that simply changing the backend of Keras from TensorFlow to CNTK would result in a performance boost. So let&amp;rsquo;s put that to the test.&lt;/p&gt;

&lt;h2&gt;Deep Learning in the Cloud&lt;/h2&gt;

&lt;p&gt;Setting up a GPU-instance for deep learning in the cloud is surprisingly underdiscussed. Most recommend simply using a &lt;a href=&quot;https://blog.keras.io/running-jupyter-notebooks-on-gpu-on-aws-a-starter-guide.html&quot;&gt;premade image&lt;/a&gt; from Amazon which includes all the necessary GPU drivers. However, Amazon EC2 &lt;a href=&quot;https://aws.amazon.com/ec2/pricing/on-demand/&quot;&gt;charges&lt;/a&gt; $0.90/hr (not-prorated) for a NVIDIA Tesla K80 GPU instance, while Google Compute Engine &lt;a href=&quot;https://cloud.google.com/compute/pricing#gpus&quot;&gt;charges&lt;/a&gt; $0.75/hr (prorated to the minute) for the same GPU, which is a nontrivial discount for the &lt;em&gt;many&lt;/em&gt; hours necessary to train deep learning models.&lt;/p&gt;

&lt;p&gt;The catch with GCE is you have to setup the deep learning drivers and frameworks from a blank Linux instance. I did that for my &lt;a href=&quot;http://minimaxir.com/2017/04/char-embeddings/&quot;&gt;first adventure&lt;/a&gt; with Keras and it was not fun. However, I recently found &lt;a href=&quot;https://medium.com/google-cloud/containerized-jupyter-notebooks-on-gpu-on-google-cloud-8e86ef7f31e9&quot;&gt;a blog post&lt;/a&gt; by Durgesh Mankekar which takes a more modern approach to managing such dependencies with &lt;a href=&quot;https://www.docker.com&quot;&gt;Docker&lt;/a&gt; containers, and also provides a setup script plus &lt;a href=&quot;https://github.com/durgeshm/dockerfiles/blob/master/jupyter-keras-gpu/Dockerfile&quot;&gt;container&lt;/a&gt; with the necessary deep learning drivers/frameworks for Keras. This container can then be loaded using &lt;a href=&quot;https://github.com/NVIDIA/nvidia-docker&quot;&gt;nvidia-docker&lt;/a&gt;, which allows Docker containers to access the GPU on the host. Running a deep learning script in the container is simply a matter of running a Docker command. After the script completes, the container is destroyed. This approach incidentally ensures that separate executions are independent; perfect for benchmarking/reproducibility.&lt;/p&gt;

&lt;p&gt;I &lt;a href=&quot;https://github.com/minimaxir/keras-cntk-docker&quot;&gt;tweaked the container&lt;/a&gt; to include an installation of CNTK, a CNTK-compatable version of Keras, and made CNTK the default backend for Keras.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/keras-cntk/cntk_keras.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;h2&gt;Benchmark Methodology&lt;/h2&gt;

&lt;p&gt;The Keras &lt;a href=&quot;https://github.com/fchollet/keras/tree/master/examples&quot;&gt;examples&lt;/a&gt; are robust and solve real-world deep learning problems; perfect for simulating real-world performance. I took &lt;a href=&quot;https://github.com/minimaxir/keras-cntk-benchmark/tree/master/test_files&quot;&gt;a variety&lt;/a&gt; of those examples, emphasizing different neural network architectures, and added a &lt;a href=&quot;https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/CustomCallback.py&quot;&gt;custom logger&lt;/a&gt; which outputs a CSV containing both model performance and elapsed time as the training progresses.&lt;/p&gt;

&lt;p&gt;As mentioned earlier, the only change needed to switch between backends is setting a flag. Even though CNTK is the default backend for Keras in the container, a simple &lt;code&gt;-e KERAS_BACKEND=&amp;#39;tensorflow&amp;#39;&lt;/code&gt; argument in the Docker command switches it to TensorFlow.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/keras-cntk/tensorflow_keras_2.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;I wrote a Python &lt;a href=&quot;https://github.com/minimaxir/keras-cntk-benchmark/blob/master/keras_cntk_benchmark.py&quot;&gt;benchmark script&lt;/a&gt; (executed on the host) to administrate and run all the examples in their own Docker containers, with both CNTK and TensorFlow, and collected the resulting logs.&lt;/p&gt;

&lt;p&gt;Here are the results.&lt;/p&gt;

&lt;h2&gt;IMDb Review Dataset&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;http://ai.stanford.edu/%7Eamaas/data/sentiment/&quot;&gt;IMDb review dataset&lt;/a&gt; is a famous dataset for benchmarking natural language processing (NLP) for sentiment analysis. The 25,000 reviews in the dataset are tagged as positive or negative. Good machine learning models developed &lt;a href=&quot;http://ai.stanford.edu/%7Eamaas/papers/wvSent_acl2011.pdf&quot;&gt;before deep learning became mainstream&lt;/a&gt; score about 88% classification accuracy on the test dataset.&lt;/p&gt;

&lt;p&gt;The first &lt;a href=&quot;https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/imdb_bidirectional_lstm.py&quot;&gt;model approach&lt;/a&gt; is with a &lt;strong&gt;Bidirectional LSTM&lt;/strong&gt;, which weights the model by the sequence of words, both forward &lt;em&gt;and&lt;/em&gt; backward.&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s look at the classification accuracy of the test set at various points in time while the model is being trained:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: all charts in this blog post are interactive &lt;a href=&quot;https://plot.ly&quot;&gt;Plotly&lt;/a&gt; charts; feel free to mouse-over data points for exact values and use the controls in the upper-right to manipulate the chart.&lt;/em&gt;&lt;/p&gt;

&lt;div id=&quot;htmlwidget_container&quot;&gt;
  &lt;div id=&quot;d5e3793d2a4e&quot; style=&quot;width:100%;height:400px;&quot; class=&quot;plotly html-widget&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;script type=&quot;application/json&quot; data-for=&quot;d5e3793d2a4e&quot;&gt;{&quot;x&quot;:{&quot;data&quot;:[{&quot;x&quot;:[1,2,3,4],&quot;y&quot;:[0.85196,0.84492,0.83352,0.83544],&quot;text&quot;:[&quot;epoch: 1&lt;br /&gt;val_acc: 0.85196&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 2&lt;br /&gt;val_acc: 0.84492&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 3&lt;br /&gt;val_acc: 0.83352&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 4&lt;br /&gt;val_acc: 0.83544&lt;br /&gt;framework: CNTK&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;dash&quot;:&quot;solid&quot;},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[1,2,3,4],&quot;y&quot;:[0.84356,0.84584,0.836,0.83432],&quot;text&quot;:[&quot;epoch: 1&lt;br /&gt;val_acc: 0.84356&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 2&lt;br /&gt;val_acc: 0.84584&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 3&lt;br /&gt;val_acc: 0.83600&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 4&lt;br /&gt;val_acc: 0.83432&lt;br /&gt;framework: TensorFlow&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;dash&quot;:&quot;solid&quot;},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[4,3,2,1],&quot;y&quot;:[0.83544,0.83352,0.84492,0.85196],&quot;text&quot;:[&quot;epoch: 4&lt;br /&gt;val_acc: 0.83544&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 3&lt;br /&gt;val_acc: 0.83352&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 2&lt;br /&gt;val_acc: 0.84492&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 1&lt;br /&gt;val_acc: 0.85196&lt;br /&gt;framework: CNTK&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;markers&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;opacity&quot;:1,&quot;size&quot;:7.55905511811024,&quot;symbol&quot;:&quot;circle&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;}},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[4,3,2,1],&quot;y&quot;:[0.83432,0.836,0.84584,0.84356],&quot;text&quot;:[&quot;epoch: 4&lt;br /&gt;val_acc: 0.83432&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 3&lt;br /&gt;val_acc: 0.83600&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 2&lt;br /&gt;val_acc: 0.84584&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 1&lt;br /&gt;val_acc: 0.84356&lt;br /&gt;framework: TensorFlow&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;markers&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;opacity&quot;:1,&quot;size&quot;:7.55905511811024,&quot;symbol&quot;:&quot;circle&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;}},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null}],&quot;layout&quot;:{&quot;margin&quot;:{&quot;t&quot;:53.3612287256123,&quot;r&quot;:9.29846409298464,&quot;b&quot;:53.1686176836862,&quot;l&quot;:69.7384806973848},&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;title&quot;:&quot;Performance of Bidirectional LSTM Approach on IMDb Data&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:22.3163138231631},&quot;xaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.85,4.15],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;],&quot;tickvals&quot;:[1,2,3,4],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;y&quot;,&quot;title&quot;:&quot;Epoch&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;yaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.832598,0.852882],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;83.5%&quot;,&quot;84.0%&quot;,&quot;84.5%&quot;,&quot;85.0%&quot;],&quot;tickvals&quot;:[0.835,0.84,0.845,0.85],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;83.5%&quot;,&quot;84.0%&quot;,&quot;84.5%&quot;,&quot;85.0%&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;x&quot;,&quot;title&quot;:&quot;Test Accuracy (Higher is Better)&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;shapes&quot;:[{&quot;type&quot;:&quot;rect&quot;,&quot;fillcolor&quot;:null,&quot;line&quot;:{&quot;color&quot;:null,&quot;width&quot;:0,&quot;linetype&quot;:[]},&quot;yref&quot;:&quot;paper&quot;,&quot;xref&quot;:&quot;paper&quot;,&quot;x0&quot;:0,&quot;x1&quot;:1,&quot;y0&quot;:0,&quot;y1&quot;:1}],&quot;showlegend&quot;:true,&quot;legend&quot;:{&quot;bgcolor&quot;:null,&quot;bordercolor&quot;:null,&quot;borderwidth&quot;:0,&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;y&quot;:0.877694488188976},&quot;hovermode&quot;:&quot;closest&quot;,&quot;width&quot;:null,&quot;height&quot;:400,&quot;barmode&quot;:&quot;relative&quot;},&quot;config&quot;:{&quot;doubleClick&quot;:&quot;reset&quot;,&quot;modeBarButtonsToAdd&quot;:[{&quot;name&quot;:&quot;Collaborate&quot;,&quot;icon&quot;:{&quot;width&quot;:1000,&quot;ascent&quot;:500,&quot;descent&quot;:-50,&quot;path&quot;:&quot;M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z&quot;},&quot;click&quot;:&quot;function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }&quot;}],&quot;cloud&quot;:false},&quot;source&quot;:&quot;A&quot;,&quot;attrs&quot;:{&quot;d5e33b9e64b4&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;colour&quot;:{},&quot;type&quot;:&quot;scatter&quot;},&quot;d5e31be58400&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;colour&quot;:{}}},&quot;cur_data&quot;:&quot;d5e33b9e64b4&quot;,&quot;visdat&quot;:{&quot;d5e33b9e64b4&quot;:[&quot;function (y) &quot;,&quot;x&quot;],&quot;d5e31be58400&quot;:[&quot;function (y) &quot;,&quot;x&quot;]},&quot;highlight&quot;:{&quot;on&quot;:&quot;plotly_click&quot;,&quot;persistent&quot;:false,&quot;dynamic&quot;:false,&quot;selectize&quot;:false,&quot;opacityDim&quot;:0.2,&quot;selected&quot;:{&quot;opacity&quot;:1}},&quot;base_url&quot;:&quot;https://plot.ly&quot;},&quot;evals&quot;:[&quot;config.modeBarButtonsToAdd.0.click&quot;],&quot;jsHooks&quot;:{&quot;render&quot;:[{&quot;code&quot;:&quot;function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\&quot;on\&quot;:\&quot;plotly_click\&quot;,\&quot;persistent\&quot;:false,\&quot;dynamic\&quot;:false,\&quot;selectize\&quot;:false,\&quot;opacityDim\&quot;:0.2,\&quot;selected\&quot;:{\&quot;opacity\&quot;:1}}); }&quot;,&quot;data&quot;:null}]}}&lt;/script&gt;

&lt;script type=&quot;application/htmlwidget-sizing&quot; data-for=&quot;d5e3793d2a4e&quot;&gt;{&quot;viewer&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:15,&quot;fill&quot;:false},&quot;browser&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:0,&quot;fill&quot;:false}}&lt;/script&gt;

&lt;p&gt;Normally the accuracy &lt;em&gt;increases&lt;/em&gt; as training proceeds; Bidirectional LSTMs take a long time to train to get improving results, but at the least both frameworks are equally performant.&lt;/p&gt;

&lt;p&gt;To gauge the speed of algorithm, we can calculate the average amount of time it takes to train an epoch (i.e. each time the model sees the entire training set). The time is mostly consistent per epoch but there is some variability; each measurement will have a 95% confidence interval for the true average, obtained via &lt;a href=&quot;https://en.wikipedia.org/wiki/Bootstrapping_(statistics)&quot;&gt;nonparametric bootstrapping&lt;/a&gt;. In the case of the Bidirectional LSTM:&lt;/p&gt;

&lt;div id=&quot;htmlwidget_container&quot;&gt;
  &lt;div id=&quot;d5e353d0aad9&quot; style=&quot;width:100%;height:400px;&quot; class=&quot;plotly html-widget&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;script type=&quot;application/json&quot; data-for=&quot;d5e353d0aad9&quot;&gt;{&quot;x&quot;:{&quot;data&quot;:[{&quot;orientation&quot;:&quot;v&quot;,&quot;width&quot;:0.25,&quot;base&quot;:0,&quot;x&quot;:[1],&quot;y&quot;:[152.378944575787],&quot;text&quot;:&quot;framework: CNTK&lt;br /&gt;mean: 152.3789&quot;,&quot;type&quot;:&quot;bar&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;transparent&quot;}},&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;orientation&quot;:&quot;v&quot;,&quot;width&quot;:0.25,&quot;base&quot;:0,&quot;x&quot;:[2],&quot;y&quot;:[276.647240757942],&quot;text&quot;:&quot;framework: TensorFlow&lt;br /&gt;mean: 276.6472&quot;,&quot;type&quot;:&quot;bar&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;transparent&quot;}},&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[1],&quot;y&quot;:[152.378944575787],&quot;text&quot;:&quot;framework: CNTK&lt;br /&gt;mean: 152.3789&quot;,&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;opacity&quot;:1,&quot;line&quot;:{&quot;color&quot;:&quot;transparent&quot;},&quot;error_y&quot;:{&quot;array&quot;:[4.1145806312561],&quot;arrayminus&quot;:[2.95618027448654],&quot;type&quot;:&quot;data&quot;,&quot;width&quot;:19.8863636363636,&quot;symmetric&quot;:false,&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;},&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[2],&quot;y&quot;:[276.647240757942],&quot;text&quot;:&quot;framework: TensorFlow&lt;br /&gt;mean: 276.6472&quot;,&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;opacity&quot;:1,&quot;line&quot;:{&quot;color&quot;:&quot;transparent&quot;},&quot;error_y&quot;:{&quot;array&quot;:[7.57349234819412],&quot;arrayminus&quot;:[4.70573216676712],&quot;type&quot;:&quot;data&quot;,&quot;width&quot;:19.8863636363636,&quot;symmetric&quot;:false,&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;},&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null}],&quot;layout&quot;:{&quot;margin&quot;:{&quot;t&quot;:53.3612287256123,&quot;r&quot;:9.29846409298464,&quot;b&quot;:53.1686176836862,&quot;l&quot;:54.8609381486094},&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;title&quot;:&quot;Speed of Bidirectional LSTM Approach on IMDb Data&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:22.3163138231631},&quot;xaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.4,2.6],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;CNTK&quot;,&quot;TensorFlow&quot;],&quot;tickvals&quot;:[1,2],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;CNTK&quot;,&quot;TensorFlow&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;y&quot;,&quot;title&quot;:&quot;Keras Backend&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;yaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[-14.2110366553068,298.431769761443],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;0&quot;,&quot;100&quot;,&quot;200&quot;],&quot;tickvals&quot;:[0,100,200],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;0&quot;,&quot;100&quot;,&quot;200&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;x&quot;,&quot;title&quot;:&quot;Average Epoch Runtime (seconds)&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;shapes&quot;:[{&quot;type&quot;:&quot;rect&quot;,&quot;fillcolor&quot;:null,&quot;line&quot;:{&quot;color&quot;:null,&quot;width&quot;:0,&quot;linetype&quot;:[]},&quot;yref&quot;:&quot;paper&quot;,&quot;xref&quot;:&quot;paper&quot;,&quot;x0&quot;:0,&quot;x1&quot;:1,&quot;y0&quot;:0,&quot;y1&quot;:1}],&quot;showlegend&quot;:true,&quot;legend&quot;:{&quot;bgcolor&quot;:null,&quot;bordercolor&quot;:null,&quot;borderwidth&quot;:0,&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;y&quot;:0.877694488188976},&quot;hovermode&quot;:&quot;closest&quot;,&quot;width&quot;:null,&quot;height&quot;:400,&quot;barmode&quot;:&quot;relative&quot;},&quot;config&quot;:{&quot;doubleClick&quot;:&quot;reset&quot;,&quot;modeBarButtonsToAdd&quot;:[{&quot;name&quot;:&quot;Collaborate&quot;,&quot;icon&quot;:{&quot;width&quot;:1000,&quot;ascent&quot;:500,&quot;descent&quot;:-50,&quot;path&quot;:&quot;M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z&quot;},&quot;click&quot;:&quot;function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }&quot;}],&quot;cloud&quot;:false},&quot;source&quot;:&quot;A&quot;,&quot;attrs&quot;:{&quot;d5e310f8be0a&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;ymin&quot;:{},&quot;ymax&quot;:{},&quot;fill&quot;:{},&quot;type&quot;:&quot;bar&quot;},&quot;d5e33a848b3a&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;ymin&quot;:{},&quot;ymax&quot;:{},&quot;fill&quot;:{}}},&quot;cur_data&quot;:&quot;d5e310f8be0a&quot;,&quot;visdat&quot;:{&quot;d5e310f8be0a&quot;:[&quot;function (y) &quot;,&quot;x&quot;],&quot;d5e33a848b3a&quot;:[&quot;function (y) &quot;,&quot;x&quot;]},&quot;highlight&quot;:{&quot;on&quot;:&quot;plotly_click&quot;,&quot;persistent&quot;:false,&quot;dynamic&quot;:false,&quot;selectize&quot;:false,&quot;opacityDim&quot;:0.2,&quot;selected&quot;:{&quot;opacity&quot;:1}},&quot;base_url&quot;:&quot;https://plot.ly&quot;},&quot;evals&quot;:[&quot;config.modeBarButtonsToAdd.0.click&quot;],&quot;jsHooks&quot;:{&quot;render&quot;:[{&quot;code&quot;:&quot;function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\&quot;on\&quot;:\&quot;plotly_click\&quot;,\&quot;persistent\&quot;:false,\&quot;dynamic\&quot;:false,\&quot;selectize\&quot;:false,\&quot;opacityDim\&quot;:0.2,\&quot;selected\&quot;:{\&quot;opacity\&quot;:1}}); }&quot;,&quot;data&quot;:null}]}}&lt;/script&gt;

&lt;script type=&quot;application/htmlwidget-sizing&quot; data-for=&quot;d5e353d0aad9&quot;&gt;{&quot;viewer&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:15,&quot;fill&quot;:false},&quot;browser&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:0,&quot;fill&quot;:false}}&lt;/script&gt;

&lt;p&gt;Wow, CNTK is much faster! Not the 5x-10x speedup the &lt;a href=&quot;https://arxiv.org/abs/1608.07249&quot;&gt;benchmarks&lt;/a&gt; highlighted for working with LSTMs, but nearly halving the runtime by simply setting a backend flag is still impressive.&lt;/p&gt;

&lt;p&gt;Next, we&amp;rsquo;ll look at the modern &lt;strong&gt;fasttext&lt;/strong&gt; &lt;a href=&quot;https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/imdb_fasttext.py&quot;&gt;approach&lt;/a&gt; on the same dataset. Fasttext is a newer algorithm that averages word vector Embeddings together (irrespective of order), but gets incredible results at incredible speeds even when using the CPU only, as with Facebook&amp;rsquo;s &lt;a href=&quot;https://github.com/facebookresearch/fastText&quot;&gt;official implementation&lt;/a&gt; for fasttext. (for this benchmark, I opt to include bigrams)&lt;/p&gt;

&lt;div id=&quot;htmlwidget_container&quot;&gt;
  &lt;div id=&quot;d5e3d468e4c&quot; style=&quot;width:100%;height:400px;&quot; class=&quot;plotly html-widget&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;script type=&quot;application/json&quot; data-for=&quot;d5e3d468e4c&quot;&gt;{&quot;x&quot;:{&quot;data&quot;:[{&quot;x&quot;:[1,2,3,4,5],&quot;y&quot;:[0.85992,0.89428,0.90168,0.904,0.90636],&quot;text&quot;:[&quot;epoch: 1&lt;br /&gt;val_acc: 0.85992&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 2&lt;br /&gt;val_acc: 0.89428&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 3&lt;br /&gt;val_acc: 0.90168&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 4&lt;br /&gt;val_acc: 0.90400&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 5&lt;br /&gt;val_acc: 0.90636&lt;br /&gt;framework: CNTK&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;dash&quot;:&quot;solid&quot;},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[1,2,3,4,5],&quot;y&quot;:[0.86024,0.89416,0.90144,0.90384,0.90684],&quot;text&quot;:[&quot;epoch: 1&lt;br /&gt;val_acc: 0.86024&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 2&lt;br /&gt;val_acc: 0.89416&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 3&lt;br /&gt;val_acc: 0.90144&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 4&lt;br /&gt;val_acc: 0.90384&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 5&lt;br /&gt;val_acc: 0.90684&lt;br /&gt;framework: TensorFlow&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;dash&quot;:&quot;solid&quot;},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[5,4,3,2,1],&quot;y&quot;:[0.90636,0.904,0.90168,0.89428,0.85992],&quot;text&quot;:[&quot;epoch: 5&lt;br /&gt;val_acc: 0.90636&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 4&lt;br /&gt;val_acc: 0.90400&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 3&lt;br /&gt;val_acc: 0.90168&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 2&lt;br /&gt;val_acc: 0.89428&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 1&lt;br /&gt;val_acc: 0.85992&lt;br /&gt;framework: CNTK&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;markers&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;opacity&quot;:1,&quot;size&quot;:7.55905511811024,&quot;symbol&quot;:&quot;circle&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;}},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[5,4,3,2,1],&quot;y&quot;:[0.90684,0.90384,0.90144,0.89416,0.86024],&quot;text&quot;:[&quot;epoch: 5&lt;br /&gt;val_acc: 0.90684&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 4&lt;br /&gt;val_acc: 0.90384&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 3&lt;br /&gt;val_acc: 0.90144&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 2&lt;br /&gt;val_acc: 0.89416&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 1&lt;br /&gt;val_acc: 0.86024&lt;br /&gt;framework: TensorFlow&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;markers&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;opacity&quot;:1,&quot;size&quot;:7.55905511811024,&quot;symbol&quot;:&quot;circle&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;}},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null}],&quot;layout&quot;:{&quot;margin&quot;:{&quot;t&quot;:53.3612287256123,&quot;r&quot;:9.29846409298464,&quot;b&quot;:53.1686176836862,&quot;l&quot;:54.8609381486094},&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;title&quot;:&quot;Performance of fasttext Approach on IMDb Data&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:22.3163138231631},&quot;xaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.8,5.2],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;],&quot;tickvals&quot;:[1,2,3,4,5],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;y&quot;,&quot;title&quot;:&quot;Epoch&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;yaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.857574,0.909186],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;86%&quot;,&quot;87%&quot;,&quot;88%&quot;,&quot;89%&quot;,&quot;90%&quot;],&quot;tickvals&quot;:[0.86,0.87,0.88,0.89,0.9],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;86%&quot;,&quot;87%&quot;,&quot;88%&quot;,&quot;89%&quot;,&quot;90%&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;x&quot;,&quot;title&quot;:&quot;Test Accuracy (Higher is Better)&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;shapes&quot;:[{&quot;type&quot;:&quot;rect&quot;,&quot;fillcolor&quot;:null,&quot;line&quot;:{&quot;color&quot;:null,&quot;width&quot;:0,&quot;linetype&quot;:[]},&quot;yref&quot;:&quot;paper&quot;,&quot;xref&quot;:&quot;paper&quot;,&quot;x0&quot;:0,&quot;x1&quot;:1,&quot;y0&quot;:0,&quot;y1&quot;:1}],&quot;showlegend&quot;:true,&quot;legend&quot;:{&quot;bgcolor&quot;:null,&quot;bordercolor&quot;:null,&quot;borderwidth&quot;:0,&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;y&quot;:0.877694488188976},&quot;hovermode&quot;:&quot;closest&quot;,&quot;width&quot;:null,&quot;height&quot;:400,&quot;barmode&quot;:&quot;relative&quot;},&quot;config&quot;:{&quot;doubleClick&quot;:&quot;reset&quot;,&quot;modeBarButtonsToAdd&quot;:[{&quot;name&quot;:&quot;Collaborate&quot;,&quot;icon&quot;:{&quot;width&quot;:1000,&quot;ascent&quot;:500,&quot;descent&quot;:-50,&quot;path&quot;:&quot;M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z&quot;},&quot;click&quot;:&quot;function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }&quot;}],&quot;cloud&quot;:false},&quot;source&quot;:&quot;A&quot;,&quot;attrs&quot;:{&quot;d5e317b24b8b&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;colour&quot;:{},&quot;type&quot;:&quot;scatter&quot;},&quot;d5e33a7d9ed4&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;colour&quot;:{}}},&quot;cur_data&quot;:&quot;d5e317b24b8b&quot;,&quot;visdat&quot;:{&quot;d5e317b24b8b&quot;:[&quot;function (y) &quot;,&quot;x&quot;],&quot;d5e33a7d9ed4&quot;:[&quot;function (y) &quot;,&quot;x&quot;]},&quot;highlight&quot;:{&quot;on&quot;:&quot;plotly_click&quot;,&quot;persistent&quot;:false,&quot;dynamic&quot;:false,&quot;selectize&quot;:false,&quot;opacityDim&quot;:0.2,&quot;selected&quot;:{&quot;opacity&quot;:1}},&quot;base_url&quot;:&quot;https://plot.ly&quot;},&quot;evals&quot;:[&quot;config.modeBarButtonsToAdd.0.click&quot;],&quot;jsHooks&quot;:{&quot;render&quot;:[{&quot;code&quot;:&quot;function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\&quot;on\&quot;:\&quot;plotly_click\&quot;,\&quot;persistent\&quot;:false,\&quot;dynamic\&quot;:false,\&quot;selectize\&quot;:false,\&quot;opacityDim\&quot;:0.2,\&quot;selected\&quot;:{\&quot;opacity\&quot;:1}}); }&quot;,&quot;data&quot;:null}]}}&lt;/script&gt;

&lt;script type=&quot;application/htmlwidget-sizing&quot; data-for=&quot;d5e3d468e4c&quot;&gt;{&quot;viewer&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:15,&quot;fill&quot;:false},&quot;browser&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:0,&quot;fill&quot;:false}}&lt;/script&gt;

&lt;div id=&quot;htmlwidget_container&quot;&gt;
  &lt;div id=&quot;d5e34103705b&quot; style=&quot;width:100%;height:400px;&quot; class=&quot;plotly html-widget&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;script type=&quot;application/json&quot; data-for=&quot;d5e34103705b&quot;&gt;{&quot;x&quot;:{&quot;data&quot;:[{&quot;orientation&quot;:&quot;v&quot;,&quot;width&quot;:0.5,&quot;base&quot;:0,&quot;x&quot;:[1],&quot;y&quot;:[69.4701688766479],&quot;text&quot;:&quot;framework: CNTK&lt;br /&gt;mean: 69.47017&quot;,&quot;type&quot;:&quot;bar&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;transparent&quot;}},&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;orientation&quot;:&quot;v&quot;,&quot;width&quot;:0.5,&quot;base&quot;:0,&quot;x&quot;:[2],&quot;y&quot;:[58.3061577320099],&quot;text&quot;:&quot;framework: TensorFlow&lt;br /&gt;mean: 58.30616&quot;,&quot;type&quot;:&quot;bar&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;transparent&quot;}},&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[1],&quot;y&quot;:[69.4701688766479],&quot;text&quot;:&quot;framework: CNTK&lt;br /&gt;mean: 69.47017&quot;,&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;opacity&quot;:1,&quot;line&quot;:{&quot;color&quot;:&quot;transparent&quot;},&quot;error_y&quot;:{&quot;array&quot;:[0.0613580703735437],&quot;arrayminus&quot;:[0.0351073741912842],&quot;type&quot;:&quot;data&quot;,&quot;width&quot;:19.8863636363636,&quot;symmetric&quot;:false,&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;},&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[2],&quot;y&quot;:[58.3061577320099],&quot;text&quot;:&quot;framework: TensorFlow&lt;br /&gt;mean: 58.30616&quot;,&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;opacity&quot;:1,&quot;line&quot;:{&quot;color&quot;:&quot;transparent&quot;},&quot;error_y&quot;:{&quot;array&quot;:[0.416218662261961],&quot;arrayminus&quot;:[0.21453107081912],&quot;type&quot;:&quot;data&quot;,&quot;width&quot;:19.8863636363636,&quot;symmetric&quot;:false,&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;},&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null}],&quot;layout&quot;:{&quot;margin&quot;:{&quot;t&quot;:53.3612287256123,&quot;r&quot;:9.29846409298464,&quot;b&quot;:53.1686176836862,&quot;l&quot;:47.4221668742217},&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;title&quot;:&quot;Speed of fasttext Approach on IMDb Data&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:22.3163138231631},&quot;xaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.4,2.6],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;CNTK&quot;,&quot;TensorFlow&quot;],&quot;tickvals&quot;:[1,2],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;CNTK&quot;,&quot;TensorFlow&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;y&quot;,&quot;title&quot;:&quot;Keras Backend&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;yaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[-3.47657634735107,73.0081032943726],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;0&quot;,&quot;20&quot;,&quot;40&quot;,&quot;60&quot;],&quot;tickvals&quot;:[0,20,40,60],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;0&quot;,&quot;20&quot;,&quot;40&quot;,&quot;60&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;x&quot;,&quot;title&quot;:&quot;Average Epoch Runtime (seconds)&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;shapes&quot;:[{&quot;type&quot;:&quot;rect&quot;,&quot;fillcolor&quot;:null,&quot;line&quot;:{&quot;color&quot;:null,&quot;width&quot;:0,&quot;linetype&quot;:[]},&quot;yref&quot;:&quot;paper&quot;,&quot;xref&quot;:&quot;paper&quot;,&quot;x0&quot;:0,&quot;x1&quot;:1,&quot;y0&quot;:0,&quot;y1&quot;:1}],&quot;showlegend&quot;:true,&quot;legend&quot;:{&quot;bgcolor&quot;:null,&quot;bordercolor&quot;:null,&quot;borderwidth&quot;:0,&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;y&quot;:0.877694488188976},&quot;hovermode&quot;:&quot;closest&quot;,&quot;width&quot;:null,&quot;height&quot;:400,&quot;barmode&quot;:&quot;relative&quot;},&quot;config&quot;:{&quot;doubleClick&quot;:&quot;reset&quot;,&quot;modeBarButtonsToAdd&quot;:[{&quot;name&quot;:&quot;Collaborate&quot;,&quot;icon&quot;:{&quot;width&quot;:1000,&quot;ascent&quot;:500,&quot;descent&quot;:-50,&quot;path&quot;:&quot;M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z&quot;},&quot;click&quot;:&quot;function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }&quot;}],&quot;cloud&quot;:false},&quot;source&quot;:&quot;A&quot;,&quot;attrs&quot;:{&quot;d5e320cea520&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;ymin&quot;:{},&quot;ymax&quot;:{},&quot;fill&quot;:{},&quot;type&quot;:&quot;bar&quot;},&quot;d5e35ebae8b3&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;ymin&quot;:{},&quot;ymax&quot;:{},&quot;fill&quot;:{}}},&quot;cur_data&quot;:&quot;d5e320cea520&quot;,&quot;visdat&quot;:{&quot;d5e320cea520&quot;:[&quot;function (y) &quot;,&quot;x&quot;],&quot;d5e35ebae8b3&quot;:[&quot;function (y) &quot;,&quot;x&quot;]},&quot;highlight&quot;:{&quot;on&quot;:&quot;plotly_click&quot;,&quot;persistent&quot;:false,&quot;dynamic&quot;:false,&quot;selectize&quot;:false,&quot;opacityDim&quot;:0.2,&quot;selected&quot;:{&quot;opacity&quot;:1}},&quot;base_url&quot;:&quot;https://plot.ly&quot;},&quot;evals&quot;:[&quot;config.modeBarButtonsToAdd.0.click&quot;],&quot;jsHooks&quot;:{&quot;render&quot;:[{&quot;code&quot;:&quot;function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\&quot;on\&quot;:\&quot;plotly_click\&quot;,\&quot;persistent\&quot;:false,\&quot;dynamic\&quot;:false,\&quot;selectize\&quot;:false,\&quot;opacityDim\&quot;:0.2,\&quot;selected\&quot;:{\&quot;opacity\&quot;:1}}); }&quot;,&quot;data&quot;:null}]}}&lt;/script&gt;

&lt;script type=&quot;application/htmlwidget-sizing&quot; data-for=&quot;d5e34103705b&quot;&gt;{&quot;viewer&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:15,&quot;fill&quot;:false},&quot;browser&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:0,&quot;fill&quot;:false}}&lt;/script&gt;

&lt;p&gt;Both frameworks have nearly identical accuracy due to model simplicity, but in this case, TensorFlow is faster at working with Embeddings. (at the least, fasttext clearly much faster than the Bidirectional LSTM approach!) In addition, fasttext blows away the 88% benchmark, which may be worth considering for other machine learning projects.&lt;/p&gt;

&lt;h2&gt;MNIST Dataset&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot;&gt;MNIST dataset&lt;/a&gt; is another famous dataset of handwritten digits, good for testing computer vision (60,000 training images, 10,000 test images). Generally, good models get above 99% classification accuracy on the test set.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;multilayer perceptron (MLP)&lt;/strong&gt; &lt;a href=&quot;https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/mnist_mlp.py&quot;&gt;approach&lt;/a&gt; just uses a large fully-connected network and lets &lt;em&gt;Deep Learning Magic &amp;trade;&lt;/em&gt; take over. Sometimes that can be enough.&lt;/p&gt;

&lt;div id=&quot;htmlwidget_container&quot;&gt;
  &lt;div id=&quot;d5e3134e61e6&quot; style=&quot;width:100%;height:400px;&quot; class=&quot;plotly html-widget&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;script type=&quot;application/json&quot; data-for=&quot;d5e3134e61e6&quot;&gt;{&quot;x&quot;:{&quot;data&quot;:[{&quot;x&quot;:[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],&quot;y&quot;:[0.9584,0.9709,0.976,0.9802,0.9789,0.9831,0.984,0.9842,0.9835,0.9825,0.9823,0.9819,0.9842,0.9832,0.9839,0.9835,0.9838,0.9825,0.9833,0.9796],&quot;text&quot;:[&quot;epoch:  1&lt;br /&gt;val_acc: 0.9584&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  2&lt;br /&gt;val_acc: 0.9709&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  3&lt;br /&gt;val_acc: 0.9760&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  4&lt;br /&gt;val_acc: 0.9802&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  5&lt;br /&gt;val_acc: 0.9789&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  6&lt;br /&gt;val_acc: 0.9831&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  7&lt;br /&gt;val_acc: 0.9840&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  8&lt;br /&gt;val_acc: 0.9842&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  9&lt;br /&gt;val_acc: 0.9835&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 10&lt;br /&gt;val_acc: 0.9825&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 11&lt;br /&gt;val_acc: 0.9823&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 12&lt;br /&gt;val_acc: 0.9819&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 13&lt;br /&gt;val_acc: 0.9842&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 14&lt;br /&gt;val_acc: 0.9832&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 15&lt;br /&gt;val_acc: 0.9839&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 16&lt;br /&gt;val_acc: 0.9835&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 17&lt;br /&gt;val_acc: 0.9838&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 18&lt;br /&gt;val_acc: 0.9825&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 19&lt;br /&gt;val_acc: 0.9833&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 20&lt;br /&gt;val_acc: 0.9796&lt;br /&gt;framework: CNTK&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;dash&quot;:&quot;solid&quot;},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],&quot;y&quot;:[0.9664,0.9773,0.9788,0.9764,0.981,0.9788,0.9792,0.9823,0.9823,0.984,0.9851,0.982,0.9838,0.9843,0.9855,0.9841,0.9818,0.9837,0.9832,0.9839],&quot;text&quot;:[&quot;epoch:  1&lt;br /&gt;val_acc: 0.9664&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  2&lt;br /&gt;val_acc: 0.9773&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  3&lt;br /&gt;val_acc: 0.9788&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  4&lt;br /&gt;val_acc: 0.9764&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  5&lt;br /&gt;val_acc: 0.9810&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  6&lt;br /&gt;val_acc: 0.9788&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  7&lt;br /&gt;val_acc: 0.9792&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  8&lt;br /&gt;val_acc: 0.9823&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  9&lt;br /&gt;val_acc: 0.9823&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 10&lt;br /&gt;val_acc: 0.9840&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 11&lt;br /&gt;val_acc: 0.9851&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 12&lt;br /&gt;val_acc: 0.9820&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 13&lt;br /&gt;val_acc: 0.9838&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 14&lt;br /&gt;val_acc: 0.9843&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 15&lt;br /&gt;val_acc: 0.9855&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 16&lt;br /&gt;val_acc: 0.9841&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 17&lt;br /&gt;val_acc: 0.9818&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 18&lt;br /&gt;val_acc: 0.9837&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 19&lt;br /&gt;val_acc: 0.9832&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 20&lt;br /&gt;val_acc: 0.9839&lt;br /&gt;framework: TensorFlow&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;dash&quot;:&quot;solid&quot;},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1],&quot;y&quot;:[0.9796,0.9833,0.9825,0.9838,0.9835,0.9839,0.9832,0.9842,0.9819,0.9823,0.9825,0.9835,0.9842,0.984,0.9831,0.9789,0.9802,0.976,0.9709,0.9584],&quot;text&quot;:[&quot;epoch: 20&lt;br /&gt;val_acc: 0.9796&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 19&lt;br /&gt;val_acc: 0.9833&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 18&lt;br /&gt;val_acc: 0.9825&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 17&lt;br /&gt;val_acc: 0.9838&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 16&lt;br /&gt;val_acc: 0.9835&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 15&lt;br /&gt;val_acc: 0.9839&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 14&lt;br /&gt;val_acc: 0.9832&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 13&lt;br /&gt;val_acc: 0.9842&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 12&lt;br /&gt;val_acc: 0.9819&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 11&lt;br /&gt;val_acc: 0.9823&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 10&lt;br /&gt;val_acc: 0.9825&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  9&lt;br /&gt;val_acc: 0.9835&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  8&lt;br /&gt;val_acc: 0.9842&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  7&lt;br /&gt;val_acc: 0.9840&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  6&lt;br /&gt;val_acc: 0.9831&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  5&lt;br /&gt;val_acc: 0.9789&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  4&lt;br /&gt;val_acc: 0.9802&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  3&lt;br /&gt;val_acc: 0.9760&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  2&lt;br /&gt;val_acc: 0.9709&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  1&lt;br /&gt;val_acc: 0.9584&lt;br /&gt;framework: CNTK&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;markers&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;opacity&quot;:1,&quot;size&quot;:7.55905511811024,&quot;symbol&quot;:&quot;circle&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;}},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,3,2,1,4],&quot;y&quot;:[0.9839,0.9832,0.9837,0.9818,0.9841,0.9855,0.9843,0.9838,0.982,0.9851,0.984,0.9823,0.9823,0.9792,0.9788,0.981,0.9788,0.9773,0.9664,0.9764],&quot;text&quot;:[&quot;epoch: 20&lt;br /&gt;val_acc: 0.9839&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 19&lt;br /&gt;val_acc: 0.9832&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 18&lt;br /&gt;val_acc: 0.9837&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 17&lt;br /&gt;val_acc: 0.9818&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 16&lt;br /&gt;val_acc: 0.9841&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 15&lt;br /&gt;val_acc: 0.9855&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 14&lt;br /&gt;val_acc: 0.9843&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 13&lt;br /&gt;val_acc: 0.9838&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 12&lt;br /&gt;val_acc: 0.9820&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 11&lt;br /&gt;val_acc: 0.9851&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 10&lt;br /&gt;val_acc: 0.9840&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  9&lt;br /&gt;val_acc: 0.9823&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  8&lt;br /&gt;val_acc: 0.9823&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  7&lt;br /&gt;val_acc: 0.9792&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  6&lt;br /&gt;val_acc: 0.9788&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  5&lt;br /&gt;val_acc: 0.9810&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  3&lt;br /&gt;val_acc: 0.9788&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  2&lt;br /&gt;val_acc: 0.9773&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  1&lt;br /&gt;val_acc: 0.9664&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  4&lt;br /&gt;val_acc: 0.9764&lt;br /&gt;framework: TensorFlow&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;markers&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;opacity&quot;:1,&quot;size&quot;:7.55905511811024,&quot;symbol&quot;:&quot;circle&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;}},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null}],&quot;layout&quot;:{&quot;margin&quot;:{&quot;t&quot;:53.3612287256123,&quot;r&quot;:9.29846409298464,&quot;b&quot;:53.1686176836862,&quot;l&quot;:54.8609381486094},&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;title&quot;:&quot;Performance of MLP Approach on MNIST Data&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:22.3163138231631},&quot;xaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.0499999999999999,20.95],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;5&quot;,&quot;10&quot;,&quot;15&quot;,&quot;20&quot;],&quot;tickvals&quot;:[5,10,15,20],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;5&quot;,&quot;10&quot;,&quot;15&quot;,&quot;20&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;y&quot;,&quot;title&quot;:&quot;Epoch&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;yaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.957045,0.986855],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;96%&quot;,&quot;97%&quot;,&quot;98%&quot;],&quot;tickvals&quot;:[0.96,0.97,0.98],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;96%&quot;,&quot;97%&quot;,&quot;98%&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;x&quot;,&quot;title&quot;:&quot;Test Accuracy (Higher is Better)&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;shapes&quot;:[{&quot;type&quot;:&quot;rect&quot;,&quot;fillcolor&quot;:null,&quot;line&quot;:{&quot;color&quot;:null,&quot;width&quot;:0,&quot;linetype&quot;:[]},&quot;yref&quot;:&quot;paper&quot;,&quot;xref&quot;:&quot;paper&quot;,&quot;x0&quot;:0,&quot;x1&quot;:1,&quot;y0&quot;:0,&quot;y1&quot;:1}],&quot;showlegend&quot;:true,&quot;legend&quot;:{&quot;bgcolor&quot;:null,&quot;bordercolor&quot;:null,&quot;borderwidth&quot;:0,&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;y&quot;:0.877694488188976},&quot;hovermode&quot;:&quot;closest&quot;,&quot;width&quot;:null,&quot;height&quot;:400,&quot;barmode&quot;:&quot;relative&quot;},&quot;config&quot;:{&quot;doubleClick&quot;:&quot;reset&quot;,&quot;modeBarButtonsToAdd&quot;:[{&quot;name&quot;:&quot;Collaborate&quot;,&quot;icon&quot;:{&quot;width&quot;:1000,&quot;ascent&quot;:500,&quot;descent&quot;:-50,&quot;path&quot;:&quot;M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z&quot;},&quot;click&quot;:&quot;function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }&quot;}],&quot;cloud&quot;:false},&quot;source&quot;:&quot;A&quot;,&quot;attrs&quot;:{&quot;d5e3506d9704&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;colour&quot;:{},&quot;type&quot;:&quot;scatter&quot;},&quot;d5e34ad5b0dc&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;colour&quot;:{}}},&quot;cur_data&quot;:&quot;d5e3506d9704&quot;,&quot;visdat&quot;:{&quot;d5e3506d9704&quot;:[&quot;function (y) &quot;,&quot;x&quot;],&quot;d5e34ad5b0dc&quot;:[&quot;function (y) &quot;,&quot;x&quot;]},&quot;highlight&quot;:{&quot;on&quot;:&quot;plotly_click&quot;,&quot;persistent&quot;:false,&quot;dynamic&quot;:false,&quot;selectize&quot;:false,&quot;opacityDim&quot;:0.2,&quot;selected&quot;:{&quot;opacity&quot;:1}},&quot;base_url&quot;:&quot;https://plot.ly&quot;},&quot;evals&quot;:[&quot;config.modeBarButtonsToAdd.0.click&quot;],&quot;jsHooks&quot;:{&quot;render&quot;:[{&quot;code&quot;:&quot;function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\&quot;on\&quot;:\&quot;plotly_click\&quot;,\&quot;persistent\&quot;:false,\&quot;dynamic\&quot;:false,\&quot;selectize\&quot;:false,\&quot;opacityDim\&quot;:0.2,\&quot;selected\&quot;:{\&quot;opacity\&quot;:1}}); }&quot;,&quot;data&quot;:null}]}}&lt;/script&gt;

&lt;script type=&quot;application/htmlwidget-sizing&quot; data-for=&quot;d5e3134e61e6&quot;&gt;{&quot;viewer&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:15,&quot;fill&quot;:false},&quot;browser&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:0,&quot;fill&quot;:false}}&lt;/script&gt;

&lt;div id=&quot;htmlwidget_container&quot;&gt;
  &lt;div id=&quot;d5e37c954f2c&quot; style=&quot;width:100%;height:400px;&quot; class=&quot;plotly html-widget&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;script type=&quot;application/json&quot; data-for=&quot;d5e37c954f2c&quot;&gt;{&quot;x&quot;:{&quot;data&quot;:[{&quot;orientation&quot;:&quot;v&quot;,&quot;width&quot;:0.5,&quot;base&quot;:0,&quot;x&quot;:[1],&quot;y&quot;:[2.76817538738251],&quot;text&quot;:&quot;framework: CNTK&lt;br /&gt;mean: 2.768175&quot;,&quot;type&quot;:&quot;bar&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;transparent&quot;}},&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;orientation&quot;:&quot;v&quot;,&quot;width&quot;:0.5,&quot;base&quot;:0,&quot;x&quot;:[2],&quot;y&quot;:[3.37409842014313],&quot;text&quot;:&quot;framework: TensorFlow&lt;br /&gt;mean: 3.374098&quot;,&quot;type&quot;:&quot;bar&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;transparent&quot;}},&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[1],&quot;y&quot;:[2.76817538738251],&quot;text&quot;:&quot;framework: CNTK&lt;br /&gt;mean: 2.768175&quot;,&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;opacity&quot;:1,&quot;line&quot;:{&quot;color&quot;:&quot;transparent&quot;},&quot;error_y&quot;:{&quot;array&quot;:[0.0490834009408876],&quot;arrayminus&quot;:[0.0264339576589818],&quot;type&quot;:&quot;data&quot;,&quot;width&quot;:19.8863636363636,&quot;symmetric&quot;:false,&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;},&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[2],&quot;y&quot;:[3.37409842014313],&quot;text&quot;:&quot;framework: TensorFlow&lt;br /&gt;mean: 3.374098&quot;,&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;opacity&quot;:1,&quot;line&quot;:{&quot;color&quot;:&quot;transparent&quot;},&quot;error_y&quot;:{&quot;array&quot;:[0.122406645406047],&quot;arrayminus&quot;:[0.039851210974144],&quot;type&quot;:&quot;data&quot;,&quot;width&quot;:19.8863636363636,&quot;symmetric&quot;:false,&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;},&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null}],&quot;layout&quot;:{&quot;margin&quot;:{&quot;t&quot;:53.3612287256123,&quot;r&quot;:9.29846409298464,&quot;b&quot;:53.1686176836862,&quot;l&quot;:39.983395599834},&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;title&quot;:&quot;Speed of MLP Approach on MNIST Data&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:22.3163138231631},&quot;xaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.4,2.6],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;CNTK&quot;,&quot;TensorFlow&quot;],&quot;tickvals&quot;:[1,2],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;CNTK&quot;,&quot;TensorFlow&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;y&quot;,&quot;title&quot;:&quot;Keras Backend&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;yaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[-0.174825253277459,3.67133031882663],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;],&quot;tickvals&quot;:[0,1,2,3],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;x&quot;,&quot;title&quot;:&quot;Average Epoch Runtime (seconds)&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;shapes&quot;:[{&quot;type&quot;:&quot;rect&quot;,&quot;fillcolor&quot;:null,&quot;line&quot;:{&quot;color&quot;:null,&quot;width&quot;:0,&quot;linetype&quot;:[]},&quot;yref&quot;:&quot;paper&quot;,&quot;xref&quot;:&quot;paper&quot;,&quot;x0&quot;:0,&quot;x1&quot;:1,&quot;y0&quot;:0,&quot;y1&quot;:1}],&quot;showlegend&quot;:true,&quot;legend&quot;:{&quot;bgcolor&quot;:null,&quot;bordercolor&quot;:null,&quot;borderwidth&quot;:0,&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;y&quot;:0.877694488188976},&quot;hovermode&quot;:&quot;closest&quot;,&quot;width&quot;:null,&quot;height&quot;:400,&quot;barmode&quot;:&quot;relative&quot;},&quot;config&quot;:{&quot;doubleClick&quot;:&quot;reset&quot;,&quot;modeBarButtonsToAdd&quot;:[{&quot;name&quot;:&quot;Collaborate&quot;,&quot;icon&quot;:{&quot;width&quot;:1000,&quot;ascent&quot;:500,&quot;descent&quot;:-50,&quot;path&quot;:&quot;M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z&quot;},&quot;click&quot;:&quot;function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }&quot;}],&quot;cloud&quot;:false},&quot;source&quot;:&quot;A&quot;,&quot;attrs&quot;:{&quot;d5e3533c155a&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;ymin&quot;:{},&quot;ymax&quot;:{},&quot;fill&quot;:{},&quot;type&quot;:&quot;bar&quot;},&quot;d5e3d9df267&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;ymin&quot;:{},&quot;ymax&quot;:{},&quot;fill&quot;:{}}},&quot;cur_data&quot;:&quot;d5e3533c155a&quot;,&quot;visdat&quot;:{&quot;d5e3533c155a&quot;:[&quot;function (y) &quot;,&quot;x&quot;],&quot;d5e3d9df267&quot;:[&quot;function (y) &quot;,&quot;x&quot;]},&quot;highlight&quot;:{&quot;on&quot;:&quot;plotly_click&quot;,&quot;persistent&quot;:false,&quot;dynamic&quot;:false,&quot;selectize&quot;:false,&quot;opacityDim&quot;:0.2,&quot;selected&quot;:{&quot;opacity&quot;:1}},&quot;base_url&quot;:&quot;https://plot.ly&quot;},&quot;evals&quot;:[&quot;config.modeBarButtonsToAdd.0.click&quot;],&quot;jsHooks&quot;:{&quot;render&quot;:[{&quot;code&quot;:&quot;function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\&quot;on\&quot;:\&quot;plotly_click\&quot;,\&quot;persistent\&quot;:false,\&quot;dynamic\&quot;:false,\&quot;selectize\&quot;:false,\&quot;opacityDim\&quot;:0.2,\&quot;selected\&quot;:{\&quot;opacity\&quot;:1}}); }&quot;,&quot;data&quot;:null}]}}&lt;/script&gt;

&lt;script type=&quot;application/htmlwidget-sizing&quot; data-for=&quot;d5e37c954f2c&quot;&gt;{&quot;viewer&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:15,&quot;fill&quot;:false},&quot;browser&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:0,&quot;fill&quot;:false}}&lt;/script&gt;

&lt;p&gt;Both frameworks train the model extremely quickly taking only a few seconds per epoch; there&amp;rsquo;s no clear winner in terms of accuracy (although neither broke 99%), but CNTK is faster.&lt;/p&gt;

&lt;p&gt;Another &lt;a href=&quot;https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/mnist_cnn.py&quot;&gt;approach&lt;/a&gt; is the &lt;strong&gt;convolutional neural network (CNN)&lt;/strong&gt;, which utilizes the inherent relationships between adjacent pixels and is a more logical architecture for image data.&lt;/p&gt;

&lt;div id=&quot;htmlwidget_container&quot;&gt;
  &lt;div id=&quot;d5e32ffdeaf8&quot; style=&quot;width:100%;height:400px;&quot; class=&quot;plotly html-widget&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;script type=&quot;application/json&quot; data-for=&quot;d5e32ffdeaf8&quot;&gt;{&quot;x&quot;:{&quot;data&quot;:[{&quot;x&quot;:[1,2,3,4,5,6,7,8,9,10,11,12],&quot;y&quot;:[0.9768,0.9827,0.9853,0.9865,0.9882,0.9883,0.9899,0.9899,0.9894,0.9895,0.9893,0.9892],&quot;text&quot;:[&quot;epoch:  1&lt;br /&gt;val_acc: 0.9768&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  2&lt;br /&gt;val_acc: 0.9827&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  3&lt;br /&gt;val_acc: 0.9853&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  4&lt;br /&gt;val_acc: 0.9865&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  5&lt;br /&gt;val_acc: 0.9882&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  6&lt;br /&gt;val_acc: 0.9883&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  7&lt;br /&gt;val_acc: 0.9899&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  8&lt;br /&gt;val_acc: 0.9899&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  9&lt;br /&gt;val_acc: 0.9894&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 10&lt;br /&gt;val_acc: 0.9895&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 11&lt;br /&gt;val_acc: 0.9893&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 12&lt;br /&gt;val_acc: 0.9892&lt;br /&gt;framework: CNTK&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;dash&quot;:&quot;solid&quot;},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[1,2,3,4,5,6,7,8,9,10,11,12],&quot;y&quot;:[0.9745,0.983,0.9866,0.9884,0.989,0.9891,0.9901,0.9899,0.9911,0.9911,0.9918,0.9916],&quot;text&quot;:[&quot;epoch:  1&lt;br /&gt;val_acc: 0.9745&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  2&lt;br /&gt;val_acc: 0.9830&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  3&lt;br /&gt;val_acc: 0.9866&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  4&lt;br /&gt;val_acc: 0.9884&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  5&lt;br /&gt;val_acc: 0.9890&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  6&lt;br /&gt;val_acc: 0.9891&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  7&lt;br /&gt;val_acc: 0.9901&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  8&lt;br /&gt;val_acc: 0.9899&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  9&lt;br /&gt;val_acc: 0.9911&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 10&lt;br /&gt;val_acc: 0.9911&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 11&lt;br /&gt;val_acc: 0.9918&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 12&lt;br /&gt;val_acc: 0.9916&lt;br /&gt;framework: TensorFlow&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;dash&quot;:&quot;solid&quot;},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[12,11,10,9,7,6,5,4,3,8,2,1],&quot;y&quot;:[0.9892,0.9893,0.9895,0.9894,0.9899,0.9883,0.9882,0.9865,0.9853,0.9899,0.9827,0.9768],&quot;text&quot;:[&quot;epoch: 12&lt;br /&gt;val_acc: 0.9892&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 11&lt;br /&gt;val_acc: 0.9893&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 10&lt;br /&gt;val_acc: 0.9895&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  9&lt;br /&gt;val_acc: 0.9894&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  7&lt;br /&gt;val_acc: 0.9899&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  6&lt;br /&gt;val_acc: 0.9883&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  5&lt;br /&gt;val_acc: 0.9882&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  4&lt;br /&gt;val_acc: 0.9865&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  3&lt;br /&gt;val_acc: 0.9853&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  8&lt;br /&gt;val_acc: 0.9899&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  2&lt;br /&gt;val_acc: 0.9827&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  1&lt;br /&gt;val_acc: 0.9768&lt;br /&gt;framework: CNTK&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;markers&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;opacity&quot;:1,&quot;size&quot;:7.55905511811024,&quot;symbol&quot;:&quot;circle&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;}},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[12,11,10,9,8,7,6,5,4,3,2,1],&quot;y&quot;:[0.9916,0.9918,0.9911,0.9911,0.9899,0.9901,0.9891,0.989,0.9884,0.9866,0.983,0.9745],&quot;text&quot;:[&quot;epoch: 12&lt;br /&gt;val_acc: 0.9916&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 11&lt;br /&gt;val_acc: 0.9918&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 10&lt;br /&gt;val_acc: 0.9911&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  9&lt;br /&gt;val_acc: 0.9911&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  8&lt;br /&gt;val_acc: 0.9899&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  7&lt;br /&gt;val_acc: 0.9901&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  6&lt;br /&gt;val_acc: 0.9891&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  5&lt;br /&gt;val_acc: 0.9890&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  4&lt;br /&gt;val_acc: 0.9884&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  3&lt;br /&gt;val_acc: 0.9866&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  2&lt;br /&gt;val_acc: 0.9830&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  1&lt;br /&gt;val_acc: 0.9745&lt;br /&gt;framework: TensorFlow&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;markers&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;opacity&quot;:1,&quot;size&quot;:7.55905511811024,&quot;symbol&quot;:&quot;circle&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;}},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null}],&quot;layout&quot;:{&quot;margin&quot;:{&quot;t&quot;:53.3612287256123,&quot;r&quot;:9.29846409298464,&quot;b&quot;:53.1686176836862,&quot;l&quot;:69.7384806973848},&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;title&quot;:&quot;Performance of CNN Approach on MNIST Data&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:22.3163138231631},&quot;xaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.45,12.55],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;2.5&quot;,&quot;5.0&quot;,&quot;7.5&quot;,&quot;10.0&quot;,&quot;12.5&quot;],&quot;tickvals&quot;:[2.5,5,7.5,10,12.5],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;2.5&quot;,&quot;5.0&quot;,&quot;7.5&quot;,&quot;10.0&quot;,&quot;12.5&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;y&quot;,&quot;title&quot;:&quot;Epoch&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;yaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.973635,0.992665],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;97.5%&quot;,&quot;98.0%&quot;,&quot;98.5%&quot;,&quot;99.0%&quot;],&quot;tickvals&quot;:[0.975,0.98,0.985,0.99],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;97.5%&quot;,&quot;98.0%&quot;,&quot;98.5%&quot;,&quot;99.0%&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;x&quot;,&quot;title&quot;:&quot;Test Accuracy (Higher is Better)&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;shapes&quot;:[{&quot;type&quot;:&quot;rect&quot;,&quot;fillcolor&quot;:null,&quot;line&quot;:{&quot;color&quot;:null,&quot;width&quot;:0,&quot;linetype&quot;:[]},&quot;yref&quot;:&quot;paper&quot;,&quot;xref&quot;:&quot;paper&quot;,&quot;x0&quot;:0,&quot;x1&quot;:1,&quot;y0&quot;:0,&quot;y1&quot;:1}],&quot;showlegend&quot;:true,&quot;legend&quot;:{&quot;bgcolor&quot;:null,&quot;bordercolor&quot;:null,&quot;borderwidth&quot;:0,&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;y&quot;:0.877694488188976},&quot;hovermode&quot;:&quot;closest&quot;,&quot;width&quot;:null,&quot;height&quot;:400,&quot;barmode&quot;:&quot;relative&quot;},&quot;config&quot;:{&quot;doubleClick&quot;:&quot;reset&quot;,&quot;modeBarButtonsToAdd&quot;:[{&quot;name&quot;:&quot;Collaborate&quot;,&quot;icon&quot;:{&quot;width&quot;:1000,&quot;ascent&quot;:500,&quot;descent&quot;:-50,&quot;path&quot;:&quot;M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z&quot;},&quot;click&quot;:&quot;function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }&quot;}],&quot;cloud&quot;:false},&quot;source&quot;:&quot;A&quot;,&quot;attrs&quot;:{&quot;d5e31f8815b8&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;colour&quot;:{},&quot;type&quot;:&quot;scatter&quot;},&quot;d5e31f49f334&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;colour&quot;:{}}},&quot;cur_data&quot;:&quot;d5e31f8815b8&quot;,&quot;visdat&quot;:{&quot;d5e31f8815b8&quot;:[&quot;function (y) &quot;,&quot;x&quot;],&quot;d5e31f49f334&quot;:[&quot;function (y) &quot;,&quot;x&quot;]},&quot;highlight&quot;:{&quot;on&quot;:&quot;plotly_click&quot;,&quot;persistent&quot;:false,&quot;dynamic&quot;:false,&quot;selectize&quot;:false,&quot;opacityDim&quot;:0.2,&quot;selected&quot;:{&quot;opacity&quot;:1}},&quot;base_url&quot;:&quot;https://plot.ly&quot;},&quot;evals&quot;:[&quot;config.modeBarButtonsToAdd.0.click&quot;],&quot;jsHooks&quot;:{&quot;render&quot;:[{&quot;code&quot;:&quot;function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\&quot;on\&quot;:\&quot;plotly_click\&quot;,\&quot;persistent\&quot;:false,\&quot;dynamic\&quot;:false,\&quot;selectize\&quot;:false,\&quot;opacityDim\&quot;:0.2,\&quot;selected\&quot;:{\&quot;opacity\&quot;:1}}); }&quot;,&quot;data&quot;:null}]}}&lt;/script&gt;

&lt;script type=&quot;application/htmlwidget-sizing&quot; data-for=&quot;d5e32ffdeaf8&quot;&gt;{&quot;viewer&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:15,&quot;fill&quot;:false},&quot;browser&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:0,&quot;fill&quot;:false}}&lt;/script&gt;

&lt;div id=&quot;htmlwidget_container&quot;&gt;
  &lt;div id=&quot;d5e3638d0839&quot; style=&quot;width:100%;height:400px;&quot; class=&quot;plotly html-widget&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;script type=&quot;application/json&quot; data-for=&quot;d5e3638d0839&quot;&gt;{&quot;x&quot;:{&quot;data&quot;:[{&quot;orientation&quot;:&quot;v&quot;,&quot;width&quot;:0.5,&quot;base&quot;:0,&quot;x&quot;:[1],&quot;y&quot;:[15.8528816103935],&quot;text&quot;:&quot;framework: CNTK&lt;br /&gt;mean: 15.85288&quot;,&quot;type&quot;:&quot;bar&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;transparent&quot;}},&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;orientation&quot;:&quot;v&quot;,&quot;width&quot;:0.5,&quot;base&quot;:0,&quot;x&quot;:[2],&quot;y&quot;:[11.0993242661158],&quot;text&quot;:&quot;framework: TensorFlow&lt;br /&gt;mean: 11.09932&quot;,&quot;type&quot;:&quot;bar&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;transparent&quot;}},&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[1],&quot;y&quot;:[15.8528816103935],&quot;text&quot;:&quot;framework: CNTK&lt;br /&gt;mean: 15.85288&quot;,&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;opacity&quot;:1,&quot;line&quot;:{&quot;color&quot;:&quot;transparent&quot;},&quot;error_y&quot;:{&quot;array&quot;:[0.431788841883343],&quot;arrayminus&quot;:[0.114179053223424],&quot;type&quot;:&quot;data&quot;,&quot;width&quot;:19.8863636363636,&quot;symmetric&quot;:false,&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;},&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[2],&quot;y&quot;:[11.0993242661158],&quot;text&quot;:&quot;framework: TensorFlow&lt;br /&gt;mean: 11.09932&quot;,&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;opacity&quot;:1,&quot;line&quot;:{&quot;color&quot;:&quot;transparent&quot;},&quot;error_y&quot;:{&quot;array&quot;:[0.42946408474802],&quot;arrayminus&quot;:[0.116898433586767],&quot;type&quot;:&quot;data&quot;,&quot;width&quot;:19.8863636363636,&quot;symmetric&quot;:false,&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;},&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null}],&quot;layout&quot;:{&quot;margin&quot;:{&quot;t&quot;:53.3612287256123,&quot;r&quot;:9.29846409298464,&quot;b&quot;:53.1686176836862,&quot;l&quot;:47.4221668742217},&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;title&quot;:&quot;Speed of CNN Approach on MNIST Data&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:22.3163138231631},&quot;xaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.4,2.6],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;CNTK&quot;,&quot;TensorFlow&quot;],&quot;tickvals&quot;:[1,2],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;CNTK&quot;,&quot;TensorFlow&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;y&quot;,&quot;title&quot;:&quot;Keras Backend&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;yaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[-0.814233522613843,17.0989039748907],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;0&quot;,&quot;5&quot;,&quot;10&quot;,&quot;15&quot;],&quot;tickvals&quot;:[0,5,10,15],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;0&quot;,&quot;5&quot;,&quot;10&quot;,&quot;15&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;x&quot;,&quot;title&quot;:&quot;Average Epoch Runtime (seconds)&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;shapes&quot;:[{&quot;type&quot;:&quot;rect&quot;,&quot;fillcolor&quot;:null,&quot;line&quot;:{&quot;color&quot;:null,&quot;width&quot;:0,&quot;linetype&quot;:[]},&quot;yref&quot;:&quot;paper&quot;,&quot;xref&quot;:&quot;paper&quot;,&quot;x0&quot;:0,&quot;x1&quot;:1,&quot;y0&quot;:0,&quot;y1&quot;:1}],&quot;showlegend&quot;:true,&quot;legend&quot;:{&quot;bgcolor&quot;:null,&quot;bordercolor&quot;:null,&quot;borderwidth&quot;:0,&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;y&quot;:0.877694488188976},&quot;hovermode&quot;:&quot;closest&quot;,&quot;width&quot;:null,&quot;height&quot;:400,&quot;barmode&quot;:&quot;relative&quot;},&quot;config&quot;:{&quot;doubleClick&quot;:&quot;reset&quot;,&quot;modeBarButtonsToAdd&quot;:[{&quot;name&quot;:&quot;Collaborate&quot;,&quot;icon&quot;:{&quot;width&quot;:1000,&quot;ascent&quot;:500,&quot;descent&quot;:-50,&quot;path&quot;:&quot;M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z&quot;},&quot;click&quot;:&quot;function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }&quot;}],&quot;cloud&quot;:false},&quot;source&quot;:&quot;A&quot;,&quot;attrs&quot;:{&quot;d5e3421703f2&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;ymin&quot;:{},&quot;ymax&quot;:{},&quot;fill&quot;:{},&quot;type&quot;:&quot;bar&quot;},&quot;d5e3750426c3&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;ymin&quot;:{},&quot;ymax&quot;:{},&quot;fill&quot;:{}}},&quot;cur_data&quot;:&quot;d5e3421703f2&quot;,&quot;visdat&quot;:{&quot;d5e3421703f2&quot;:[&quot;function (y) &quot;,&quot;x&quot;],&quot;d5e3750426c3&quot;:[&quot;function (y) &quot;,&quot;x&quot;]},&quot;highlight&quot;:{&quot;on&quot;:&quot;plotly_click&quot;,&quot;persistent&quot;:false,&quot;dynamic&quot;:false,&quot;selectize&quot;:false,&quot;opacityDim&quot;:0.2,&quot;selected&quot;:{&quot;opacity&quot;:1}},&quot;base_url&quot;:&quot;https://plot.ly&quot;},&quot;evals&quot;:[&quot;config.modeBarButtonsToAdd.0.click&quot;],&quot;jsHooks&quot;:{&quot;render&quot;:[{&quot;code&quot;:&quot;function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\&quot;on\&quot;:\&quot;plotly_click\&quot;,\&quot;persistent\&quot;:false,\&quot;dynamic\&quot;:false,\&quot;selectize\&quot;:false,\&quot;opacityDim\&quot;:0.2,\&quot;selected\&quot;:{\&quot;opacity\&quot;:1}}); }&quot;,&quot;data&quot;:null}]}}&lt;/script&gt;

&lt;script type=&quot;application/htmlwidget-sizing&quot; data-for=&quot;d5e3638d0839&quot;&gt;{&quot;viewer&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:15,&quot;fill&quot;:false},&quot;browser&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:0,&quot;fill&quot;:false}}&lt;/script&gt;

&lt;p&gt;In this case, TensorFlow performs better, both in accuracy &lt;em&gt;and&lt;/em&gt; speed (and it breaks 99% too).&lt;/p&gt;

&lt;h2&gt;CIFAR-10&lt;/h2&gt;

&lt;p&gt;Going more into complex real-world models, the &lt;a href=&quot;https://www.cs.toronto.edu/%7Ekriz/cifar.html&quot;&gt;CIFAR-10 dataset&lt;/a&gt; is a dataset used for image classification of 10 different objects. The architecture in the &lt;a href=&quot;https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/cifar10_cnn.py&quot;&gt;benchmark script&lt;/a&gt; is a &lt;strong&gt;Deep CNN + MLP&lt;/strong&gt; of many layers similar in architecture to the famous &lt;a href=&quot;https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3&quot;&gt;VGG-16&lt;/a&gt; model, but more simple since most people do not have a super-computer cluster to train it.&lt;/p&gt;

&lt;div id=&quot;htmlwidget_container&quot;&gt;
  &lt;div id=&quot;d5e33466a90c&quot; style=&quot;width:100%;height:400px;&quot; class=&quot;plotly html-widget&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;script type=&quot;application/json&quot; data-for=&quot;d5e33466a90c&quot;&gt;{&quot;x&quot;:{&quot;data&quot;:[{&quot;x&quot;:[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],&quot;y&quot;:[0.4301,0.4885,0.5267,0.5329,0.6017,0.6228,0.6412,0.6423,0.6695,0.6745,0.675,0.6939,0.7043,0.7066,0.6976,0.7036,0.7328,0.7259,0.7279,0.74],&quot;text&quot;:[&quot;epoch:  1&lt;br /&gt;val_acc: 0.4301&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  2&lt;br /&gt;val_acc: 0.4885&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  3&lt;br /&gt;val_acc: 0.5267&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  4&lt;br /&gt;val_acc: 0.5329&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  5&lt;br /&gt;val_acc: 0.6017&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  6&lt;br /&gt;val_acc: 0.6228&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  7&lt;br /&gt;val_acc: 0.6412&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  8&lt;br /&gt;val_acc: 0.6423&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  9&lt;br /&gt;val_acc: 0.6695&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 10&lt;br /&gt;val_acc: 0.6745&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 11&lt;br /&gt;val_acc: 0.6750&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 12&lt;br /&gt;val_acc: 0.6939&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 13&lt;br /&gt;val_acc: 0.7043&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 14&lt;br /&gt;val_acc: 0.7066&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 15&lt;br /&gt;val_acc: 0.6976&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 16&lt;br /&gt;val_acc: 0.7036&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 17&lt;br /&gt;val_acc: 0.7328&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 18&lt;br /&gt;val_acc: 0.7259&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 19&lt;br /&gt;val_acc: 0.7279&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 20&lt;br /&gt;val_acc: 0.7400&lt;br /&gt;framework: CNTK&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;dash&quot;:&quot;solid&quot;},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],&quot;y&quot;:[0.4381,0.5066,0.5545,0.5603,0.6146,0.6296,0.6458,0.6396,0.6652,0.6786,0.6943,0.7035,0.7055,0.699,0.7159,0.7171,0.7327,0.7348,0.737,0.7453],&quot;text&quot;:[&quot;epoch:  1&lt;br /&gt;val_acc: 0.4381&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  2&lt;br /&gt;val_acc: 0.5066&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  3&lt;br /&gt;val_acc: 0.5545&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  4&lt;br /&gt;val_acc: 0.5603&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  5&lt;br /&gt;val_acc: 0.6146&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  6&lt;br /&gt;val_acc: 0.6296&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  7&lt;br /&gt;val_acc: 0.6458&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  8&lt;br /&gt;val_acc: 0.6396&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  9&lt;br /&gt;val_acc: 0.6652&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 10&lt;br /&gt;val_acc: 0.6786&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 11&lt;br /&gt;val_acc: 0.6943&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 12&lt;br /&gt;val_acc: 0.7035&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 13&lt;br /&gt;val_acc: 0.7055&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 14&lt;br /&gt;val_acc: 0.6990&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 15&lt;br /&gt;val_acc: 0.7159&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 16&lt;br /&gt;val_acc: 0.7171&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 17&lt;br /&gt;val_acc: 0.7327&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 18&lt;br /&gt;val_acc: 0.7348&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 19&lt;br /&gt;val_acc: 0.7370&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 20&lt;br /&gt;val_acc: 0.7453&lt;br /&gt;framework: TensorFlow&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;dash&quot;:&quot;solid&quot;},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1],&quot;y&quot;:[0.74,0.7279,0.7259,0.7328,0.7036,0.6976,0.7066,0.7043,0.6939,0.675,0.6745,0.6695,0.6423,0.6412,0.6228,0.6017,0.5329,0.5267,0.4885,0.4301],&quot;text&quot;:[&quot;epoch: 20&lt;br /&gt;val_acc: 0.7400&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 19&lt;br /&gt;val_acc: 0.7279&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 18&lt;br /&gt;val_acc: 0.7259&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 17&lt;br /&gt;val_acc: 0.7328&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 16&lt;br /&gt;val_acc: 0.7036&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 15&lt;br /&gt;val_acc: 0.6976&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 14&lt;br /&gt;val_acc: 0.7066&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 13&lt;br /&gt;val_acc: 0.7043&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 12&lt;br /&gt;val_acc: 0.6939&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 11&lt;br /&gt;val_acc: 0.6750&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 10&lt;br /&gt;val_acc: 0.6745&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  9&lt;br /&gt;val_acc: 0.6695&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  8&lt;br /&gt;val_acc: 0.6423&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  7&lt;br /&gt;val_acc: 0.6412&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  6&lt;br /&gt;val_acc: 0.6228&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  5&lt;br /&gt;val_acc: 0.6017&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  4&lt;br /&gt;val_acc: 0.5329&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  3&lt;br /&gt;val_acc: 0.5267&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  2&lt;br /&gt;val_acc: 0.4885&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  1&lt;br /&gt;val_acc: 0.4301&lt;br /&gt;framework: CNTK&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;markers&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;opacity&quot;:1,&quot;size&quot;:7.55905511811024,&quot;symbol&quot;:&quot;circle&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;}},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1],&quot;y&quot;:[0.7453,0.737,0.7348,0.7327,0.7171,0.7159,0.699,0.7055,0.7035,0.6943,0.6786,0.6652,0.6396,0.6458,0.6296,0.6146,0.5603,0.5545,0.5066,0.4381],&quot;text&quot;:[&quot;epoch: 20&lt;br /&gt;val_acc: 0.7453&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 19&lt;br /&gt;val_acc: 0.7370&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 18&lt;br /&gt;val_acc: 0.7348&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 17&lt;br /&gt;val_acc: 0.7327&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 16&lt;br /&gt;val_acc: 0.7171&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 15&lt;br /&gt;val_acc: 0.7159&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 14&lt;br /&gt;val_acc: 0.6990&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 13&lt;br /&gt;val_acc: 0.7055&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 12&lt;br /&gt;val_acc: 0.7035&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 11&lt;br /&gt;val_acc: 0.6943&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 10&lt;br /&gt;val_acc: 0.6786&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  9&lt;br /&gt;val_acc: 0.6652&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  8&lt;br /&gt;val_acc: 0.6396&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  7&lt;br /&gt;val_acc: 0.6458&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  6&lt;br /&gt;val_acc: 0.6296&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  5&lt;br /&gt;val_acc: 0.6146&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  4&lt;br /&gt;val_acc: 0.5603&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  3&lt;br /&gt;val_acc: 0.5545&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  2&lt;br /&gt;val_acc: 0.5066&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  1&lt;br /&gt;val_acc: 0.4381&lt;br /&gt;framework: TensorFlow&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;markers&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;opacity&quot;:1,&quot;size&quot;:7.55905511811024,&quot;symbol&quot;:&quot;circle&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;}},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null}],&quot;layout&quot;:{&quot;margin&quot;:{&quot;t&quot;:53.3612287256123,&quot;r&quot;:9.29846409298464,&quot;b&quot;:53.1686176836862,&quot;l&quot;:54.8609381486094},&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;title&quot;:&quot;Performance of CNN Approach on CIFAR-10 Data&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:22.3163138231631},&quot;xaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.0499999999999999,20.95],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;5&quot;,&quot;10&quot;,&quot;15&quot;,&quot;20&quot;],&quot;tickvals&quot;:[5,10,15,20],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;5&quot;,&quot;10&quot;,&quot;15&quot;,&quot;20&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;y&quot;,&quot;title&quot;:&quot;Epoch&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;yaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.41434,0.76106],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;50%&quot;,&quot;60%&quot;,&quot;70%&quot;],&quot;tickvals&quot;:[0.5,0.6,0.7],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;50%&quot;,&quot;60%&quot;,&quot;70%&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;x&quot;,&quot;title&quot;:&quot;Test Accuracy (Higher is Better)&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;shapes&quot;:[{&quot;type&quot;:&quot;rect&quot;,&quot;fillcolor&quot;:null,&quot;line&quot;:{&quot;color&quot;:null,&quot;width&quot;:0,&quot;linetype&quot;:[]},&quot;yref&quot;:&quot;paper&quot;,&quot;xref&quot;:&quot;paper&quot;,&quot;x0&quot;:0,&quot;x1&quot;:1,&quot;y0&quot;:0,&quot;y1&quot;:1}],&quot;showlegend&quot;:true,&quot;legend&quot;:{&quot;bgcolor&quot;:null,&quot;bordercolor&quot;:null,&quot;borderwidth&quot;:0,&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;y&quot;:0.877694488188976},&quot;hovermode&quot;:&quot;closest&quot;,&quot;width&quot;:null,&quot;height&quot;:400,&quot;barmode&quot;:&quot;relative&quot;},&quot;config&quot;:{&quot;doubleClick&quot;:&quot;reset&quot;,&quot;modeBarButtonsToAdd&quot;:[{&quot;name&quot;:&quot;Collaborate&quot;,&quot;icon&quot;:{&quot;width&quot;:1000,&quot;ascent&quot;:500,&quot;descent&quot;:-50,&quot;path&quot;:&quot;M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z&quot;},&quot;click&quot;:&quot;function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }&quot;}],&quot;cloud&quot;:false},&quot;source&quot;:&quot;A&quot;,&quot;attrs&quot;:{&quot;d5e37bf8b19a&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;colour&quot;:{},&quot;type&quot;:&quot;scatter&quot;},&quot;d5e3454350c&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;colour&quot;:{}}},&quot;cur_data&quot;:&quot;d5e37bf8b19a&quot;,&quot;visdat&quot;:{&quot;d5e37bf8b19a&quot;:[&quot;function (y) &quot;,&quot;x&quot;],&quot;d5e3454350c&quot;:[&quot;function (y) &quot;,&quot;x&quot;]},&quot;highlight&quot;:{&quot;on&quot;:&quot;plotly_click&quot;,&quot;persistent&quot;:false,&quot;dynamic&quot;:false,&quot;selectize&quot;:false,&quot;opacityDim&quot;:0.2,&quot;selected&quot;:{&quot;opacity&quot;:1}},&quot;base_url&quot;:&quot;https://plot.ly&quot;},&quot;evals&quot;:[&quot;config.modeBarButtonsToAdd.0.click&quot;],&quot;jsHooks&quot;:{&quot;render&quot;:[{&quot;code&quot;:&quot;function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\&quot;on\&quot;:\&quot;plotly_click\&quot;,\&quot;persistent\&quot;:false,\&quot;dynamic\&quot;:false,\&quot;selectize\&quot;:false,\&quot;opacityDim\&quot;:0.2,\&quot;selected\&quot;:{\&quot;opacity\&quot;:1}}); }&quot;,&quot;data&quot;:null}]}}&lt;/script&gt;

&lt;script type=&quot;application/htmlwidget-sizing&quot; data-for=&quot;d5e33466a90c&quot;&gt;{&quot;viewer&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:15,&quot;fill&quot;:false},&quot;browser&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:0,&quot;fill&quot;:false}}&lt;/script&gt;

&lt;div id=&quot;htmlwidget_container&quot;&gt;
  &lt;div id=&quot;d5e33bb3e726&quot; style=&quot;width:100%;height:400px;&quot; class=&quot;plotly html-widget&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;script type=&quot;application/json&quot; data-for=&quot;d5e33bb3e726&quot;&gt;{&quot;x&quot;:{&quot;data&quot;:[{&quot;orientation&quot;:&quot;v&quot;,&quot;width&quot;:0.5,&quot;base&quot;:0,&quot;x&quot;:[1],&quot;y&quot;:[40.2903408885002],&quot;text&quot;:&quot;framework: CNTK&lt;br /&gt;mean: 40.29034&quot;,&quot;type&quot;:&quot;bar&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;transparent&quot;}},&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;orientation&quot;:&quot;v&quot;,&quot;width&quot;:0.5,&quot;base&quot;:0,&quot;x&quot;:[2],&quot;y&quot;:[39.2076283454895],&quot;text&quot;:&quot;framework: TensorFlow&lt;br /&gt;mean: 39.20763&quot;,&quot;type&quot;:&quot;bar&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;transparent&quot;}},&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[1],&quot;y&quot;:[40.2903408885002],&quot;text&quot;:&quot;framework: CNTK&lt;br /&gt;mean: 40.29034&quot;,&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;opacity&quot;:1,&quot;line&quot;:{&quot;color&quot;:&quot;transparent&quot;},&quot;error_y&quot;:{&quot;array&quot;:[0.629282373999914],&quot;arrayminus&quot;:[0.678987689315889],&quot;type&quot;:&quot;data&quot;,&quot;width&quot;:19.8863636363636,&quot;symmetric&quot;:false,&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;},&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[2],&quot;y&quot;:[39.2076283454895],&quot;text&quot;:&quot;framework: TensorFlow&lt;br /&gt;mean: 39.20763&quot;,&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;opacity&quot;:1,&quot;line&quot;:{&quot;color&quot;:&quot;transparent&quot;},&quot;error_y&quot;:{&quot;array&quot;:[1.48350351100346],&quot;arrayminus&quot;:[0.962176468314958],&quot;type&quot;:&quot;data&quot;,&quot;width&quot;:19.8863636363636,&quot;symmetric&quot;:false,&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;},&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null}],&quot;layout&quot;:{&quot;margin&quot;:{&quot;t&quot;:53.3612287256123,&quot;r&quot;:9.29846409298464,&quot;b&quot;:53.1686176836862,&quot;l&quot;:47.4221668742217},&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;title&quot;:&quot;Speed of CNN Approach on CIFAR-10 Data&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:22.3163138231631},&quot;xaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.4,2.6],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;CNTK&quot;,&quot;TensorFlow&quot;],&quot;tickvals&quot;:[1,2],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;CNTK&quot;,&quot;TensorFlow&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;y&quot;,&quot;title&quot;:&quot;Keras Backend&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;yaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[-2.04598116312501,42.9656044256251],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;0&quot;,&quot;10&quot;,&quot;20&quot;,&quot;30&quot;,&quot;40&quot;],&quot;tickvals&quot;:[0,10,20,30,40],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;0&quot;,&quot;10&quot;,&quot;20&quot;,&quot;30&quot;,&quot;40&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;x&quot;,&quot;title&quot;:&quot;Average Epoch Runtime (seconds)&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;shapes&quot;:[{&quot;type&quot;:&quot;rect&quot;,&quot;fillcolor&quot;:null,&quot;line&quot;:{&quot;color&quot;:null,&quot;width&quot;:0,&quot;linetype&quot;:[]},&quot;yref&quot;:&quot;paper&quot;,&quot;xref&quot;:&quot;paper&quot;,&quot;x0&quot;:0,&quot;x1&quot;:1,&quot;y0&quot;:0,&quot;y1&quot;:1}],&quot;showlegend&quot;:true,&quot;legend&quot;:{&quot;bgcolor&quot;:null,&quot;bordercolor&quot;:null,&quot;borderwidth&quot;:0,&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;y&quot;:0.877694488188976},&quot;hovermode&quot;:&quot;closest&quot;,&quot;width&quot;:null,&quot;height&quot;:400,&quot;barmode&quot;:&quot;relative&quot;},&quot;config&quot;:{&quot;doubleClick&quot;:&quot;reset&quot;,&quot;modeBarButtonsToAdd&quot;:[{&quot;name&quot;:&quot;Collaborate&quot;,&quot;icon&quot;:{&quot;width&quot;:1000,&quot;ascent&quot;:500,&quot;descent&quot;:-50,&quot;path&quot;:&quot;M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z&quot;},&quot;click&quot;:&quot;function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }&quot;}],&quot;cloud&quot;:false},&quot;source&quot;:&quot;A&quot;,&quot;attrs&quot;:{&quot;d5e362e819c7&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;ymin&quot;:{},&quot;ymax&quot;:{},&quot;fill&quot;:{},&quot;type&quot;:&quot;bar&quot;},&quot;d5e373f48a8b&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;ymin&quot;:{},&quot;ymax&quot;:{},&quot;fill&quot;:{}}},&quot;cur_data&quot;:&quot;d5e362e819c7&quot;,&quot;visdat&quot;:{&quot;d5e362e819c7&quot;:[&quot;function (y) &quot;,&quot;x&quot;],&quot;d5e373f48a8b&quot;:[&quot;function (y) &quot;,&quot;x&quot;]},&quot;highlight&quot;:{&quot;on&quot;:&quot;plotly_click&quot;,&quot;persistent&quot;:false,&quot;dynamic&quot;:false,&quot;selectize&quot;:false,&quot;opacityDim&quot;:0.2,&quot;selected&quot;:{&quot;opacity&quot;:1}},&quot;base_url&quot;:&quot;https://plot.ly&quot;},&quot;evals&quot;:[&quot;config.modeBarButtonsToAdd.0.click&quot;],&quot;jsHooks&quot;:{&quot;render&quot;:[{&quot;code&quot;:&quot;function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\&quot;on\&quot;:\&quot;plotly_click\&quot;,\&quot;persistent\&quot;:false,\&quot;dynamic\&quot;:false,\&quot;selectize\&quot;:false,\&quot;opacityDim\&quot;:0.2,\&quot;selected\&quot;:{\&quot;opacity\&quot;:1}}); }&quot;,&quot;data&quot;:null}]}}&lt;/script&gt;

&lt;script type=&quot;application/htmlwidget-sizing&quot; data-for=&quot;d5e33bb3e726&quot;&gt;{&quot;viewer&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:15,&quot;fill&quot;:false},&quot;browser&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:0,&quot;fill&quot;:false}}&lt;/script&gt;

&lt;p&gt;In this case, performance between the two backends is &lt;em&gt;equal&lt;/em&gt;, both in accuracy and speed. Perhaps the MLP benefits of CNTK and the CNN benefits of TensorFlow canceled each other out.&lt;/p&gt;

&lt;h2&gt;Nietzsche Text Generation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/lstm_text_generation.py&quot;&gt;Text generation&lt;/a&gt; based off of &lt;a href=&quot;https://github.com/karpathy/char-rnn&quot;&gt;char-rnn&lt;/a&gt; is popular. Specifically, it uses a &lt;strong&gt;LSTM&lt;/strong&gt; to &amp;ldquo;learn&amp;rdquo; the text and sample new text. In the Keras example using &lt;a href=&quot;https://en.wikipedia.org/wiki/Friedrich_Nietzsche&quot;&gt;Nietzsche&amp;rsquo;s&lt;/a&gt; &lt;a href=&quot;https://s3.amazonaws.com/text-datasets/nietzsche.txt&quot;&gt;ramblings&lt;/a&gt; as the source dataset, the model attempts to predict the next character using the previous 40 characters, and minimize the training loss. Ideally you want below 1.00 loss before generated text is grammatically coherent.&lt;/p&gt;

&lt;div id=&quot;htmlwidget_container&quot;&gt;
  &lt;div id=&quot;d5e39946fc&quot; style=&quot;width:100%;height:400px;&quot; class=&quot;plotly html-widget&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;script type=&quot;application/json&quot; data-for=&quot;d5e39946fc&quot;&gt;{&quot;x&quot;:{&quot;data&quot;:[{&quot;x&quot;:[1,2,3,4,5,6,7,8,9,10],&quot;y&quot;:[2.00440777019,1.64802151896,1.55469932735,1.50813395339,1.47896788624,1.45615055283,1.44040952227,1.42692558066,1.41612182536,1.40472281286],&quot;text&quot;:[&quot;epoch:  1&lt;br /&gt;loss: 2.004408&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  2&lt;br /&gt;loss: 1.648022&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  3&lt;br /&gt;loss: 1.554699&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  4&lt;br /&gt;loss: 1.508134&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  5&lt;br /&gt;loss: 1.478968&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  6&lt;br /&gt;loss: 1.456151&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  7&lt;br /&gt;loss: 1.440410&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  8&lt;br /&gt;loss: 1.426926&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  9&lt;br /&gt;loss: 1.416122&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 10&lt;br /&gt;loss: 1.404723&lt;br /&gt;framework: CNTK&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;dash&quot;:&quot;solid&quot;},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[1,2,3,4,5,6,7,8,9,10],&quot;y&quot;:[2.01015738712,1.66685410016,1.55497078145,1.50765904726,1.47890785292,1.45558360526,1.44119170534,1.42519511551,1.41509616279,1.40604660672],&quot;text&quot;:[&quot;epoch:  1&lt;br /&gt;loss: 2.010157&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  2&lt;br /&gt;loss: 1.666854&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  3&lt;br /&gt;loss: 1.554971&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  4&lt;br /&gt;loss: 1.507659&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  5&lt;br /&gt;loss: 1.478908&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  6&lt;br /&gt;loss: 1.455584&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  7&lt;br /&gt;loss: 1.441192&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  8&lt;br /&gt;loss: 1.425195&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  9&lt;br /&gt;loss: 1.415096&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 10&lt;br /&gt;loss: 1.406047&lt;br /&gt;framework: TensorFlow&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;dash&quot;:&quot;solid&quot;},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[10,9,8,7,6,5,4,2,3,1],&quot;y&quot;:[1.40472281286,1.41612182536,1.42692558066,1.44040952227,1.45615055283,1.47896788624,1.50813395339,1.64802151896,1.55469932735,2.00440777019],&quot;text&quot;:[&quot;epoch: 10&lt;br /&gt;loss: 1.404723&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  9&lt;br /&gt;loss: 1.416122&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  8&lt;br /&gt;loss: 1.426926&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  7&lt;br /&gt;loss: 1.440410&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  6&lt;br /&gt;loss: 1.456151&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  5&lt;br /&gt;loss: 1.478968&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  4&lt;br /&gt;loss: 1.508134&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  2&lt;br /&gt;loss: 1.648022&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  3&lt;br /&gt;loss: 1.554699&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  1&lt;br /&gt;loss: 2.004408&lt;br /&gt;framework: CNTK&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;markers&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;opacity&quot;:1,&quot;size&quot;:7.55905511811024,&quot;symbol&quot;:&quot;circle&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;}},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[10,9,8,7,6,5,4,3,2,1],&quot;y&quot;:[1.40604660672,1.41509616279,1.42519511551,1.44119170534,1.45558360526,1.47890785292,1.50765904726,1.55497078145,1.66685410016,2.01015738712],&quot;text&quot;:[&quot;epoch: 10&lt;br /&gt;loss: 1.406047&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  9&lt;br /&gt;loss: 1.415096&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  8&lt;br /&gt;loss: 1.425195&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  7&lt;br /&gt;loss: 1.441192&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  6&lt;br /&gt;loss: 1.455584&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  5&lt;br /&gt;loss: 1.478908&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  4&lt;br /&gt;loss: 1.507659&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  3&lt;br /&gt;loss: 1.554971&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  2&lt;br /&gt;loss: 1.666854&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  1&lt;br /&gt;loss: 2.010157&lt;br /&gt;framework: TensorFlow&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;markers&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;opacity&quot;:1,&quot;size&quot;:7.55905511811024,&quot;symbol&quot;:&quot;circle&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;}},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null}],&quot;layout&quot;:{&quot;margin&quot;:{&quot;t&quot;:53.3612287256123,&quot;r&quot;:9.29846409298464,&quot;b&quot;:53.1686176836862,&quot;l&quot;:54.8609381486094},&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;title&quot;:&quot;Performance of Text Generation via LSTM&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:22.3163138231631},&quot;xaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.55,10.45],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;2.5&quot;,&quot;5.0&quot;,&quot;7.5&quot;,&quot;10.0&quot;],&quot;tickvals&quot;:[2.5,5,7.5,10],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;2.5&quot;,&quot;5.0&quot;,&quot;7.5&quot;,&quot;10.0&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;y&quot;,&quot;title&quot;:&quot;Epoch&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;yaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[1.374451084147,2.040429115833],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;1.4&quot;,&quot;1.6&quot;,&quot;1.8&quot;,&quot;2.0&quot;],&quot;tickvals&quot;:[1.4,1.6,1.8,2],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;1.4&quot;,&quot;1.6&quot;,&quot;1.8&quot;,&quot;2.0&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;x&quot;,&quot;title&quot;:&quot;Loss (Lower is Better)&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;shapes&quot;:[{&quot;type&quot;:&quot;rect&quot;,&quot;fillcolor&quot;:null,&quot;line&quot;:{&quot;color&quot;:null,&quot;width&quot;:0,&quot;linetype&quot;:[]},&quot;yref&quot;:&quot;paper&quot;,&quot;xref&quot;:&quot;paper&quot;,&quot;x0&quot;:0,&quot;x1&quot;:1,&quot;y0&quot;:0,&quot;y1&quot;:1}],&quot;showlegend&quot;:true,&quot;legend&quot;:{&quot;bgcolor&quot;:null,&quot;bordercolor&quot;:null,&quot;borderwidth&quot;:0,&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;y&quot;:0.877694488188976},&quot;hovermode&quot;:&quot;closest&quot;,&quot;width&quot;:null,&quot;height&quot;:400,&quot;barmode&quot;:&quot;relative&quot;},&quot;config&quot;:{&quot;doubleClick&quot;:&quot;reset&quot;,&quot;modeBarButtonsToAdd&quot;:[{&quot;name&quot;:&quot;Collaborate&quot;,&quot;icon&quot;:{&quot;width&quot;:1000,&quot;ascent&quot;:500,&quot;descent&quot;:-50,&quot;path&quot;:&quot;M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z&quot;},&quot;click&quot;:&quot;function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }&quot;}],&quot;cloud&quot;:false},&quot;source&quot;:&quot;A&quot;,&quot;attrs&quot;:{&quot;d5e32c23029d&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;colour&quot;:{},&quot;type&quot;:&quot;scatter&quot;},&quot;d5e32e80a80e&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;colour&quot;:{}}},&quot;cur_data&quot;:&quot;d5e32c23029d&quot;,&quot;visdat&quot;:{&quot;d5e32c23029d&quot;:[&quot;function (y) &quot;,&quot;x&quot;],&quot;d5e32e80a80e&quot;:[&quot;function (y) &quot;,&quot;x&quot;]},&quot;highlight&quot;:{&quot;on&quot;:&quot;plotly_click&quot;,&quot;persistent&quot;:false,&quot;dynamic&quot;:false,&quot;selectize&quot;:false,&quot;opacityDim&quot;:0.2,&quot;selected&quot;:{&quot;opacity&quot;:1}},&quot;base_url&quot;:&quot;https://plot.ly&quot;},&quot;evals&quot;:[&quot;config.modeBarButtonsToAdd.0.click&quot;],&quot;jsHooks&quot;:{&quot;render&quot;:[{&quot;code&quot;:&quot;function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\&quot;on\&quot;:\&quot;plotly_click\&quot;,\&quot;persistent\&quot;:false,\&quot;dynamic\&quot;:false,\&quot;selectize\&quot;:false,\&quot;opacityDim\&quot;:0.2,\&quot;selected\&quot;:{\&quot;opacity\&quot;:1}}); }&quot;,&quot;data&quot;:null}]}}&lt;/script&gt;

&lt;script type=&quot;application/htmlwidget-sizing&quot; data-for=&quot;d5e39946fc&quot;&gt;{&quot;viewer&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:15,&quot;fill&quot;:false},&quot;browser&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:0,&quot;fill&quot;:false}}&lt;/script&gt;

&lt;div id=&quot;htmlwidget_container&quot;&gt;
  &lt;div id=&quot;d5e31d02de77&quot; style=&quot;width:100%;height:400px;&quot; class=&quot;plotly html-widget&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;script type=&quot;application/json&quot; data-for=&quot;d5e31d02de77&quot;&gt;{&quot;x&quot;:{&quot;data&quot;:[{&quot;orientation&quot;:&quot;v&quot;,&quot;width&quot;:0.5,&quot;base&quot;:0,&quot;x&quot;:[1],&quot;y&quot;:[46.3590185880661],&quot;text&quot;:&quot;framework: CNTK&lt;br /&gt;mean: 46.35902&quot;,&quot;type&quot;:&quot;bar&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;transparent&quot;}},&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;orientation&quot;:&quot;v&quot;,&quot;width&quot;:0.5,&quot;base&quot;:0,&quot;x&quot;:[2],&quot;y&quot;:[87.6180793523788],&quot;text&quot;:&quot;framework: TensorFlow&lt;br /&gt;mean: 87.61808&quot;,&quot;type&quot;:&quot;bar&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;transparent&quot;}},&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[1],&quot;y&quot;:[46.3590185880661],&quot;text&quot;:&quot;framework: CNTK&lt;br /&gt;mean: 46.35902&quot;,&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;opacity&quot;:1,&quot;line&quot;:{&quot;color&quot;:&quot;transparent&quot;},&quot;error_y&quot;:{&quot;array&quot;:[1.3510494601399],&quot;arrayminus&quot;:[0.473184503636254],&quot;type&quot;:&quot;data&quot;,&quot;width&quot;:19.8863636363636,&quot;symmetric&quot;:false,&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;},&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[2],&quot;y&quot;:[87.6180793523788],&quot;text&quot;:&quot;framework: TensorFlow&lt;br /&gt;mean: 87.61808&quot;,&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;opacity&quot;:1,&quot;line&quot;:{&quot;color&quot;:&quot;transparent&quot;},&quot;error_y&quot;:{&quot;array&quot;:[1.02050027749789],&quot;arrayminus&quot;:[0.707233857780466],&quot;type&quot;:&quot;data&quot;,&quot;width&quot;:19.8863636363636,&quot;symmetric&quot;:false,&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;},&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null}],&quot;layout&quot;:{&quot;margin&quot;:{&quot;t&quot;:53.3612287256123,&quot;r&quot;:9.29846409298464,&quot;b&quot;:53.1686176836862,&quot;l&quot;:47.4221668742217},&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;title&quot;:&quot;Speed of Text Generation via LSTM&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:22.3163138231631},&quot;xaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.4,2.6],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;CNTK&quot;,&quot;TensorFlow&quot;],&quot;tickvals&quot;:[1,2],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;CNTK&quot;,&quot;TensorFlow&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;y&quot;,&quot;title&quot;:&quot;Keras Backend&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;yaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[-4.43192898149384,93.0705086113706],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;0&quot;,&quot;25&quot;,&quot;50&quot;,&quot;75&quot;],&quot;tickvals&quot;:[0,25,50,75],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;0&quot;,&quot;25&quot;,&quot;50&quot;,&quot;75&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;x&quot;,&quot;title&quot;:&quot;Average Epoch Runtime (seconds)&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;shapes&quot;:[{&quot;type&quot;:&quot;rect&quot;,&quot;fillcolor&quot;:null,&quot;line&quot;:{&quot;color&quot;:null,&quot;width&quot;:0,&quot;linetype&quot;:[]},&quot;yref&quot;:&quot;paper&quot;,&quot;xref&quot;:&quot;paper&quot;,&quot;x0&quot;:0,&quot;x1&quot;:1,&quot;y0&quot;:0,&quot;y1&quot;:1}],&quot;showlegend&quot;:true,&quot;legend&quot;:{&quot;bgcolor&quot;:null,&quot;bordercolor&quot;:null,&quot;borderwidth&quot;:0,&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;y&quot;:0.877694488188976},&quot;hovermode&quot;:&quot;closest&quot;,&quot;width&quot;:null,&quot;height&quot;:400,&quot;barmode&quot;:&quot;relative&quot;},&quot;config&quot;:{&quot;doubleClick&quot;:&quot;reset&quot;,&quot;modeBarButtonsToAdd&quot;:[{&quot;name&quot;:&quot;Collaborate&quot;,&quot;icon&quot;:{&quot;width&quot;:1000,&quot;ascent&quot;:500,&quot;descent&quot;:-50,&quot;path&quot;:&quot;M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z&quot;},&quot;click&quot;:&quot;function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }&quot;}],&quot;cloud&quot;:false},&quot;source&quot;:&quot;A&quot;,&quot;attrs&quot;:{&quot;d5e32f5a5bdc&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;ymin&quot;:{},&quot;ymax&quot;:{},&quot;fill&quot;:{},&quot;type&quot;:&quot;bar&quot;},&quot;d5e35544e0cd&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;ymin&quot;:{},&quot;ymax&quot;:{},&quot;fill&quot;:{}}},&quot;cur_data&quot;:&quot;d5e32f5a5bdc&quot;,&quot;visdat&quot;:{&quot;d5e32f5a5bdc&quot;:[&quot;function (y) &quot;,&quot;x&quot;],&quot;d5e35544e0cd&quot;:[&quot;function (y) &quot;,&quot;x&quot;]},&quot;highlight&quot;:{&quot;on&quot;:&quot;plotly_click&quot;,&quot;persistent&quot;:false,&quot;dynamic&quot;:false,&quot;selectize&quot;:false,&quot;opacityDim&quot;:0.2,&quot;selected&quot;:{&quot;opacity&quot;:1}},&quot;base_url&quot;:&quot;https://plot.ly&quot;},&quot;evals&quot;:[&quot;config.modeBarButtonsToAdd.0.click&quot;],&quot;jsHooks&quot;:{&quot;render&quot;:[{&quot;code&quot;:&quot;function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\&quot;on\&quot;:\&quot;plotly_click\&quot;,\&quot;persistent\&quot;:false,\&quot;dynamic\&quot;:false,\&quot;selectize\&quot;:false,\&quot;opacityDim\&quot;:0.2,\&quot;selected\&quot;:{\&quot;opacity\&quot;:1}}); }&quot;,&quot;data&quot;:null}]}}&lt;/script&gt;

&lt;script type=&quot;application/htmlwidget-sizing&quot; data-for=&quot;d5e31d02de77&quot;&gt;{&quot;viewer&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:15,&quot;fill&quot;:false},&quot;browser&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:0,&quot;fill&quot;:false}}&lt;/script&gt;

&lt;p&gt;Both have similar changes in loss over time (unfortunately, a loss of 1.40 will still result in gibberish text generated), although performance on CTNK is again fast due to the LSTM architecture.&lt;/p&gt;

&lt;p&gt;For this next benchmark, I will &lt;em&gt;not&lt;/em&gt; use a official Keras example script, but instead use my &lt;em&gt;own&lt;/em&gt; &lt;a href=&quot;https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/text_generator_keras.py&quot;&gt;text generator architecture&lt;/a&gt;, created during &lt;a href=&quot;http://minimaxir.com/2017/04/char-embeddings/&quot;&gt;my previous Keras post&lt;/a&gt;.&lt;/p&gt;

&lt;div id=&quot;htmlwidget_container&quot;&gt;
  &lt;div id=&quot;d5e33e1d4f93&quot; style=&quot;width:100%;height:400px;&quot; class=&quot;plotly html-widget&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;script type=&quot;application/json&quot; data-for=&quot;d5e33e1d4f93&quot;&gt;{&quot;x&quot;:{&quot;data&quot;:[{&quot;x&quot;:[1,2,3,4,5,6,7,8,9,10],&quot;y&quot;:[2.00351653112,1.72485264239,1.60276587148,1.51993696254,1.46022373265,1.41141036682,1.37179022428,1.33587299931,1.3062139421,1.27900991821],&quot;text&quot;:[&quot;epoch:  1&lt;br /&gt;loss: 2.003517&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  2&lt;br /&gt;loss: 1.724853&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  3&lt;br /&gt;loss: 1.602766&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  4&lt;br /&gt;loss: 1.519937&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  5&lt;br /&gt;loss: 1.460224&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  6&lt;br /&gt;loss: 1.411410&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  7&lt;br /&gt;loss: 1.371790&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  8&lt;br /&gt;loss: 1.335873&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  9&lt;br /&gt;loss: 1.306214&lt;br /&gt;framework: CNTK&quot;,&quot;epoch: 10&lt;br /&gt;loss: 1.279010&lt;br /&gt;framework: CNTK&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;dash&quot;:&quot;solid&quot;},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[1,2,3,4,5,6,7,8,9,10],&quot;y&quot;:[2.00852505003,1.73037641162,1.60431502849,1.52106773144,1.45870362669,1.40930885724,1.36821400507,1.33404665547,1.30228422424,1.27453109394],&quot;text&quot;:[&quot;epoch:  1&lt;br /&gt;loss: 2.008525&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  2&lt;br /&gt;loss: 1.730376&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  3&lt;br /&gt;loss: 1.604315&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  4&lt;br /&gt;loss: 1.521068&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  5&lt;br /&gt;loss: 1.458704&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  6&lt;br /&gt;loss: 1.409309&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  7&lt;br /&gt;loss: 1.368214&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  8&lt;br /&gt;loss: 1.334047&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  9&lt;br /&gt;loss: 1.302284&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch: 10&lt;br /&gt;loss: 1.274531&lt;br /&gt;framework: TensorFlow&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;dash&quot;:&quot;solid&quot;},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[10,9,8,7,6,5,4,3,2,1],&quot;y&quot;:[1.27900991821,1.3062139421,1.33587299931,1.37179022428,1.41141036682,1.46022373265,1.51993696254,1.60276587148,1.72485264239,2.00351653112],&quot;text&quot;:[&quot;epoch: 10&lt;br /&gt;loss: 1.279010&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  9&lt;br /&gt;loss: 1.306214&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  8&lt;br /&gt;loss: 1.335873&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  7&lt;br /&gt;loss: 1.371790&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  6&lt;br /&gt;loss: 1.411410&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  5&lt;br /&gt;loss: 1.460224&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  4&lt;br /&gt;loss: 1.519937&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  3&lt;br /&gt;loss: 1.602766&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  2&lt;br /&gt;loss: 1.724853&lt;br /&gt;framework: CNTK&quot;,&quot;epoch:  1&lt;br /&gt;loss: 2.003517&lt;br /&gt;framework: CNTK&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;markers&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;opacity&quot;:1,&quot;size&quot;:7.55905511811024,&quot;symbol&quot;:&quot;circle&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;}},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[10,9,8,7,6,5,4,3,2,1],&quot;y&quot;:[1.27453109394,1.30228422424,1.33404665547,1.36821400507,1.40930885724,1.45870362669,1.52106773144,1.60431502849,1.73037641162,2.00852505003],&quot;text&quot;:[&quot;epoch: 10&lt;br /&gt;loss: 1.274531&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  9&lt;br /&gt;loss: 1.302284&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  8&lt;br /&gt;loss: 1.334047&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  7&lt;br /&gt;loss: 1.368214&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  6&lt;br /&gt;loss: 1.409309&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  5&lt;br /&gt;loss: 1.458704&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  4&lt;br /&gt;loss: 1.521068&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  3&lt;br /&gt;loss: 1.604315&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  2&lt;br /&gt;loss: 1.730376&lt;br /&gt;framework: TensorFlow&quot;,&quot;epoch:  1&lt;br /&gt;loss: 2.008525&lt;br /&gt;framework: TensorFlow&quot;],&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;markers&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;opacity&quot;:1,&quot;size&quot;:7.55905511811024,&quot;symbol&quot;:&quot;circle&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;}},&quot;hoveron&quot;:&quot;points&quot;,&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null}],&quot;layout&quot;:{&quot;margin&quot;:{&quot;t&quot;:53.3612287256123,&quot;r&quot;:9.29846409298464,&quot;b&quot;:53.1686176836862,&quot;l&quot;:54.8609381486094},&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;title&quot;:&quot;Performance of Text Generation via Custom Keras Model&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:22.3163138231631},&quot;xaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.55,10.45],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;2.5&quot;,&quot;5.0&quot;,&quot;7.5&quot;,&quot;10.0&quot;],&quot;tickvals&quot;:[2.5,5,7.5,10],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;2.5&quot;,&quot;5.0&quot;,&quot;7.5&quot;,&quot;10.0&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;y&quot;,&quot;title&quot;:&quot;Epoch&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;yaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[1.2378313961355,2.0452247478345],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;1.4&quot;,&quot;1.6&quot;,&quot;1.8&quot;,&quot;2.0&quot;],&quot;tickvals&quot;:[1.4,1.6,1.8,2],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;1.4&quot;,&quot;1.6&quot;,&quot;1.8&quot;,&quot;2.0&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;x&quot;,&quot;title&quot;:&quot;Loss (Lower is Better)&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;shapes&quot;:[{&quot;type&quot;:&quot;rect&quot;,&quot;fillcolor&quot;:null,&quot;line&quot;:{&quot;color&quot;:null,&quot;width&quot;:0,&quot;linetype&quot;:[]},&quot;yref&quot;:&quot;paper&quot;,&quot;xref&quot;:&quot;paper&quot;,&quot;x0&quot;:0,&quot;x1&quot;:1,&quot;y0&quot;:0,&quot;y1&quot;:1}],&quot;showlegend&quot;:true,&quot;legend&quot;:{&quot;bgcolor&quot;:null,&quot;bordercolor&quot;:null,&quot;borderwidth&quot;:0,&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;y&quot;:0.877694488188976},&quot;hovermode&quot;:&quot;closest&quot;,&quot;width&quot;:null,&quot;height&quot;:400,&quot;barmode&quot;:&quot;relative&quot;},&quot;config&quot;:{&quot;doubleClick&quot;:&quot;reset&quot;,&quot;modeBarButtonsToAdd&quot;:[{&quot;name&quot;:&quot;Collaborate&quot;,&quot;icon&quot;:{&quot;width&quot;:1000,&quot;ascent&quot;:500,&quot;descent&quot;:-50,&quot;path&quot;:&quot;M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z&quot;},&quot;click&quot;:&quot;function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }&quot;}],&quot;cloud&quot;:false},&quot;source&quot;:&quot;A&quot;,&quot;attrs&quot;:{&quot;d5e3711b058f&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;colour&quot;:{},&quot;type&quot;:&quot;scatter&quot;},&quot;d5e3250a294c&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;colour&quot;:{}}},&quot;cur_data&quot;:&quot;d5e3711b058f&quot;,&quot;visdat&quot;:{&quot;d5e3711b058f&quot;:[&quot;function (y) &quot;,&quot;x&quot;],&quot;d5e3250a294c&quot;:[&quot;function (y) &quot;,&quot;x&quot;]},&quot;highlight&quot;:{&quot;on&quot;:&quot;plotly_click&quot;,&quot;persistent&quot;:false,&quot;dynamic&quot;:false,&quot;selectize&quot;:false,&quot;opacityDim&quot;:0.2,&quot;selected&quot;:{&quot;opacity&quot;:1}},&quot;base_url&quot;:&quot;https://plot.ly&quot;},&quot;evals&quot;:[&quot;config.modeBarButtonsToAdd.0.click&quot;],&quot;jsHooks&quot;:{&quot;render&quot;:[{&quot;code&quot;:&quot;function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\&quot;on\&quot;:\&quot;plotly_click\&quot;,\&quot;persistent\&quot;:false,\&quot;dynamic\&quot;:false,\&quot;selectize\&quot;:false,\&quot;opacityDim\&quot;:0.2,\&quot;selected\&quot;:{\&quot;opacity\&quot;:1}}); }&quot;,&quot;data&quot;:null}]}}&lt;/script&gt;

&lt;script type=&quot;application/htmlwidget-sizing&quot; data-for=&quot;d5e33e1d4f93&quot;&gt;{&quot;viewer&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:15,&quot;fill&quot;:false},&quot;browser&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:0,&quot;fill&quot;:false}}&lt;/script&gt;

&lt;div id=&quot;htmlwidget_container&quot;&gt;
  &lt;div id=&quot;d5e357be0802&quot; style=&quot;width:100%;height:400px;&quot; class=&quot;plotly html-widget&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;script type=&quot;application/json&quot; data-for=&quot;d5e357be0802&quot;&gt;{&quot;x&quot;:{&quot;data&quot;:[{&quot;orientation&quot;:&quot;v&quot;,&quot;width&quot;:0.5,&quot;base&quot;:0,&quot;x&quot;:[1],&quot;y&quot;:[100.614623141289],&quot;text&quot;:&quot;framework: CNTK&lt;br /&gt;mean: 100.6146&quot;,&quot;type&quot;:&quot;bar&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(41,128,185,1)&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;transparent&quot;}},&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;orientation&quot;:&quot;v&quot;,&quot;width&quot;:0.5,&quot;base&quot;:0,&quot;x&quot;:[2],&quot;y&quot;:[108.959185552597],&quot;text&quot;:&quot;framework: TensorFlow&lt;br /&gt;mean: 108.9592&quot;,&quot;type&quot;:&quot;bar&quot;,&quot;marker&quot;:{&quot;autocolorscale&quot;:false,&quot;color&quot;:&quot;rgba(192,57,43,1)&quot;,&quot;line&quot;:{&quot;width&quot;:1.88976377952756,&quot;color&quot;:&quot;transparent&quot;}},&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:true,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[1],&quot;y&quot;:[100.614623141289],&quot;text&quot;:&quot;framework: CNTK&lt;br /&gt;mean: 100.6146&quot;,&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;opacity&quot;:1,&quot;line&quot;:{&quot;color&quot;:&quot;transparent&quot;},&quot;error_y&quot;:{&quot;array&quot;:[1.08758223846512],&quot;arrayminus&quot;:[0.724098751095781],&quot;type&quot;:&quot;data&quot;,&quot;width&quot;:19.8863636363636,&quot;symmetric&quot;:false,&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;},&quot;name&quot;:&quot;CNTK&quot;,&quot;legendgroup&quot;:&quot;CNTK&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null},{&quot;x&quot;:[2],&quot;y&quot;:[108.959185552597],&quot;text&quot;:&quot;framework: TensorFlow&lt;br /&gt;mean: 108.9592&quot;,&quot;type&quot;:&quot;scatter&quot;,&quot;mode&quot;:&quot;lines&quot;,&quot;opacity&quot;:1,&quot;line&quot;:{&quot;color&quot;:&quot;transparent&quot;},&quot;error_y&quot;:{&quot;array&quot;:[0.933447687045287],&quot;arrayminus&quot;:[0.662856574837988],&quot;type&quot;:&quot;data&quot;,&quot;width&quot;:19.8863636363636,&quot;symmetric&quot;:false,&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;},&quot;name&quot;:&quot;TensorFlow&quot;,&quot;legendgroup&quot;:&quot;TensorFlow&quot;,&quot;showlegend&quot;:false,&quot;xaxis&quot;:&quot;x&quot;,&quot;yaxis&quot;:&quot;y&quot;,&quot;hoverinfo&quot;:&quot;text&quot;,&quot;frame&quot;:null}],&quot;layout&quot;:{&quot;margin&quot;:{&quot;t&quot;:53.3612287256123,&quot;r&quot;:9.29846409298464,&quot;b&quot;:53.1686176836862,&quot;l&quot;:47.4221668742217},&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;title&quot;:&quot;Speed of Text Generation via Custom Keras Model&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:22.3163138231631},&quot;xaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[0.4,2.6],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;CNTK&quot;,&quot;TensorFlow&quot;],&quot;tickvals&quot;:[1,2],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;CNTK&quot;,&quot;TensorFlow&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;y&quot;,&quot;title&quot;:&quot;Keras Backend&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;yaxis&quot;:{&quot;domain&quot;:[0,1],&quot;type&quot;:&quot;linear&quot;,&quot;autorange&quot;:false,&quot;range&quot;:[-5.49463166198212,115.387264901624],&quot;tickmode&quot;:&quot;array&quot;,&quot;ticktext&quot;:[&quot;0&quot;,&quot;30&quot;,&quot;60&quot;,&quot;90&quot;],&quot;tickvals&quot;:[0,30,60,90],&quot;categoryorder&quot;:&quot;array&quot;,&quot;categoryarray&quot;:[&quot;0&quot;,&quot;30&quot;,&quot;60&quot;,&quot;90&quot;],&quot;nticks&quot;:null,&quot;ticks&quot;:&quot;&quot;,&quot;tickcolor&quot;:null,&quot;ticklen&quot;:4.64923204649232,&quot;tickwidth&quot;:0,&quot;showticklabels&quot;:true,&quot;tickfont&quot;:{&quot;color&quot;:&quot;rgba(77,77,77,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;tickangle&quot;:-0,&quot;showline&quot;:false,&quot;linecolor&quot;:null,&quot;linewidth&quot;:0,&quot;showgrid&quot;:true,&quot;gridcolor&quot;:&quot;rgba(235,235,235,1)&quot;,&quot;gridwidth&quot;:0.66417600664176,&quot;zeroline&quot;:false,&quot;anchor&quot;:&quot;x&quot;,&quot;title&quot;:&quot;Average Epoch Runtime (seconds)&quot;,&quot;titlefont&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:18.5969281859693},&quot;hoverformat&quot;:&quot;.2f&quot;},&quot;shapes&quot;:[{&quot;type&quot;:&quot;rect&quot;,&quot;fillcolor&quot;:null,&quot;line&quot;:{&quot;color&quot;:null,&quot;width&quot;:0,&quot;linetype&quot;:[]},&quot;yref&quot;:&quot;paper&quot;,&quot;xref&quot;:&quot;paper&quot;,&quot;x0&quot;:0,&quot;x1&quot;:1,&quot;y0&quot;:0,&quot;y1&quot;:1}],&quot;showlegend&quot;:true,&quot;legend&quot;:{&quot;bgcolor&quot;:null,&quot;bordercolor&quot;:null,&quot;borderwidth&quot;:0,&quot;font&quot;:{&quot;color&quot;:&quot;rgba(0,0,0,1)&quot;,&quot;family&quot;:&quot;Source Sans Pro&quot;,&quot;size&quot;:14.8775425487754},&quot;y&quot;:0.877694488188976},&quot;hovermode&quot;:&quot;closest&quot;,&quot;width&quot;:null,&quot;height&quot;:400,&quot;barmode&quot;:&quot;relative&quot;},&quot;config&quot;:{&quot;doubleClick&quot;:&quot;reset&quot;,&quot;modeBarButtonsToAdd&quot;:[{&quot;name&quot;:&quot;Collaborate&quot;,&quot;icon&quot;:{&quot;width&quot;:1000,&quot;ascent&quot;:500,&quot;descent&quot;:-50,&quot;path&quot;:&quot;M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z&quot;},&quot;click&quot;:&quot;function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }&quot;}],&quot;cloud&quot;:false},&quot;source&quot;:&quot;A&quot;,&quot;attrs&quot;:{&quot;d5e3314208e1&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;ymin&quot;:{},&quot;ymax&quot;:{},&quot;fill&quot;:{},&quot;type&quot;:&quot;bar&quot;},&quot;d5e36655050a&quot;:{&quot;x&quot;:{},&quot;y&quot;:{},&quot;ymin&quot;:{},&quot;ymax&quot;:{},&quot;fill&quot;:{}}},&quot;cur_data&quot;:&quot;d5e3314208e1&quot;,&quot;visdat&quot;:{&quot;d5e3314208e1&quot;:[&quot;function (y) &quot;,&quot;x&quot;],&quot;d5e36655050a&quot;:[&quot;function (y) &quot;,&quot;x&quot;]},&quot;highlight&quot;:{&quot;on&quot;:&quot;plotly_click&quot;,&quot;persistent&quot;:false,&quot;dynamic&quot;:false,&quot;selectize&quot;:false,&quot;opacityDim&quot;:0.2,&quot;selected&quot;:{&quot;opacity&quot;:1}},&quot;base_url&quot;:&quot;https://plot.ly&quot;},&quot;evals&quot;:[&quot;config.modeBarButtonsToAdd.0.click&quot;],&quot;jsHooks&quot;:{&quot;render&quot;:[{&quot;code&quot;:&quot;function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\&quot;on\&quot;:\&quot;plotly_click\&quot;,\&quot;persistent\&quot;:false,\&quot;dynamic\&quot;:false,\&quot;selectize\&quot;:false,\&quot;opacityDim\&quot;:0.2,\&quot;selected\&quot;:{\&quot;opacity\&quot;:1}}); }&quot;,&quot;data&quot;:null}]}}&lt;/script&gt;

&lt;script type=&quot;application/htmlwidget-sizing&quot; data-for=&quot;d5e357be0802&quot;&gt;{&quot;viewer&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:15,&quot;fill&quot;:false},&quot;browser&quot;:{&quot;width&quot;:&quot;100%&quot;,&quot;height&quot;:400,&quot;padding&quot;:0,&quot;fill&quot;:false}}&lt;/script&gt;

&lt;p&gt;My network avoids converging early with only a minor cost to training speed in the TensorFlow case; unfortunately, CNTK speed is &lt;em&gt;much&lt;/em&gt; slower than the simple model, but still faster than TensorFlow in the advanced model.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the generated text output from the TensorFlow-trained model on my architecture:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;hinks the rich man must be wholly perverity and connection of the english sin of the philosophers of the basis of the same profound of his placed and evil and exception of fear to plants to me such as the case of the will seems to the will to be every such a remark as a primates of a strong of
[...]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And here&amp;rsquo;s the output from the CNTK-trained model:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(_x2js1hevjg4z_?z_a√¶?q_gpmj:sn![?(f3_ch=lhw4y n6)gkh
kujau
momu,?!lj√´7g)k,!?[45 0as9[d.68√©hhptvsx jd_n√¶i,√§_z!cwkr&amp;quot;_f6√´-mu_(epp
[...]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Wait, what? Apparently my model architecture caused CNTK to hit a legitimate bug when making predictions, which did not happen with CNTK + the simple LSTM architecture. Thanks to my QA skills, I found that &lt;a href=&quot;https://keras.io/layers/normalization/&quot;&gt;batch normalization&lt;/a&gt; was the cause of the bug and &lt;a href=&quot;https://github.com/Microsoft/CNTK/issues/1994&quot;&gt;filed the issue&lt;/a&gt; appropriately.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In all, the title of this post does not follow &lt;a href=&quot;https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines&quot;&gt;Betteridge&amp;rsquo;s law of headlines&lt;/a&gt;; deciding the better Keras framework is not as clear cut as expected. Accuracy is mostly identical between the two frameworks. CNTK is faster at LSTMs/MLPs, TensorFlow is faster at CNNs/Embeddings, but when networks implement &lt;em&gt;both&lt;/em&gt;, it&amp;rsquo;s a tie.&lt;/p&gt;

&lt;p&gt;Random bug aside, it&amp;rsquo;s possible that CNTK is not fully optimized for running on Keras (indeed, the 1bit-SGD functionality &lt;a href=&quot;https://github.com/Microsoft/CNTK/issues/1975&quot;&gt;does not work&lt;/a&gt; yet) so there is still room for future improvement. Despite that, the results for simply setting a flag are &lt;em&gt;extremely&lt;/em&gt; impressive, and it is worth testing Keras models on both CNTK and TensorFlow now to see which is better before deploying them to production.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;All scripts for running the benchmark are available in &lt;a href=&quot;https://github.com/minimaxir/keras-cntk-benchmark&quot;&gt;this GitHub repo&lt;/a&gt;. You can view the R/ggplot2 code used to process the logs and create the interactive visualizations in &lt;a href=&quot;http://minimaxir.com/notebooks/keras-cntk/&quot;&gt;this R Notebook&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 12 Jun 2017 08:30:00 -0700</pubDate>
        <link>http://minimaxir.com/2017/06/keras-cntk/</link>
        <guid isPermaLink="true">http://minimaxir.com/2017/06/keras-cntk/</guid>
        
        
        <category>Data</category>
        
        <category>Interactive</category>
        
      </item>
    
      <item>
        <title>Advantages of Using R Notebooks For Data Analysis Instead of Jupyter Notebooks</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://jupyter.org&quot;&gt;Jupyter Notebooks&lt;/a&gt;, formerly known as &lt;a href=&quot;https://ipython.org/notebook.html&quot;&gt;IPython Notebooks&lt;/a&gt;, are ubiquitous in modern data analysis. The Notebook format allows statistical code and its output to be viewed on any computer in a logical and &lt;em&gt;reproducible&lt;/em&gt; manner, avoiding both the confusion caused by unclear code and the inevitable &amp;ldquo;it only works on my system&amp;rdquo; curse.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/r-notebooks/jupyterdemo.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;In Jupyter Notebooks, each block of Python input code executes in its own cell, and the output of the block appears inline; this allows the user to iterate on the results, both to make the data transformations explicit and to and make sure the results are as expected.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/r-notebooks/jupyter.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;In addition to code blocks, Jupyter Notebooks support &lt;a href=&quot;https://en.wikipedia.org/wiki/Markdown&quot;&gt;Markdown&lt;/a&gt; cells, allowing for more detailed write-ups with easy formatting. The final Notebook can be exported as a HTML file displayable in a browser, or the raw Notebook file can be shared and &lt;a href=&quot;https://github.com/blog/1995-github-jupyter-notebooks-3&quot;&gt;rendered&lt;/a&gt; on sites like &lt;a href=&quot;https://github.com&quot;&gt;GitHub&lt;/a&gt;. Although Jupyter is a Python application, it can run kernels of &lt;a href=&quot;https://irkernel.github.io&quot;&gt;non-Python languages&lt;/a&gt;, such as &lt;a href=&quot;https://www.r-project.org&quot;&gt;R&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Over the years, there have a been a few new competitors in the reproducible data analysis field, such as &lt;a href=&quot;http://beakernotebook.com/features&quot;&gt;Beaker Notebook&lt;/a&gt; and, for heavy-duty business problems, &lt;a href=&quot;https://zeppelin.apache.org&quot;&gt;Apache Zeppelin&lt;/a&gt;. However, today we&amp;rsquo;ll look at the relatively new &lt;a href=&quot;http://rmarkdown.rstudio.com/r_notebooks.html&quot;&gt;R Notebooks&lt;/a&gt;, and how they help improve the workflows of common data analysis in ways Jupyter Notebooks can&amp;rsquo;t without third-party extensions.&lt;/p&gt;

&lt;h2&gt;About R Notebooks&lt;/h2&gt;

&lt;p&gt;R Notebooks are a format maintained by &lt;a href=&quot;https://www.rstudio.com&quot;&gt;RStudio&lt;/a&gt;, which develops and maintains a large number of open source R packages and tools, most notably the free-for-consumer RStudio R IDE. More specifically, R Notebooks are an extension of the earlier &lt;a href=&quot;http://rmarkdown.rstudio.com&quot;&gt;R Markdown&lt;/a&gt; &lt;code&gt;.Rmd&lt;/code&gt; format, useful for rendering analyses into HTML/PDFs, or other cool formats like &lt;a href=&quot;http://rmarkdown.rstudio.com/tufte_handout_format.html&quot;&gt;Tufte handouts&lt;/a&gt; or even &lt;a href=&quot;https://bookdown.org&quot;&gt;books&lt;/a&gt;. The default output of an R Notebook file is a &lt;code&gt;.nb.html&lt;/code&gt; file, which can be viewed as a webpage on any system. (&lt;a href=&quot;https://rpubs.com&quot;&gt;RPubs&lt;/a&gt; has many examples of R Notebooks, although I recommend using &lt;a href=&quot;https://pages.github.com&quot;&gt;GitHub Pages&lt;/a&gt; to host notebooks publicly).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/r-notebooks/RNotebookAnimation.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Instead of having separate cells for code and text, a R Markdown file is all plain text. The cells are indicated by three backticks and a gray background in RStudio, which makes it easy to enter a code block, easy to identify code blocks at a glance, and easy to execute a notebook block-by-block. Each cell also has a green indicator bar which shows which code is running and which code is queued, line-by-line.&lt;/p&gt;

&lt;p&gt;For Notebook files, a HTML webpage is automatically generated whenever the file is saved, which can immediately be viewed in any browser (the generated webpage stores the cell output and any necessary dependencies).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/r-notebooks/notebooktest.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;R Notebooks can only be created and edited in RStudio, but this is a case where tight vertical integration of open-source software is a good thing. Among many other features, RStudio includes a file manager, a function help, a variable explorer, and a project manager; all of which make analysis much easier and faster as opposed to the browser-only Jupyter.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/r-notebooks/rstudio.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve made many, many Jupyter Notebooks and R Notebooks &lt;a href=&quot;http://minimaxir.com/data-portfolio&quot;&gt;over the years&lt;/a&gt;, which has given me insight into the strengths and weaknesses of both formats. Here are a few native features of R Notebooks which present an objective advantage over Jupyter Notebooks, particularly those not highlighted in the documentation:&lt;/p&gt;

&lt;h2&gt;Version Control&lt;/h2&gt;

&lt;p&gt;Version control of files with tools such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Git&quot;&gt;git&lt;/a&gt; is important as it both maintains an explorable database of changes to the code files and also improves collaboration by using a centralized server (e.g. GitHub) where anyone with access to the repository can pull and push changes to the code. In the data science world, large startups such as &lt;a href=&quot;https://stripe.com/blog/reproducible-research&quot;&gt;Stripe&lt;/a&gt; and &lt;a href=&quot;https://medium.com/airbnb-engineering/scaling-knowledge-at-airbnb-875d73eff091&quot;&gt;Airbnb&lt;/a&gt; have seen a lot of success with this approach.&lt;/p&gt;

&lt;p&gt;RStudio incidentally has a native git client for tracking and committing changes to a &lt;code&gt;.Rmd&lt;/code&gt; file, which is easy since &lt;code&gt;.Rmd&lt;/code&gt; files are effectively plain text files where you can see differences between versions at a per-line level. (You may not want to store the changes to the generated &lt;code&gt;.nb.html&lt;/code&gt; Notebook since they will be large and redundant to the changes made in the corresponding &lt;code&gt;.Rmd&lt;/code&gt;; I recommend adding a &lt;code&gt;*.nb.html&lt;/code&gt; rule to a &lt;code&gt;.gitignore&lt;/code&gt; file during analysis).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/r-notebooks/git.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;.ipynb&lt;/code&gt; Jupyter Notebook files are blobs of JSON that also store cell output, which will result in large diffs if you keep them in version control and make any changes which result in different output. This can cause the git database to balloon and makes reading per-line diffs hard if not impossible.&lt;/p&gt;

&lt;p&gt;On Hacker News, the version control issues in Jupyter are &lt;a href=&quot;https://news.ycombinator.com/item?id=14034341&quot;&gt;a common complaint&lt;/a&gt;, however a Jupyter developer noted of a possibility of &lt;a href=&quot;https://news.ycombinator.com/item?id=14035158&quot;&gt;working with RStudio&lt;/a&gt; on solving this issue.&lt;/p&gt;

&lt;h2&gt;Inline Code Rendering&lt;/h2&gt;

&lt;p&gt;A common practice in Jupyter Notebooks is to print common values as a part of a write-up or testing statistical code. In Jupyter Notebooks, if you want to verify the number of rows in a dataset for exploratory data analysis, you have to add an appropriate print statement to the cell to get the number &lt;code&gt;n&lt;/code&gt; rows, and then add a Markdown cell to redundantly describe what you just print in the output. &lt;/p&gt;

&lt;p&gt;In R Notebooks, you can skip a step by calling such print statements in-line in the Markdown text, which will then be rendered with the Notebook. This also avoids hard-coding such numbers in the Markdown text if you change the data beforehand (e.g. parameter tuning) or if the values are nontrivial to calculate by hand.&lt;/p&gt;

&lt;p&gt;For example, these lines of R Markdown from my &lt;a href=&quot;http://minimaxir.com/notebooks/first-comment/&quot;&gt;Reddit First Comment Notebook&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/r-notebooks/inline.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;translate into:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/r-notebooks/reddit.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;h2&gt;Metadata&lt;/h2&gt;

&lt;p&gt;R Notebooks are configured with a &lt;a href=&quot;http://yaml.org&quot;&gt;YAML&lt;/a&gt; header, which can include common attributes such as title, author, date published, and other relevant options. These fields will then be configured correctly in the metadata for HTML/PDF/Handouts output. Here&amp;rsquo;s an example from &lt;a href=&quot;http://minimaxir.com/notebooks/amazon-spark/&quot;&gt;one of my notebooks&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Playing&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Million&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Amazon&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Product&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Review&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Ratings&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Using&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Apache&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Spark&amp;quot;&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;author&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Max&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Woolf&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;(@minimaxir)&amp;quot;&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;January&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2nd,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2017&amp;quot;&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;html_notebook&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;highlight&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;tango&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;mathjax&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;null&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;number_sections&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;yes&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;theme&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spacelab&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;toc&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;yes&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;toc_float&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;yes&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Said metadata features are &lt;a href=&quot;https://github.com/ipython/ipython/issues/6073&quot;&gt;often requested but unimplemented&lt;/a&gt; in Jupyter.&lt;/p&gt;

&lt;h2&gt;Notebook Theming&lt;/h2&gt;

&lt;p&gt;As noted in the example metadata above, R Notebooks allow extensive theming. Jupyter Notebooks do &lt;a href=&quot;https://github.com/dunovank/jupyter-themes&quot;&gt;support themes&lt;/a&gt;, but with a third-party Python package, or placing custom CSS in an &lt;a href=&quot;https://stackoverflow.com/a/32158550&quot;&gt;odd location&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Like Jupyter Notebooks, the front-end of browser-based R Notebooks is based off of the &lt;a href=&quot;http://getbootstrap.com&quot;&gt;Bootstrap&lt;/a&gt; HTML framework. R Notebooks, however, allow you to natively select the style of code syntax highlighting via &lt;code&gt;highlight&lt;/code&gt; (similar options as &lt;a href=&quot;https://help.farbox.com/pygments.html&quot;&gt;pygments&lt;/a&gt;) and also the entire Bootstrap theme via &lt;code&gt;theme&lt;/code&gt; (with a selection from the excellent &lt;a href=&quot;https://bootswatch.com&quot;&gt;Bootswatch&lt;/a&gt; themes by &lt;a href=&quot;https://twitter.com/thomashpark&quot;&gt;Thomas Park&lt;/a&gt;), giving your Notebook a unique look without adding dependencies.&lt;/p&gt;

&lt;h2&gt;Data Tables&lt;/h2&gt;

&lt;p&gt;When you print a data frame in a Jupyter Notebook, the output appears as a standard &lt;em&gt;boring&lt;/em&gt; HTML table:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/r-notebooks/htmltable.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;No cell block output is ever truncated. Accidentally printing an entire 100,000+ row table to a Jupyter Notebook is a mistake you only make &lt;em&gt;once&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;R Notebook tables are pretty tables with pagination for both rows and columns, and can support large amounts of data if necessary.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/r-notebooks/rtable.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The R Notebook output table also includes the data type of the column, which is helpful for debugging unexpected issues where a column has an unintended data type (e.g. a numeric &lt;code&gt;&amp;lt;dbl&amp;gt;&lt;/code&gt; column or a datetime &lt;code&gt;&amp;lt;S3: POSIXct&amp;gt;&lt;/code&gt; column is parsed as a text-based &lt;code&gt;&amp;lt;chr&amp;gt;&lt;/code&gt; column).&lt;/p&gt;

&lt;h2&gt;Table of Contents&lt;/h2&gt;

&lt;p&gt;A Table of Contents always helps navigating, particularly in a PDF export. Jupyter Notebooks &lt;a href=&quot;https://github.com/minrk/ipython_extensions&quot;&gt;requires an extension&lt;/a&gt; for a ToC, while R Notebooks will natively create one from section headers (controllable via &lt;code&gt;toc&lt;/code&gt; and &lt;code&gt;number_sections&lt;/code&gt;). An optional &lt;code&gt;toc_float&lt;/code&gt; parameter causes the Table of Contents to float on the left in the browser, making it always accessible.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/r-notebooks/notebookheader.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;In conclusion, R Notebooks haven&amp;rsquo;t received much publicity since the benefits aren&amp;rsquo;t immediately obvious, but for the purpose of reproducible analyses, the breadth of native features allows for excellent utility while avoiding dependency hell. Running R in an R Notebook is a significantly better experience than running R in a Jupyter Notebook. The advantages present in R Notebooks can also provide guidance for feature development in other Notebook software, which improves the data analysis ecosystem as a whole.&lt;/p&gt;

&lt;p&gt;However, there&amp;rsquo;s an elephant in the room&amp;hellip;&lt;/p&gt;

&lt;h2&gt;What About Python?&lt;/h2&gt;

&lt;p&gt;So you might be thinking &amp;ldquo;an R Notebook forces you to use R, but &lt;em&gt;serious&lt;/em&gt; data science work is done using Python!&amp;rdquo; Plot twist: you can use Python in an R Notebook!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/r-notebooks/python.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Well, sort of. The Python session ends after the cell executes, making it unhelpful for tasks other than &lt;em&gt;ad hoc&lt;/em&gt; scripts.&lt;/p&gt;

&lt;p&gt;The topic on whether R or Python is better for data analysis is a &lt;a href=&quot;https://news.ycombinator.com/item?id=14056098&quot;&gt;common&lt;/a&gt; &lt;a href=&quot;https://news.ycombinator.com/item?id=13239530&quot;&gt;religious&lt;/a&gt; &lt;a href=&quot;https://news.ycombinator.com/item?id=12301996&quot;&gt;flamewar&lt;/a&gt; topic which is best saved for a separate blog post (tl;dr: I disagree with the paraphrased quote above in that both languages have their advantages and you&amp;rsquo;ll benefit significantly from knowing both ecosystems).&lt;/p&gt;

&lt;p&gt;And I wouldn&amp;rsquo;t count R out of &amp;ldquo;serious data science&amp;rdquo;. You can use R &lt;a href=&quot;http://spark.rstudio.com&quot;&gt;seamlessly&lt;/a&gt; with big data tools like &lt;a href=&quot;https://spark.apache.org&quot;&gt;Apache Spark&lt;/a&gt;, and R can &lt;a href=&quot;https://rstudio.github.io/keras/&quot;&gt;now&lt;/a&gt; use &lt;a href=&quot;https://keras.io&quot;&gt;Keras&lt;/a&gt;/&lt;a href=&quot;https://www.tensorflow.org&quot;&gt;TensorFlow&lt;/a&gt; for deep learning with near-API-parity to the Python version. &lt;em&gt;Hmm&lt;/em&gt;.&lt;/p&gt;
</description>
        <pubDate>Tue, 06 Jun 2017 08:30:00 -0700</pubDate>
        <link>http://minimaxir.com/2017/06/r-notebooks/</link>
        <guid isPermaLink="true">http://minimaxir.com/2017/06/r-notebooks/</guid>
        
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Leaving Apple Inc.</title>
        <description>&lt;p&gt;I‚Äôve been working in the San Francisco Bay Area for about 5 years, but I‚Äôve never publicly said where I‚Äôve worked. Well, I was a Software QA Engineer at &lt;a href=&quot;https://www.apple.com&quot;&gt;Apple Inc.&lt;/a&gt;, on the Applications team.&lt;/p&gt;

&lt;p&gt;As of last week, I handed in my resignation. While I am thankful for the opportunities I have had at Apple, it is time for me to pursue working in other areas I am passionate about and search for other companies to further my personal growth and technical skills. Resigning from a good job to look for something new might defy conventional wisdom, but the time is right for me to make this bold career move.&lt;/p&gt;

&lt;h2&gt;My Apple Story&lt;/h2&gt;

&lt;p&gt;I graduated with university honors at &lt;a href=&quot;http://www.cmu.edu&quot;&gt;Carnegie Mellon University&lt;/a&gt;, from the &lt;a href=&quot;http://tepper.cmu.edu&quot;&gt;Tepper School of Business&lt;/a&gt; with a focus on Computing and Information Technology (i.e. data architecture and coding algorithms), and a minor in Statistics.&lt;/p&gt;

&lt;p&gt;At the end of my senior year, I received an e-mail from a Software QA Manager at Apple (who followed my &lt;a href=&quot;http://techcommntr.tumblr.com&quot;&gt;comments&lt;/a&gt; at the bottom of &lt;a href=&quot;https://techcrunch.com&quot;&gt;TechCrunch&lt;/a&gt; articles) inviting me for an on-site interview. Following an offer, I moved to the Bay Area to start my first post-undergrad job in Cupertino.&lt;/p&gt;

&lt;p&gt;While I can&amp;rsquo;t really talk about what I worked on at Apple, I genuinely enjoyed the work, the product, and team. I had a high impact on the final result and I successfully helped qualify many major software releases. However, after a few years, I realized that my technical skill growth was stalling, so I looked for an an internal transfer to another department, ideally in a data analysis/software engineering role.&lt;/p&gt;

&lt;p&gt;Having received no responses internally, I realized I would have to expand my search to outside of Apple.&lt;/p&gt;

&lt;h2&gt;My Job Hunt&lt;/h2&gt;

&lt;p&gt;I have a strong technical background from my CMU classes, but not having an explicit Computer Science degree has made it difficult to prove aptitude despite my positive annual reviews and proven experience / technical skills at Apple. So I made the decision to blog with a technical focus here at &lt;a href=&quot;http://minimaxir.com&quot;&gt;minimaxir.com&lt;/a&gt;, which gave me an avenue to showcase my programmatic skills and the opportunity to self-learn practical new tools not covered during the school curriculum, such as &lt;a href=&quot;https://www.python.org&quot;&gt;Python&lt;/a&gt;, &lt;a href=&quot;http://ggplot2.org&quot;&gt;ggplot2&lt;/a&gt;, version control with &lt;a href=&quot;https://git-scm.com&quot;&gt;git&lt;/a&gt;, and reproducible analyses via &lt;a href=&quot;http://jupyter.org&quot;&gt;Jupyter/IPython Notebooks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This approach has been successful and many readers have liked my my blog posts: often topping &lt;a href=&quot;https://www.reddit.com/r/dataisbeautiful/comments/4bwr7o/relationship_between_rotten_tomatoes_tomatometer/&quot;&gt;Reddit&lt;/a&gt; and &lt;a href=&quot;https://news.ycombinator.com/item?id=13429656&quot;&gt;Hacker News&lt;/a&gt;, driving hundreds of thousands of pageviews. Additionally, a couple of my posts were even cited in larger publications such as the &lt;a href=&quot;https://www.washingtonpost.com/news/the-intersect/wp/2016/06/30/facebook-news-feed-and-the-tyranny-of-positive-content/&quot;&gt;Washington Post&lt;/a&gt; and &lt;a href=&quot;https://www.buzzfeed.com/tomphillips/photos-that-prove-game-of-thrones-happened-in-real-life&quot;&gt;BuzzFeed&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also published many open-source technical projects to my &lt;a href=&quot;https://github.com/minimaxir&quot;&gt;GitHub&lt;/a&gt;. My &lt;a href=&quot;https://github.com/minimaxir/big-list-of-naughty-strings&quot;&gt;Big List of Naughty Strings&lt;/a&gt;, a project I made in a couple hours on a weekend inspired by my QA-ing at work, is now at &lt;strong&gt;20,000+ Stars&lt;/strong&gt; on GitHub. My &lt;a href=&quot;https://github.com/minimaxir/facebook-page-post-scraper&quot;&gt;Facebook Page Post Scraper&lt;/a&gt;, which does what the name implies, is now at 1,000+ Stars and has been used by many other businesses and journalists.&lt;/p&gt;

&lt;p&gt;Developers have long argued that job seekers should have a strong public portfolio, as demonstrated experience can account for the lack of a relevant degree. After years of building up my portfolio, it became apparent that most outside recruiters I talked with never looked at my blog/GitHub, despite a strong emphasis of both on my r√©sum√©.&lt;/p&gt;

&lt;p&gt;I subsequently rededicated my blog as a pragmatic demonstration of relevant skills in the data analysis job market, focusing more on practical analysis instead of quirky insights and thoughts. In the process, I obtained proficiency in a number of modern tools, including &lt;a href=&quot;http://minimaxir.com/2016/08/clickbait-cluster/&quot;&gt;interactive data visualizations&lt;/a&gt; on the web with &lt;a href=&quot;https://plot.ly&quot;&gt;Plotly&lt;/a&gt;, processing &lt;a href=&quot;http://minimaxir.com/2017/01/amazon-spark/&quot;&gt;big data&lt;/a&gt; with &lt;a href=&quot;http://spark.apache.org&quot;&gt;Apache Spark&lt;/a&gt;, high-performance &lt;a href=&quot;http://minimaxir.com/2017/02/predicting-arrests/&quot;&gt;machine learning&lt;/a&gt; with &lt;a href=&quot;https://github.com/dmlc/xgboost&quot;&gt;xgboost&lt;/a&gt; and &lt;a href=&quot;https://github.com/Microsoft/LightGBM&quot;&gt;LightGBM&lt;/a&gt;, and even &lt;a href=&quot;http://minimaxir.com/2017/04/char-embeddings/&quot;&gt;deep learning&lt;/a&gt; with &lt;a href=&quot;https://github.com/fchollet/keras&quot;&gt;Keras&lt;/a&gt; and &lt;a href=&quot;https://github.com/tensorflow/tensorflow&quot;&gt;TensorFlow&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am now actively looking for a &lt;strong&gt;data analyst/software engineering job within San Francisco&lt;/strong&gt;. If you are interested or if you know of companies who are looking for qualified people, please send me an email at &lt;strong&gt;&lt;a href=&quot;mailto:max@minimaxir.com&quot;&gt;max@minimaxir.com&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;So I‚Äôll be using my time over the next couple weeks to openly look for a new job, and to network with others in relevant industries (and be able to interview without taking a day off of work). Things have been improving: my &lt;a href=&quot;https://news.ycombinator.com/item?id=14238066&quot;&gt;comment&lt;/a&gt; in the Hacker News &amp;ldquo;Who wants to be hired?&amp;rdquo; thread generated many leads who really liked my blog/portfolio. If you‚Äôd like to meet up in San Francisco and talk about tech and data stuff, just let me know.&lt;/p&gt;

&lt;p&gt;I still intend to continue blogging, not as a hobby but in a more purposeful way. I have very ambitious goals and now have more time to execute them at a deeper level. Plans include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Web applications leveraging deep learning models, deployed at scale with &lt;a href=&quot;https://www.docker.com&quot;&gt;Docker&lt;/a&gt;/&lt;a href=&quot;https://kubernetes.io&quot;&gt;Kubernetes&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Interactive data dashboards accompanying every analytical blog post with &lt;a href=&quot;https://shiny.rstudio.com&quot;&gt;Shiny&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Code screencasts at 4k resolution on &lt;a href=&quot;https://youtube.com/minimaxir&quot;&gt;YouTube&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Data analysis live-streaming with augmented functionality on &lt;a href=&quot;https://www.twitch.tv/minimaxir&quot;&gt;Twitch&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have set up a &lt;strong&gt;&lt;a href=&quot;https://www.patreon.com/minimaxir&quot;&gt;Patreon&lt;/a&gt;&lt;/strong&gt; in order to subsidize my machine learning/deep learning/software/hardware needs for my blog posts. If you have found any of my blog posts useful, a monetary contribution to my Patreon would be appreciated and will be put to good creative use.&lt;/p&gt;

&lt;p&gt;If you want to keep up with me and my projects, feel free to follow me on &lt;strong&gt;&lt;a href=&quot;https://www.facebook.com/max.woolf&quot;&gt;Facebook&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;https://twitter.com/minimaxir&quot;&gt;Twitter&lt;/a&gt;&lt;/strong&gt; too.&lt;/p&gt;
</description>
        <pubDate>Thu, 04 May 2017 09:30:00 -0700</pubDate>
        <link>http://minimaxir.com/2017/05/leaving-apple/</link>
        <guid isPermaLink="true">http://minimaxir.com/2017/05/leaving-apple/</guid>
        
        
        <category>Personal</category>
        
      </item>
    
      <item>
        <title>Pretrained Character Embeddings for Deep Learning and Automatic Text Generation</title>
        <description>&lt;p&gt;Deep learning is the biggest, &lt;a href=&quot;http://approximatelycorrect.com/2017/03/28/the-ai-misinformation-epidemic/&quot;&gt;often misapplied&lt;/a&gt; buzzword nowadays for getting pageviews on blogs. As a result, there have been a lot of shenanigans lately with deep learning thought pieces and how deep learning can solve &lt;em&gt;anything&lt;/em&gt; and make childhood sci-fi dreams come true.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m not a fan of &lt;a href=&quot;http://tvtropes.org/pmwiki/pmwiki.php/Main/ClarkesThirdLaw&quot;&gt;Clarke&amp;rsquo;s Third Law&lt;/a&gt;, so I spent some time checking out deep learning myself. As it turns out, with modern deep learning tools like &lt;a href=&quot;https://github.com/fchollet/keras&quot;&gt;Keras&lt;/a&gt;, a higher-level framework on top of the popular &lt;a href=&quot;https://www.tensorflow.org&quot;&gt;TensorFlow&lt;/a&gt; framework, deep learning is &lt;strong&gt;easy to learn and understand&lt;/strong&gt;. Yes, easy. And it &lt;em&gt;definitely&lt;/em&gt; does not require a PhD, or even a Computer Science undergraduate degree, to implement models or make decisions based on the output.&lt;/p&gt;

&lt;p&gt;However, let&amp;rsquo;s try something more expansive than the stereotypical deep learning tutorials.&lt;/p&gt;

&lt;h2&gt;Characters Welcome&lt;/h2&gt;

&lt;p&gt;Word embeddings have been a popular machine learning trick nowadays. By using an algorithm such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Word2vec&quot;&gt;Word2vec&lt;/a&gt;, you can obtain a numeric representation of a word, and use those values to create numeric representations of higher-level representations like sentences/paragraphs/documents/etc.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/char-embeddings/word-vectors.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;However, generating word vectors for datasets can be computationally expensive (see &lt;a href=&quot;http://minimaxir.com/2016/08/clickbait-cluster/&quot;&gt;my earlier post&lt;/a&gt; which uses Apache Spark/Word2vec to create sentence vectors at scale quickly). The academic way to work around this is to use pretrained word embeddings, such as &lt;a href=&quot;https://nlp.stanford.edu/projects/glove/&quot;&gt;the GloVe vectors&lt;/a&gt; collected by researchers at Stanford NLP. However, GloVe vectors are huge; the largest one (840 billion tokens at 300D) is 5.65 GB on disk and may hit issues when loaded into memory on less-powerful computers.&lt;/p&gt;

&lt;p&gt;Why not work &lt;em&gt;backwards&lt;/em&gt; and calculate &lt;em&gt;character&lt;/em&gt; embeddings? Then you could calculate a relatively few amount of vectors which would easily fit into memory, and use those to derive word vectors, which can then be used to derive the sentence/paragraph/document/etc vectors. But training character embeddings traditionally is significantly more computationally expensive since there are 5-6x the amount of tokens, and I don&amp;rsquo;t have access to the supercomputing power of Stanford researchers.&lt;/p&gt;

&lt;p&gt;Why not use the &lt;em&gt;existing&lt;/em&gt; pretrained word embeddings to extrapolate the corresponding character embeddings within the word? Think &amp;ldquo;&lt;a href=&quot;https://en.wikipedia.org/wiki/Bag-of-words_model&quot;&gt;bag-of-words&lt;/a&gt;,&amp;rdquo; except &amp;ldquo;bag-of-characters.&amp;rdquo; For example, from the embeddings from the word &amp;ldquo;the&amp;rdquo;, we can infer the embeddings for &amp;ldquo;t&amp;rdquo;, &amp;ldquo;h,&amp;rdquo; and &amp;ldquo;e&amp;rdquo; from the parent word, and average the t/h/e vectors from &lt;em&gt;all&lt;/em&gt; words/tokens in the dataset corpus. (For this post, I will only look at the 840B/300D dataset since that is the only one with capital letters, which are rather important. If you want to use a dataset with smaller dimensionality, apply &lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_component_analysis&quot;&gt;PCA&lt;/a&gt; on the final results)&lt;/p&gt;

&lt;p&gt;I wrote a &lt;a href=&quot;https://github.com/minimaxir/char-embeddings/blob/master/create_embeddings.py&quot;&gt;simple Python script&lt;/a&gt; that takes in the specified pretrained word embeddings and does just that, &lt;a href=&quot;https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt&quot;&gt;outputting the character embeddings&lt;/a&gt; in the same format. (for simplicity, only ASCII characters are included; the &lt;a href=&quot;https://en.wikipedia.org/wiki/Extended_ASCII&quot;&gt;extended ASCII characters&lt;/a&gt; are  intentionally omitted due to compatibility reasons. Additionally, by construction, space and newline characters are not represented in the derived dataset.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/char-embeddings/char-embeddings.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;You may be thinking that I&amp;rsquo;m cheating. So let&amp;rsquo;s set a point-of-reference. Colin Morris &lt;a href=&quot;http://colinmorris.github.io/blog/1b-words-char-embeddings&quot;&gt;found&lt;/a&gt; that when 16D character embeddings from a model used in Google&amp;rsquo;s &lt;a href=&quot;https://arxiv.org/abs/1312.3005&quot;&gt;One Billion Word Benchmark&lt;/a&gt; are projected into a 2D space via t-SNE, patterns emerge: digits are close, lowercase and uppercase letters are often paired, and punctuation marks are loosely paired.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/char-embeddings/tsne_embeddings.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s do that for my derived character embeddings, but with &lt;a href=&quot;https://www.r-project.org&quot;&gt;R&lt;/a&gt; and &lt;a href=&quot;http://docs.ggplot2.org/current/&quot;&gt;ggplot2&lt;/a&gt;. t-SNE is &lt;a href=&quot;http://distill.pub/2016/misread-tsne/&quot;&gt;difficult to use&lt;/a&gt; for high-dimensional vectors as combinations of parameters can result in wildly different output, so let&amp;rsquo;s try a couple projections. Here&amp;rsquo;s what happens when my pretrained projections are preprojected from 300D to 16D via &lt;a href=&quot;http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/&quot;&gt;PCA whitening&lt;/a&gt;, and setting perplexity (number of optimal neighbors) to 7.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/char-embeddings/char-tsne.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The algorithm manages to separate and group lowercase, uppercase, and numerals rather distinctly. Quadrupling the dimensionality of the preprocessing step to 64D and changing perplexity to 2 generates a depiction closer to the Google model projection:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/char-embeddings/char-tsne-2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;My pretrained character embeddings trick isn&amp;rsquo;t academic, but it&amp;rsquo;s successfully identifying realistic relationships. There might be something here worthwhile.&lt;/p&gt;

&lt;h2&gt;The Coolness of Deep Learning&lt;/h2&gt;

&lt;p&gt;Keras, maintained by Google employee &lt;a href=&quot;https://twitter.com/fchollet&quot;&gt;Fran√ßois Chollet&lt;/a&gt;, is so good that it is effectively cheating in the field of machine learning, where even TensorFlow tutorials can be replaced with a single line of code. (which is important for iteration; Keras layers are effectively Lego blocks). A simple read of the &lt;a href=&quot;https://github.com/fchollet/keras/tree/master/examples&quot;&gt;Keras examples&lt;/a&gt; and &lt;a href=&quot;https://keras.io/&quot;&gt;documentation&lt;/a&gt; will let you reverse-engineer most the revolutionary deep learning clickbait thought pieces. Some create entire startups by changing the source dataset of the Keras examples and pitch them to investors none-the-wiser, or make very light wrappers on top the examples for teaching tutorial videos and get thousands of subscribers on YouTube.&lt;/p&gt;

&lt;p&gt;I prefer to parse documentation/examples as a proof-of-concept, but never as gospel. Examples are often not the most efficient ways to implement a solution to a problem, just merely a start. In the case of Keras&amp;rsquo;s &lt;a href=&quot;https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py&quot;&gt;text generator example&lt;/a&gt;, the initial code was likely modeled after the 2015 blog post &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt; by Andrej Karpathy and the corresponding project &lt;a href=&quot;https://github.com/karpathy/char-rnn&quot;&gt;char-rnn&lt;/a&gt;. There have been many new developments in neural network architecture since 2015 that can improve both speed and performance of the text generation model as a whole.&lt;/p&gt;

&lt;h2&gt;What Text to Generate?&lt;/h2&gt;

&lt;p&gt;The Keras example uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Friedrich_Nietzsche&quot;&gt;Nietzsche&lt;/a&gt; writings as a data source, which I&amp;rsquo;m not fond of because it&amp;rsquo;s difficult to differentiate bad autogenerated Nietzsche rants from actual Nietzsche rants. What I want to generate is text with &lt;em&gt;rules&lt;/em&gt;, with the algorithm being judged by how well it follows an inherent structure. My idea is to create &lt;a href=&quot;http://magic.wizards.com/en&quot;&gt;Magic: The Gathering&lt;/a&gt; cards.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/char-embeddings/dragon-whelp.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Inspired by the &lt;a href=&quot;https://twitter.com/RoboRosewater&quot;&gt;@RoboRosewater&lt;/a&gt; Twitter account by Reed Milewicz and the &lt;a href=&quot;http://www.mtgsalvation.com/forums/creativity/custom-card-creation/612057-generating-magic-cards-using-deep-recurrent-neural&quot;&gt;corresponding research&lt;/a&gt; and &lt;a href=&quot;https://motherboard.vice.com/en_us/article/the-ai-that-learned-magic-the-gathering&quot;&gt;articles&lt;/a&gt;, I aim to see if it&amp;rsquo;s possible to recreate the structured design creativity for myself.&lt;/p&gt;

&lt;p&gt;Even if you are not familiar with Magic and its rules, you can still find the &lt;a href=&quot;https://twitter.com/RoboRosewater/status/756198572282949632&quot;&gt;card text&lt;/a&gt; of RoboRosewater cards hilarious:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/char-embeddings/horse.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Occasionally RoboRosewater, using a weaker model, produces amusing &lt;a href=&quot;https://twitter.com/RoboRosewater/status/689184317721960448&quot;&gt;neural network trainwrecks&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/char-embeddings/carl.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;More importantly, all Magic cards have an explicit structure; they have a name, mana cost in the upper-right, card type, card text, and usually a power and toughness in the bottom-right.&lt;/p&gt;

&lt;p&gt;I wrote &lt;a href=&quot;https://github.com/minimaxir/char-embeddings/blob/master/create_magic_text.py&quot;&gt;another Python script&lt;/a&gt; to parse all Magic card data from &lt;a href=&quot;https://mtgjson.com&quot;&gt;MTG JSON&lt;/a&gt; into an encoding which matches this architecture, where each section transition has its own symbol delimiter, along with other encoding simplicities. For example, here is the card &lt;a href=&quot;http://gatherer.wizards.com/Pages/Card/Details.aspx?multiverseid=247314&quot;&gt;Dragon Whelp&lt;/a&gt; in my encoding:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;[Dragon Whelp@{2}{R}{R}#Creature ‚Äî Dragon$Flying|{R}: ~ gets +1/+0 until end of turn. If this ability has been activated four or more times this turn, sacrifice ~ at the beginning of the next end step.%2^3]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;These card encodings are all combined into one .txt file, which will be fed into the model.&lt;/p&gt;

&lt;h2&gt;Building and Training the Model&lt;/h2&gt;

&lt;p&gt;The Keras text generation example operates by breaking a given .txt file into 40-character sequences, and the model tries to predict the 41st character by outputting a probability for each possible character (108 in this dataset). For example, if the input based on the above example is &lt;code&gt;[&amp;#39;D&amp;#39;, &amp;#39;r&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;g&amp;#39;, ..., &amp;#39;D&amp;#39;, &amp;#39;r&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;g&amp;#39;]&lt;/code&gt; (with the latter Drag being part of the creature type), the model will optimize for outputting a probability of 1.0 of &lt;code&gt;o&lt;/code&gt;; per the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression&quot;&gt;categorical crossentropy&lt;/a&gt; loss function, the model is rewarded for assigning correct guesses with 1.0 probability and incorrect guesses with 0.0 probabilities, penalizing half-guesses and wrong guesses.&lt;/p&gt;

&lt;p&gt;Each possible 40-character sequence is collected, however only every other third sequence is kept; this prevents the model from being able to learn card text verbatim, plus it also makes training faster. (for this model, there are about &lt;strong&gt;1 million&lt;/strong&gt; sequences for the final training). The example uses only a 128-node &lt;a href=&quot;https://en.wikipedia.org/wiki/Long_short-term_memory&quot;&gt;long-short-term-memory&lt;/a&gt; (LSTM) &lt;a href=&quot;https://en.wikipedia.org/wiki/Recurrent_neural_network&quot;&gt;recurrent neural network&lt;/a&gt; (RNN) layer, popular for incorporating a &amp;ldquo;memory&amp;rdquo; into a neural network model, but the example notes at the beginning it can take awhile to train before generated text is coherent.&lt;/p&gt;

&lt;p&gt;There are a few optimizations we can make. Instead of supplying the characters directly to the RNN, we can first encode them using an &lt;a href=&quot;https://keras.io/layers/embeddings/&quot;&gt;Embedding layer&lt;/a&gt; so the model can train character context. We can stack more layers on the RNN by adding a 2-level &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;&gt;multilayer perceptron&lt;/a&gt;: a &lt;a href=&quot;https://www.reddit.com/r/ProgrammerHumor/comments/5si1f0/machine_learning_approaches/&quot;&gt;meme&lt;/a&gt;, yes, but it helps, as the network must learn latent representations of the data. Thanks to recent developments such as &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;batch normalization&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;&gt;rectified linear activations&lt;/a&gt; for these &lt;a href=&quot;https://keras.io/layers/core/#dense&quot;&gt;Dense layers&lt;/a&gt;, they can both be trained without as much computational overhead, and thanks to Keras, both can be added to a layer with a single line of code each. Lastly, we can add an auxiliary output via Keras&amp;rsquo;s &lt;a href=&quot;https://keras.io/models/model/&quot;&gt;functional API&lt;/a&gt; where the network makes a prediction based on only the output from the RNN in addition to the main output, which forces it to work smarter and ends up resulting in a &lt;em&gt;significant&lt;/em&gt; improvement in loss for the main path.&lt;/p&gt;

&lt;p&gt;The final architecture ends up looking like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/char-embeddings/model.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;And because we added an Embedding layer, we can load the pretrained 300D character embeds I made earlier, giving the model a good start in understanding character relationships.&lt;/p&gt;

&lt;p&gt;The goal of the training is to minimize the total loss of the model. (but for evaluating model performance, we only look at the loss of the main output). The model is trained in &lt;strong&gt;epochs&lt;/strong&gt;, where the model sees all the input data atleast once. During each epoch, batches of size 128 are loaded into the model and evaluated, calculating a &lt;strong&gt;batch loss&lt;/strong&gt; for each; the gradients from the batch are backpropagated into the previous layers to improve them. While training with Keras, the console reports an &lt;strong&gt;epoch loss&lt;/strong&gt;, which is the average of all the batch losses so far in the current epoch, allowing the user to see in real time how the model improves, and it&amp;rsquo;s addicting.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/char-embeddings/keras-training.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Keras/TensorFlow works just fine on the CPU, but for models with a RNN, you&amp;rsquo;ll want to consider using a GPU for performance, specifically one by nVidia. Amazon has cloud GPU instances for $0.90/hr (&lt;a href=&quot;http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html&quot;&gt;not prorated&lt;/a&gt;), but very recently, Google announced &lt;a href=&quot;https://cloud.google.com/compute/docs/gpus/add-gpus&quot;&gt;GPU instances&lt;/a&gt; of the same caliber for ~$0.75/hr (prorated to the minute), which is what I used to train this model, although Google Compute Engine requires configuring the GPU drivers first. For 20 epochs, it took about 4 hours and 20 minutes to train the model while spending $3.26, which isn&amp;rsquo;t bad as far as deep learning goes.&lt;/p&gt;

&lt;h2&gt;Making Magic&lt;/h2&gt;

&lt;p&gt;After each epoch, the original Keras text generation example takes a sentence from the input data as a seed and predicts the next character in the sequence according to the model, then uses the last 40 characters generated for the next character, etc. The sampling incorporates a diversity/temperature parameter which allows the model to make suboptimal decisions and select characters with lower natural probabilities, which allows for the romantic &amp;ldquo;creativity&amp;rdquo; popular with neural network text generation.&lt;/p&gt;

&lt;p&gt;With the Magic card dataset and my tweaked model architecture, generated text is coherent &lt;a href=&quot;https://github.com/minimaxir/char-embeddings/blob/master/output/iter-01-0_9204.txt&quot;&gt;after the 1st epoch&lt;/a&gt;! After about 20 epochs, training becomes super slow, but the predicted text becomes super interesting. Here are a few fun examples from a &lt;a href=&quot;https://github.com/minimaxir/char-embeddings/blob/master/output/text_sample.txt&quot;&gt;list of hundreds of generated cards&lt;/a&gt;. (Note: the power/toughness values at the end of the card have issues; more on that later).&lt;/p&gt;

&lt;p&gt;With low diversity, the neural network generated cards that are oddly biased toward card names which include the letter &amp;ldquo;S&amp;rdquo;. The card text also conforms to the rules of the game very well.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;[Reality Spider@{3}{G}#Creature ‚Äî Elf Warrior$Whenever ~ deals combat damage to a player, put a +1/+1 counter on it.%^]
[Dark Soul@{2}{R}#Instant$~ deals 2 damage to each creature without flying.%^]
[Standing Stand@{2}{G}#Creature ‚Äî Elf Shaman${1}{G}, {T}: Draw a card, then discard a card.%^]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In contrast, cards generated with high diversity hit the uncanny valley of coherence and incoherence in both text and game mechanic abuse, which is what makes them interesting. &lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;[Portrenline@{2}{R}#Sorcery$As an additional cost to cast ~, exile ~.%^]
[Clocidian Lorid@{W}{W}{W}#Instant$Regenerate each creature with flying and each player.%^]
[Icomic Convermant@{3}{G}#Sorcery$Search your library for a land card in your graveyard.%1^1]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The best-of-both-worlds cards are generated from diversity parameters between both extremes, and often have funny names.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;[Seal Charm@{W}{W}#Instant$Exile target creature. Its controller loses 1 life.%^]
[Shambling Assemblaster@{4}{W}#Creature ‚Äî Human Cleric$When ~ enters the battlefield, destroy target nonblack creature.%1^1]
[Lightning Strength@{3}{R}#Enchantment ‚Äî Aura$Enchant creature|Enchanted creature gets +3/+3 and has flying, flying, trample, trample, lifelink, protection from black and votile all damage unless you return that card to its owner&amp;#39;s hand.%2^2]
[Skysor of Shadows@{7}{B}{B}{B}#Enchantment$As ~ enters the battlefield, choose one ‚Äî|‚Ä¢ Put a -1/-1 counter on target creature.%2^2]
[Glinding Stadiers@{4}{W}#Creature ‚Äî Spirit$Protection from no creatures can&amp;#39;t attack.%^]
[Dragon Gault@{3}{G}{U}{U}#Creature ‚Äî Kraven$~&amp;#39;s power and toughness are 2.%2^2]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;All Keras/Python code used in this blog post, along with sample Magic card output and the trained model itself, is available open-source &lt;a href=&quot;https://github.com/minimaxir/char-embeddings&quot;&gt;in this GitHub repository&lt;/a&gt;. The repo additionally contains &lt;a href=&quot;https://github.com/minimaxir/char-embeddings/blob/master/text_generator_keras_sample.py&quot;&gt;a Python script&lt;/a&gt; which lets you generate new cards using the model, too!&lt;/p&gt;

&lt;h2&gt;Visualizing Model Performance&lt;/h2&gt;

&lt;p&gt;One thing deep learning tutorials rarely mention is &lt;em&gt;how&lt;/em&gt; to collect the loss data and visualize the change in loss over time. Thanks to Keras&amp;rsquo;s &lt;a href=&quot;https://keras.io/callbacks/&quot;&gt;utility functions&lt;/a&gt;, I wrote a custom model callback which collects the batch losses and epoch losses and writes them to a CSV file.&lt;/p&gt;

&lt;p&gt;Using R and ggplot2, I can plot the batch loss at every 50th batch to visualize how the model converges over time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/char-embeddings/batch-losses.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;After 20 epochs, the model loss ends up at about &lt;strong&gt;0.30&lt;/strong&gt; which is more-than-low-enough for coherent text. As you can see, there are large diminishing returns after a few epochs, which is the hard part of training deep learning models.&lt;/p&gt;

&lt;p&gt;Plotting the epoch loss over the batches makes the trend more clear.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/char-embeddings/epoch-losses.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;In order to prevent early convergence, we can make the model more complex (i.e. stack more layers unironically), but that has trade-offs, both in training &lt;em&gt;and&lt;/em&gt; predictive speed, the latter of which is important if using deep learning in a production application.&lt;/p&gt;

&lt;p&gt;Lastly, as with the Google One Billion Words benchmark, we can extract the &lt;a href=&quot;https://github.com/minimaxir/char-embeddings/blob/master/output/char-embeddings.txt&quot;&gt;trained character embeddings&lt;/a&gt; from the model (now augmented with Magic card context!) and plot them again to see what has changed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/char-embeddings/char-tsne-embed.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;There are more pairs of uppercase/lowercase characters, although  interestingly there isn&amp;rsquo;t much grouping with the special characters added as section breaks in the encoding, or mechanical uppercase characters such as W/U/B/R/G/C/T.&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;After building the model, I did a little more research to see if others solved the power/toughness problem. Since the sentences are only 40 characters and Magic cards are much longer than 40 characters, it&amp;rsquo;s likely that power/toughness are out-of-scope for the model and it cannot learn their exact values. Turns out that the intended solution is to use a &lt;a href=&quot;https://github.com/billzorn/mtgencode&quot;&gt;completely different encoding&lt;/a&gt;, such as this one for Dragon Whelp:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;|5creature|4|6dragon|7|8&amp;amp;^^/&amp;amp;^^^|9flying\{RR}: @ gets +&amp;amp;^/+&amp;amp; until end of turn. if this ability has been activated four or more times this turn, sacrifice @ at the beginning of the next end step.|3{^^RRRR}|0N|1dragon whelp|
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Power/toughness are generated near the &lt;em&gt;beginning&lt;/em&gt; of the card. Sections are delimited by pipes, with a numeral designating the corresponding section. Instead of numerals being used card values, carets are used, which provides a more accurate &lt;em&gt;quantification&lt;/em&gt; of values. With this encoding, each character has a &lt;em&gt;singular purpose&lt;/em&gt; in the global card context, and their embeddings would likely generate more informative visualizations. (But as a consequence, the generated cards are harder to parse at a glance).&lt;/p&gt;

&lt;p&gt;The secondary encoding highlights a potential flaw in my methodology using pretrained character embeddings. Trained machine learning models must be used apples-to-apples on similar datasets; for example, you can&amp;rsquo;t accurately perform Twitter &lt;a href=&quot;https://en.wikipedia.org/wiki/Sentiment_analysis&quot;&gt;sentiment analysis&lt;/a&gt; on a dataset using a model trained on professional movie reviews since Tweets do not follow &lt;a href=&quot;https://owl.english.purdue.edu/owl/resource/735/02/&quot;&gt;AP Style&lt;/a&gt; guidelines. In my case, the &lt;a href=&quot;http://commoncrawl.org&quot;&gt;Common Crawl&lt;/a&gt;, the source of the pretrained embeddings, follows more natural text usage and would not work analogously with the atypical character usages in &lt;em&gt;either&lt;/em&gt; of the Magic card encodings.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s still a &lt;em&gt;lot&lt;/em&gt; of work to be done in terms of working with both pretrained character embeddings and improving Magic card generation, but I believe there is promise. The better way to make character embeddings than my script is to do it the hard way and train then manually, maybe even at a higher dimensionality like 500D or 1000D. Likewise, for Magic model building, the &lt;a href=&quot;https://github.com/billzorn/mtgencode#training-a-neural-net&quot;&gt;mtg-rnn instructions&lt;/a&gt; repo uses a large LSTM stacked on a LSTM along with 120/200-character sentences, both of which combined make training &lt;strong&gt;VERY&lt;/strong&gt; slow (notably, this was the architecture of the &lt;a href=&quot;https://github.com/fchollet/keras/commit/d2b229df2ea0bab712379c418115bc44508bc6f9#diff-904d72bcf9fa38b32f9c1f868ff59367&quot;&gt;very first commit&lt;/a&gt; for the Keras text generation example, and &lt;a href=&quot;https://github.com/fchollet/keras/commit/01d5e7bc4782daafcfa99e035c1bdbe13a985145&quot;&gt;was changed&lt;/a&gt; to the easily-trainable architecture). There is also promise in a &lt;a href=&quot;http://kvfrans.com/variational-autoencoders-explained/&quot;&gt;variational autoencoder&lt;/a&gt; approach, such as with &lt;a href=&quot;https://arxiv.org/abs/1702.02390&quot;&gt;textvae&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This work is potentially very expensive and I am strongly considering setting up a &lt;a href=&quot;https://www.patreon.com&quot;&gt;Patreon&lt;/a&gt; in lieu of excess venture capital to subsidize my machine learning/deep learning tasks in the future.&lt;/p&gt;

&lt;p&gt;At minimum, working with this example gave me a sufficient application of practical work with Keras, and another tool in my toolbox for data analysis and visualization. Keras makes the model-construction aspect of deep learning trivial and not scary. Hopefully, this article justifies the use of the &amp;ldquo;deep learning&amp;rdquo; buzzword in the headline.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s also worth mentioning that I actually started working on automatic text generation 6 months ago using a different, non-deep-learning approach, but hit a snag and abandoned that project. With my work on Keras, I found a way around that snag, and on the same Magic dataset with the same input construction, I obtained a model loss of &lt;strong&gt;0.03&lt;/strong&gt; at &lt;strong&gt;20% of the cloud computing cost&lt;/strong&gt; in about the same amount of time. More on that later.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;The code for generating the R/ggplot2 data visualizations is available in this &lt;a href=&quot;http://minimaxir.com/notebooks/char-tsne/&quot;&gt;R Notebook&lt;/a&gt;, and open-sourced in &lt;a href=&quot;https://github.com/minimaxir/char-tsne-visualization&quot;&gt;this GitHub Repository.&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You are free to use the automatic text generation scripts and data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 04 Apr 2017 06:30:00 -0700</pubDate>
        <link>http://minimaxir.com/2017/04/char-embeddings/</link>
        <guid isPermaLink="true">http://minimaxir.com/2017/04/char-embeddings/</guid>
        
        
        <category>Visualization</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Predicting And Mapping Arrest Types in San Francisco with LightGBM, R, ggplot2</title>
        <description>&lt;p&gt;The new hotness in the world of data science is &lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_neural_network&quot;&gt;neural networks&lt;/a&gt;, which form the basis of &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;&gt;deep learning&lt;/a&gt;. But while everyone is obsessing about neural networks and how deep learning is &lt;em&gt;magic&lt;/em&gt; and can solve &lt;em&gt;any&lt;/em&gt; problem if you just &lt;a href=&quot;https://www.reddit.com/r/ProgrammerHumor/comments/5si1f0/machine_learning_approaches/&quot;&gt;stack enough layers&lt;/a&gt;, there have been many recent developments in the relatively nonmagical world of machine learning with &lt;em&gt;boring&lt;/em&gt; CPUs.&lt;/p&gt;

&lt;p&gt;Years before neural networks were the Swiss army knife of data science, there were &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_boosting&quot;&gt;gradient-boosted machines&lt;/a&gt;/&lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting&quot;&gt;gradient-boosted trees&lt;/a&gt;. GBMs/GBTs are machine learning methods which are effective on many types of data, and do not require the &lt;a href=&quot;http://r-statistics.co/Assumptions-of-Linear-Regression.html&quot;&gt;traditional model assumptions&lt;/a&gt; of linear/logistic regression models. Wikipedia has a good article on the advantages of &lt;a href=&quot;https://en.wikipedia.org/wiki/Decision_tree_learning&quot;&gt;decision tree learning&lt;/a&gt;, and visual diagrams of the architecture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/predicting-arrests/CART_tree_titanic_survivors.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;GBMs, as &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html&quot;&gt;implemented&lt;/a&gt; in the Python package &lt;a href=&quot;http://scikit-learn.org/stable/&quot;&gt;scikit-learn&lt;/a&gt;, are extremely popular in &lt;a href=&quot;https://www.kaggle.com&quot;&gt;Kaggle&lt;/a&gt; machine learning competitions. But scikit-learn is relatively old, and new technologies have emerged which implement GBMs/GBTs on large datasets with massive parallelization and and in-memory computation. A popular big data machine learning library, &lt;a href=&quot;http://www.h2o.ai&quot;&gt;H2O&lt;/a&gt;, has a &lt;a href=&quot;http://docs.h2o.ai/h2o-tutorials/latest-stable/tutorials/gbm-randomforest/index.html&quot;&gt;famous GBM implementation&lt;/a&gt; which, &lt;a href=&quot;https://github.com/szilard/benchm-ml&quot;&gt;per benchmarks&lt;/a&gt;, is over 10x faster than scikit-learn and is optimized for datasets with millions of records. But even &lt;em&gt;faster&lt;/em&gt; than H2O is &lt;a href=&quot;https://github.com/dmlc/xgboost&quot;&gt;xgboost&lt;/a&gt;, which can hit a 5x-10x speed-ups relative to H2O, depending on the dataset size.&lt;/p&gt;

&lt;p&gt;Enter &lt;a href=&quot;https://github.com/Microsoft/LightGBM&quot;&gt;LightGBM&lt;/a&gt;, a new (October 2016) open-source machine learning framework by &lt;a href=&quot;https://www.microsoft.com/en-us/&quot;&gt;Microsoft&lt;/a&gt; which, per &lt;a href=&quot;https://github.com/Microsoft/LightGBM/issues/211&quot;&gt;benchmarks&lt;/a&gt; on release, was up to &lt;em&gt;4x faster&lt;/em&gt; than xgboost! (xgboost very recently implemented a &lt;a href=&quot;https://github.com/dmlc/xgboost/issues/1950&quot;&gt;technique&lt;/a&gt; also used in LightGBM, which reduced the relative speedup to just ~2x). As a result, LightGBM allows for very efficient model building on large datasets without requiring cloud computing or nVidia CUDA GPUs.&lt;/p&gt;

&lt;p&gt;A year ago, I &lt;a href=&quot;http://minimaxir.com/2015/12/sf-arrests/&quot;&gt;wrote an analysis&lt;/a&gt; of the types of police arrests in San Francisco, using data from the &lt;a href=&quot;https://data.sfgov.org&quot;&gt;SF OpenData&lt;/a&gt; initiative, with a &lt;a href=&quot;http://minimaxir.com/2015/12/sf-arrest-maps/&quot;&gt;followup article&lt;/a&gt; analyzing the locations of these arrests. Months later, the same source dataset was used &lt;a href=&quot;https://www.kaggle.com/c/sf-crime&quot;&gt;for a Kaggle competition&lt;/a&gt;. Why not give the dataset another look and test LightGBM out?&lt;/p&gt;

&lt;h2&gt;Playing With The Data&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;(You can view the R code used to process the data and generate the data visualizations in &lt;a href=&quot;http://minimaxir.com/notebooks/predicting-arrests/&quot;&gt;this R Notebook&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://data.sfgov.org/Public-Safety/SFPD-Incidents-from-1-January-2003/tmnf-yvry&quot;&gt;SFPD Incidents&lt;/a&gt; dataset includes crime incidents in San Francisco from 1/1/2003 to 1/17/2017 (at time of analysis). Filtering the dataset only on incidents which resulted in arrests (since most incidents are trivial) leaves a dataset of 634,299 arrests total. The dataset also includes information on the type of crime, the location where the arrest occurred, and the date/time. There are 39 different types of arrests in the &lt;strong&gt;Category&lt;/strong&gt; column such as Assault, Burglary, and Prostitution, which serves as the response variable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/predicting-arrests/data.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Meanwhile, we can engineer features from the location and date/time. 
Performing an exploratory data analysis of both is helpful to determine at a glance which features may be relevant (fortunately, I did that a year ago).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sf-arrests/sf-arrest-when-4.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The location is given as latitude/longitude coordinates, so we can select a longitude &lt;strong&gt;X&lt;/strong&gt; and latitude &lt;strong&gt;Y&lt;/strong&gt; as features. Date/Time can be deconstructed further. We can extract the &lt;strong&gt;hour&lt;/strong&gt; in which a given arrest occurred as a feature (hour can take 24 different values from 0 ‚Äî 23). Likewise, we can extract the &lt;strong&gt;month&lt;/strong&gt; in a similar manner (12 values, from 1 ‚Äî 12). The &lt;strong&gt;year&lt;/strong&gt; the crime occurred can be extracted without special encoding. (2003 ‚Äî 2017). It is always helpful to include a year feature in predictive models to help account for change over time. The &lt;strong&gt;DayOfWeek&lt;/strong&gt; is important, but encoding it as a numeric value is tricker; we logically encode each day of the week from 1 ‚Äî 7, but which day should be #1? Making Monday #1 and Sunday #7 is the most logical, since a decision tree rule that sets a threshold on DayOfWeek values &amp;gt; 5 will translate logically to a weekend.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/predicting-arrests/predict_matrix.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s six features total. There are more features which could be helpful, but let&amp;rsquo;s check a baseline model as a start.&lt;/p&gt;

&lt;h2&gt;Modeling&lt;/h2&gt;

&lt;p&gt;Specifically, the model will predict the answer the question: &lt;em&gt;given that a San Francisco police arrest occurs at a specified time and place, what is the reason for that arrest?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;For this post, I will use the &lt;a href=&quot;https://github.com/Microsoft/LightGBM/tree/master/R-package&quot;&gt;R package&lt;/a&gt; for LightGBM (which was beta-released in January 2017; it&amp;rsquo;s &lt;em&gt;extremely&lt;/em&gt; cutting edge!) We split the dataset 70%/30% into a training set of 444,011 arrests and a test set of 190,288 arrests (due to the large amount of different category labels, the split must be &lt;a href=&quot;https://en.wikipedia.org/wiki/Stratified_sampling&quot;&gt;stratified&lt;/a&gt; to ensure the training and test sets have a balanced distribution of labels; in R, this can be implemented with the &lt;code&gt;caret&lt;/code&gt; package and &lt;code&gt;createDataPartition&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;LightGBM trains the model on the training set and evaluates it on the test set to minimize the &lt;a href=&quot;https://www.kaggle.com/c/sf-crime#evaluation&quot;&gt;multiclass logarithmic loss&lt;/a&gt; of the model. For now, I use the &lt;a href=&quot;https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.md&quot;&gt;default parameters&lt;/a&gt; of LightGBM, except to massively increase the number of iterations of the training algorithm, and to stop training the model early if the model stops improving. After about 4 minutes on my laptop (which is very fast for a dataset of this size!), the model returns a multilogloss of &lt;strong&gt;1.98&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;That number sounds arbitrary. Is it good or bad? Let&amp;rsquo;s compare it to the multilogloss from the &lt;a href=&quot;https://www.kaggle.com/c/sf-crime/leaderboard&quot;&gt;top models&lt;/a&gt; from the Kaggle version of the dataset, where a lower score is better:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/predicting-arrests/kaggle.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;okay, 1.98 &lt;em&gt;is&lt;/em&gt; a good score, and without spending much time adding features to the model and &lt;a href=&quot;https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-tuning.md&quot;&gt;tuning parameters&lt;/a&gt;! To be fair, my methodology would not necessarily result in the same score on the Kaggle dataset, but it confirms that the LightGBM model is in the top tier of models available for this problem and dataset context. And it didn&amp;rsquo;t &lt;a href=&quot;https://www.kaggle.com/smerity/sf-crime/fighting-crime-with-keras/output&quot;&gt;require any neural networks&lt;/a&gt; either!&lt;/p&gt;

&lt;p&gt;There are areas for improvement in feature engineering which &lt;a href=&quot;https://www.kaggle.com/c/sf-crime/kernels&quot;&gt;other entries&lt;/a&gt; in the Kaggle competition implemented, such as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Dummy_variable_(statistics)&quot;&gt;dummy variable&lt;/a&gt; indicating whether the offense occurred at an intersection and which SF police station was involved in the arrest. We could also encode features such as hour and DayOfWeek as categorical features (LightGBM conveniently allows this without requiring &lt;a href=&quot;https://en.wikipedia.org/wiki/One-hot&quot;&gt;one-hot encoding&lt;/a&gt; the features) instead of numeric, but in my brief testing, it made the model &lt;em&gt;worse&lt;/em&gt;, interestingly.&lt;/p&gt;

&lt;h2&gt;Analyzing the LightGBM Model&lt;/h2&gt;

&lt;p&gt;Another perk of not using a neural network for statistical model building is the ability to learn more about the importance of features in a model, as opposed to it being a &lt;a href=&quot;https://en.wikipedia.org/wiki/Black_box&quot;&gt;black box&lt;/a&gt;. In the case of gradient boosting, we can calculate the proportional contribution of each feature to the total &lt;a href=&quot;https://en.wikipedia.org/wiki/Information_gain_in_decision_trees&quot;&gt;information gain&lt;/a&gt; of the model, which will help identify the most important features, and potentially unhelpful features:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/predicting-arrests/imp.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Unsurprisingly, location features are the most important, with both location-based features establishing 70% of the total Gain in the model. But no feature is completely insignificant, which is a good thing.&lt;/p&gt;

&lt;p&gt;Back to the multilogloss of 1.98. What does that mean in the real world? What is the &lt;em&gt;accuracy&lt;/em&gt; of the model? We run each of the 190,288 arrests in the test set against the model, which returns 39 probability values for each record: one for each possible category of arrest. The category with the highest probability becomes the &lt;strong&gt;predicted&lt;/strong&gt; type of arrest.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/predicting-arrests/predicted_results.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The accuracy of the model on the test set, which is the proportion of predictions where the predicted category value matches the &lt;strong&gt;actual&lt;/strong&gt; category value, is &lt;strong&gt;39.7%&lt;/strong&gt;, with a 95% confidence interval for the true accuracy between 39.5% and 39.9%. That seems low! However, there is catch-all &amp;ldquo;Other Offenses&amp;rdquo; category for an arrest; if you predicted a &amp;ldquo;Other Offenses&amp;rdquo; label for all the test-set values, you would get an accuracy of &lt;em&gt;31.1%&lt;/em&gt;, which serves as the No Information Rate (since it would be the highest accuracy approach if there was no information at all). A 8.6 percentage point improvement is still an improvement though; many industries would &lt;em&gt;love&lt;/em&gt; an 8.6 percentage point increase in accuracy, but for this context obviously it&amp;rsquo;s not enough to usher in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Minority_Report_(film)&quot;&gt;Minority Report&lt;/a&gt;/&lt;a href=&quot;https://en.wikipedia.org/wiki/Person_of_Interest_(TV_series)&quot;&gt;Person of Interest&lt;/a&gt; future.&lt;/p&gt;

&lt;p&gt;We can visualize the classifications on the test set by the model using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Confusion_matrix&quot;&gt;confusion matrix&lt;/a&gt;; &lt;code&gt;caret&lt;/code&gt; has a simple &lt;code&gt;confusionMatrix()&lt;/code&gt; function, and ggplot2 has a &lt;code&gt;geom_tile()&lt;/code&gt; to map out the relationships, even with 39 classes. We can also annotate the tiles where actual label = predicted label by drawing a &lt;code&gt;geom_point()&lt;/code&gt; on top. Putting it all together:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/predicting-arrests/confusionMatrix.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;There is, indeed, a large amount of confusion. Many of the labels are mispredicted as Other Offenses. Specifically, the model frequently confuses the combinations of Assault, Drug/Narcotics, Larceny/Theft, and Warrants, suggesting that they also may be catch-alls.&lt;/p&gt;

&lt;p&gt;In theory, the predicted probabilities from the model between similar types of crime should also be similar, which may be causing these mispredictions. We can calculate the &lt;a href=&quot;https://en.wikipedia.org/wiki/Pearson_correlation_coefficient&quot;&gt;Pearson correlations&lt;/a&gt; between the predicted probabilities, and use &lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;hierarchical clustering&lt;/a&gt; to &lt;a href=&quot;http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization&quot;&gt;arrange and plot the correlations&lt;/a&gt; and their labels in a logical order. The majority of the correlations between labels are between 0 and +/- 0.5 (weak to moderate), but their arrangement tells a different story:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/predicting-arrests/correlationMatrix.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;From top to bottom, you can see that there is a grouping of more blue-collar, physical crimes types (Assault, Vandalism), then a grouping of less-physical, white-collar crime types (Bribery, Extortion), and then a smaller grouping of seedier crime types (Liquor Laws, Prostitution).&lt;/p&gt;

&lt;p&gt;The visualization doesn&amp;rsquo;t necessarily provide more information about the confusion matrix and the mispredictions, but &lt;em&gt;it looks cool&lt;/em&gt;, which is enough.&lt;/p&gt;

&lt;h2&gt;Mapping the Predicted Types of Arrests&lt;/h2&gt;

&lt;p&gt;Kaggle competitions emphasize model creation, but don&amp;rsquo;t discuss how to implement and execute models in practice. Since we can predict the type of crime based on the given location and date/time of an arrest, we can map boundaries of the mostly likely type of offense. Using &lt;code&gt;ggmap&lt;/code&gt; to get a map of San Francisco, splitting San Francisco into tens of thousands of points, and predicting the most-likely type of arrest at the location with a given date/time.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say we want to predict the types of crime in the future, on April 15th, 2017, during 8 PM. We construct a dataset of those points and the same date/time features used to generate the model originally. Then run those fabricated points through the model again to get new predicted labels (Additionally, we need to remove &amp;ldquo;Other Offenses&amp;rdquo; predicted labels since they cloud up the map). Plotting each point as a &lt;code&gt;geom_tile&lt;/code&gt; will interpolate regions around the city. Putting it all together:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/predicting-arrests/crime-2017-04-15-20.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Not too shabby. But that&amp;rsquo;s not all; we can &lt;em&gt;animate&lt;/em&gt; this map over a day by incrementing the hour, generating a map for each hour (while keeping the colors corresponding to the arrest type consistent), and then &lt;a href=&quot;https://github.com/minimaxir/frames-to-gif-osx&quot;&gt;stitching the maps together&lt;/a&gt; into a GIF. Let&amp;rsquo;s do March 14th, 2017 (&lt;a href=&quot;https://en.wikipedia.org/wiki/Pi_Day&quot;&gt;Pi Day&lt;/a&gt; can be dangerous!) starting at 6 AM:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/predicting-arrests/map_ani.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Wow!&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I deliberately avoided using the term &amp;ldquo;machine learning&amp;rdquo; in the headline of this post because it has been overused to the point of clickbait. Indeed, neural networks/deep learning excel at processing higher-dimensional data such as text, image, and voice data, but in cases where dataset features are &lt;a href=&quot;https://news.ycombinator.com/item?id=13563892&quot;&gt;simple and known&lt;/a&gt;, neural networks are not necessarily the most &lt;em&gt;pragmatic&lt;/em&gt; option. CPU/RAM machine learning libraries like LightGBM are still worthwhile, despite the religious fervor for deep learning.&lt;/p&gt;

&lt;p&gt;And there&amp;rsquo;s still a lot of work that can be done with the SF Crime Incidents dataset. The model only predicts the type of crime given an arrest occurred; it does not predict &lt;em&gt;if&lt;/em&gt; an arrest will occur at a given time and place, which would make a fun project for the future!&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;You can view all the R and ggplot2 code used to visualize the San Francisco crime data in &lt;a href=&quot;http://minimaxir.com/notebooks/predicting-arrests/&quot;&gt;this R Notebook&lt;/a&gt;. You can also view the images/data used for this post in &lt;a href=&quot;https://github.com/minimaxir/sf-arrests-predict&quot;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 08 Feb 2017 06:30:00 -0800</pubDate>
        <link>http://minimaxir.com/2017/02/predicting-arrests/</link>
        <guid isPermaLink="true">http://minimaxir.com/2017/02/predicting-arrests/</guid>
        
        
        <category>Visualization</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Playing with 80 Million Amazon Product Review Ratings Using Apache Spark</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://www.amazon.com&quot;&gt;Amazon&lt;/a&gt; product reviews and ratings are a very important business. Customers on Amazon often make purchasing decisions based on those reviews, and a single bad review can cause a potential purchaser to reconsider. A couple years ago, I wrote a blog post titled &lt;a href=&quot;http://minimaxir.com/2014/06/reviewing-reviews/&quot;&gt;A Statistical Analysis of 1.2 Million Amazon Reviews&lt;/a&gt;, which was well-received.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/amazon/amzn-basic-score.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Back then, I was only limited to 1.2M reviews because attempting to process more data caused out-of-memory issues and my R code took &lt;em&gt;hours&lt;/em&gt; to run.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://spark.apache.org&quot;&gt;Apache Spark&lt;/a&gt;, which makes processing gigantic amounts of data efficient and sensible, has become very popular in the past couple years (for good tutorials on using Spark with Python, I recommend the &lt;a href=&quot;https://courses.edx.org/courses/course-v1:BerkeleyX+CS105x+1T2016/info&quot;&gt;free&lt;/a&gt; &lt;a href=&quot;https://courses.edx.org/courses/course-v1:BerkeleyX+CS110x+2T2016/info&quot;&gt;eDX&lt;/a&gt; &lt;a href=&quot;https://courses.edx.org/courses/course-v1:BerkeleyX+CS120x+2T2016/info&quot;&gt;courses&lt;/a&gt;). Although data scientists often use Spark to process data with distributed cloud computing via &lt;a href=&quot;https://aws.amazon.com/ec2/&quot;&gt;Amazon EC2&lt;/a&gt; or &lt;a href=&quot;https://azure.microsoft.com/en-us/services/hdinsight/apache-spark/&quot;&gt;Microsoft Azure&lt;/a&gt;, Spark works just fine even on a typical laptop, given enough memory (for this post, I use a 2016 MacBook Pro/16GB RAM, with 8GB allocated to the Spark driver).&lt;/p&gt;

&lt;p&gt;I wrote a &lt;a href=&quot;https://github.com/minimaxir/amazon-spark/blob/master/amazon_preprocess.py&quot;&gt;simple Python script&lt;/a&gt; to combine the per-category ratings-only data from the &lt;a href=&quot;http://jmcauley.ucsd.edu/data/amazon/&quot;&gt;Amazon product reviews dataset&lt;/a&gt; curated by Julian McAuley, Rahul Pandey, and Jure Leskovec for their 2015 paper &lt;a href=&quot;http://cseweb.ucsd.edu/%7Ejmcauley/pdfs/kdd15.pdf&quot;&gt;Inferring Networks of Substitutable and Complementary Products&lt;/a&gt;. The result is a 4.53 GB CSV that would definitely not open in Microsoft Excel. The truncated and combined dataset includes the &lt;strong&gt;user_id&lt;/strong&gt; of the user leaving the review, the &lt;strong&gt;item_id&lt;/strong&gt; indicating the Amazon product receiving the review, the &lt;strong&gt;rating&lt;/strong&gt; the user gave the product from 1 to 5, and the &lt;strong&gt;timestamp&lt;/strong&gt; indicating the time when the review was written (truncated to the Day). We can also infer the &lt;strong&gt;category&lt;/strong&gt; of the reviewed product from the name of the data subset.&lt;/p&gt;

&lt;p&gt;Afterwards, using the new &lt;a href=&quot;http://spark.rstudio.com&quot;&gt;sparklyr&lt;/a&gt; package for R, I can easily start a local Spark cluster with a single &lt;code&gt;spark_connect()&lt;/code&gt; command and load the entire CSV into the cluster in seconds with a single &lt;code&gt;spark_read_csv()&lt;/code&gt; command.  &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/amazon-spark/output.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;There are 80.74 million records total in the dataset, or as the output helpfully reports, &lt;code&gt;8.074e+07&lt;/code&gt; records. Performing advanced queries with traditional tools like &lt;a href=&quot;https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html&quot;&gt;dplyr&lt;/a&gt; or even Python&amp;rsquo;s &lt;a href=&quot;http://pandas.pydata.org&quot;&gt;pandas&lt;/a&gt; on such a dataset would take a considerable amount of time to execute.&lt;/p&gt;

&lt;p&gt;With sparklyr, manipulating actually-big-data is &lt;em&gt;just as easy&lt;/em&gt; as performing an analysis on a dataset with only a few records (and an order of magnitude easier than the Python approaches taught in the eDX class mentioned above!).&lt;/p&gt;

&lt;h2&gt;Exploratory Analysis&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;(You can view the R code used to process the data with Spark and generate the data visualizations in &lt;a href=&quot;http://minimaxir.com/notebooks/amazon-spark/&quot;&gt;this R Notebook&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There are &lt;strong&gt;20,368,412&lt;/strong&gt; unique users who provided reviews in this dataset. &lt;strong&gt;51.9%&lt;/strong&gt; of those users have only written one review.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/amazon-spark/user_count_cum.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Relatedly, there are &lt;strong&gt;8,210,439&lt;/strong&gt; unique products in this dataset, where &lt;strong&gt;43.3%&lt;/strong&gt; have only one review.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/amazon-spark/item_count_cum.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;After removing duplicate ratings, I added a few more features to each rating which may help illustrate how review behavior changed over time: a ranking value indicating the # review that the author of a given review has written (1st review by author, 2nd review by author, etc.), a ranking value indicating the # review that the product of a given review has received (1st review for product, 2nd review for product, etc.), and the month and year the review was made.&lt;/p&gt;

&lt;p&gt;The first two added features require a &lt;em&gt;very&lt;/em&gt; large amount of processing power, and highlight the convenience of Spark&amp;rsquo;s speed (and the fact that Spark uses all CPU cores by default, while typical R/Python approaches are single-threaded!)&lt;/p&gt;

&lt;p&gt;These changes are cached into a Spark DataFrame &lt;code&gt;df_t&lt;/code&gt;. If I wanted to determine which Amazon product category receives the best review ratings on average, I can aggregate the data by category, calculate the average rating score for each category, and sort. Thanks to the power of Spark, the data processing for this many-millions-of-records takes seconds.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;df_agg &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; df_t &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
            group_by&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;category&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
            summarize&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;count &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; n&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; avg_rating &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;rating&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
            arrange&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;desc&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;avg_rating&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;
            collect&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/img/amazon-spark/avg.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Or, visualized in chart form using &lt;a href=&quot;http://ggplot2.org&quot;&gt;ggplot2&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/amazon-spark/avg_rating_desc.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Digital Music/CD products receive the highest reviews on average, while Video Games and Cell Phones receive the lowest reviews on average, with a &lt;strong&gt;0.77&lt;/strong&gt; rating range between them. This does make some intuitive sense; Digital Music and CDs are types of products where you know &lt;em&gt;exactly&lt;/em&gt; what you are getting with no chance of a random product defect, while Cell Phones and Accessories can have variable quality from shady third-party sellers (Video Games in particular are also prone to irrational &lt;a href=&quot;http://steamed.kotaku.com/steam-games-are-now-even-more-susceptible-to-review-bom-1774940065&quot;&gt;review bombing&lt;/a&gt; over minor grievances).&lt;/p&gt;

&lt;p&gt;We can refine this visualization by splitting each bar into a percentage breakdown of each rating from 1-5. This could be plotted with a pie chart for each category, however a stacked bar chart, scaled to 100%, looks much cleaner.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/amazon-spark/category_breakdown.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The new visualization does help support the theory above; the top categories have a significantly higher percentage of 4/5-star ratings than the bottom categories, and a much a lower proportion of 1/2/3-star ratings. The inverse holds true for the bottom categories.&lt;/p&gt;

&lt;p&gt;How have these breakdowns changed over time? Are there other factors in play?&lt;/p&gt;

&lt;h2&gt;Rating Breakdowns Over Time&lt;/h2&gt;

&lt;p&gt;Perhaps the advent of the binary Like/Dislike behaviors in social media in the 2000&amp;rsquo;s have translated into a change in behavior for a 5-star review system. Here are the rating breakdowns for reviews written in each month from January 2000 to July 2014:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/amazon-spark/time_breakdown.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The voting behavior oscillates very slightly over time with no clear spikes or inflection points, which dashes that theory.&lt;/p&gt;

&lt;h2&gt;Distribution of Average Scores&lt;/h2&gt;

&lt;p&gt;We should look at the global averages of Amazon product scores (i.e. what customers see when they buy products), and the users who give the ratings. We would expect the distributions to match, so any deviations would be interesting.&lt;/p&gt;

&lt;p&gt;Products on average, when looking at products with atleast 5 ratings, have a &lt;strong&gt;4.16&lt;/strong&gt; overall rating.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/amazon-spark/item_histogram.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;When looking at a similar graph for the overall ratings given by users, (5 ratings minimum), the average rating is slightly higher at &lt;strong&gt;4.20&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/amazon-spark/user_histogram.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The primary difference between the two distributions is that there is significantly higher proportion of Amazon customers giving &lt;em&gt;only&lt;/em&gt; 5-star reviews. Normalizing and overlaying the two charts clearly highlights that discrepancy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/amazon-spark/user_item_histogram.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;h2&gt;The Marginal Review&lt;/h2&gt;

&lt;p&gt;A few posts ago, I discussed how the &lt;a href=&quot;http://minimaxir.com/2016/11/first-comment/&quot;&gt;first comment on a Reddit post&lt;/a&gt; has dramatically more influence than subsequent comments. Does user rating behavior change after making more and more reviews? Is the typical rating behavior different for the first review of a given product? &lt;/p&gt;

&lt;p&gt;Here is the ratings breakdown for the &lt;em&gt;n&lt;/em&gt;-th Amazon review a user gives:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/amazon-spark/user_nth_breakdown.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The first user review has a slightly higher proportion of being a 1-star review than subsequent reviews. Otherwise, the voting behavior is mostly the same overtime, although users have an increased proportion of giving a 4-star review instead of a 5-star review as they get more comfortable.&lt;/p&gt;

&lt;p&gt;In contrast, here is the ratings breakdown for the &lt;em&gt;n&lt;/em&gt;-th review an Amazon product received:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/amazon-spark/item_nth_breakdown.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The first product review has a slightly higher proportion of being a 5-star review than subsequent reviews. However, after the 10th review, there is &lt;em&gt;zero&lt;/em&gt; change in the distribution of ratings, which implies that the marginal rating behavior is independent from the current score after that threshold.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;Granted, this blog post is more playing with data and less analyzing data. What might be interesting to look into for future technical posts is conditional behavior, such as predicting the rating of a review given the previous ratings on that product/by that user. However, this post shows that while &amp;ldquo;big data&amp;rdquo; may be an inscrutable buzzword nowadays, you don&amp;rsquo;t have to work for a Fortune 500 company to be able to understand it. Even with a data set consisting of 5 simple features, you can extract a large number of insights. &lt;/p&gt;

&lt;p&gt;And this post doesn&amp;rsquo;t even look at the text of the Amazon product reviews or the metadata associated with the products! I do have a few ideas lined up there which I won&amp;rsquo;t spoil.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;You can view all the R and ggplot2 code used to visualize the Amazon data in &lt;a href=&quot;http://minimaxir.com/notebooks/amazon-spark/&quot;&gt;this R Notebook&lt;/a&gt;. You can also view the images/data used for this post in &lt;a href=&quot;https://github.com/minimaxir/amazon-spark&quot;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 02 Jan 2017 09:00:00 -0800</pubDate>
        <link>http://minimaxir.com/2017/01/amazon-spark/</link>
        <guid isPermaLink="true">http://minimaxir.com/2017/01/amazon-spark/</guid>
        
        
        <category>Visualization</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Network Visualization of Breached Internet Services Using HaveIBeenPwned? Data</title>
        <description>&lt;p&gt;Last week, Yahoo &lt;a href=&quot;https://yahoo.tumblr.com/post/154479236569/important-security-information-for-yahoo-users&quot;&gt;announced&lt;/a&gt; that 1 billion accounts on their service were compromised by attackers in 2013. That is bad, and unfortunately common nowadays.&lt;/p&gt;

&lt;p&gt;Security expert &lt;a href=&quot;https://twitter.com/troyhunt&quot;&gt;Troy Hunt&lt;/a&gt; maintains &lt;a href=&quot;https://haveibeenpwned.com&quot;&gt;Have I been pwned?&lt;/a&gt;, a database of breached internet services where users can check to see if their account information has been compromised in such an attack as the Yahoo attack. Recently, Hunt &lt;a href=&quot;https://www.troyhunt.com/heres-1-4-billion-records-from-have-i-been-pwned-for-you-to-analyse/&quot;&gt;released a dataset&lt;/a&gt; consisting of &lt;strong&gt;1.4 billion&lt;/strong&gt; unique breached accounts, and the services where those specific accounts were compromised.&lt;/p&gt;

&lt;p&gt;The dataset has been scrubbed of identifying information and sensitive data (justifiability so), which strongly limits the scope of any potential analysis. However, what may be interesting is taking a look at the interrelated networks between the services and creating a neat data visualization.&lt;/p&gt;

&lt;h2&gt;Methodology&lt;/h2&gt;

&lt;p&gt;Here&amp;rsquo;s a line from the 141.8 MB text file:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Adobe;GFAN;HeroesOfNewerth;NetEase;Tumblr 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In this example, 2 users had their accounts breached at these specific 5 services.&lt;/p&gt;

&lt;p&gt;I wrote a &lt;a href=&quot;https://github.com/minimaxir/breach-network/blob/master/hibp_service_edges.py&quot;&gt;simple Python script&lt;/a&gt; that takes in the HIBP data dump and outputs two files:&lt;/p&gt;

&lt;p&gt;One, &lt;a href=&quot;https://github.com/minimaxir/breach-network/blob/master/hibp_services.csv&quot;&gt;a CSV file&lt;/a&gt; containing the total number of breached accounts from each service in this dataset. From the example data point, we would add 2 to the counts of Adobe breaches, GFAN breaches, etc. There are &lt;strong&gt;1,768,628,867&lt;/strong&gt; total records in the dataset (which is greater than the earlier 1.4B metric since it multicounts accounts which have been breached multiple times). The 1.77B number approximately matches the given number of total number of records on HIBP (1,989,141,353) minus the number of records from sensitive breaches (~221M).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/breach-network/nodes.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Two, &lt;a href=&quot;https://github.com/minimaxir/breach-network/blob/master/hibp_edges.csv&quot;&gt;a CSV file&lt;/a&gt; containing the unique combination pairs of each service in a data point, From the example data point, we would add 2 to the counts of [Adobe, GFAN], [Adobe, HeroesOfNewerth], etc. (statistically-minded readers will notice that there are 10 unique two-value combinations for a set of 5 services). &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/breach-network/edges.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Although the HIBP dataset represents 1.77 Billion records, it is far from &amp;ldquo;big data&amp;rdquo;: my Python script takes less than a minute to process it.&lt;/p&gt;

&lt;p&gt;The latter CSV serves as the edge list for a network graph. There are 10,816 unique edges. Using &lt;a href=&quot;https://www.r-project.org&quot;&gt;R&lt;/a&gt;, &lt;a href=&quot;http://ggplot2.org&quot;&gt;ggplot2&lt;/a&gt;, and &lt;a href=&quot;https://briatte.github.io/ggnetwork/&quot;&gt;ggnetwork&lt;/a&gt;, I connected all the edges into a graph network (removing edges with too few breached accounts), set the layout according to 50,000 iterations of the &lt;a href=&quot;https://github.com/gephi/gephi/wiki/Fruchterman-Reingold&quot;&gt;Fruchterman-Reingold algorithm&lt;/a&gt;, and attempted to make it pretty.&lt;/p&gt;

&lt;p&gt;The results of my first made-in-an-hour-after-getting-home-from-work draft were &lt;a href=&quot;http://i.imgur.com/FpLiFGk.png&quot;&gt;not pretty&lt;/a&gt;. On Hacker News, user flashman &lt;a href=&quot;https://news.ycombinator.com/item?id=13112461&quot;&gt;provides a sensible fix&lt;/a&gt;; only include edges in the network with a proportional number of accounts shared between the two internet services it connects (e.g. the edge contains atleast 1% of the accounts in both services).&lt;/p&gt;

&lt;p&gt;Removing edges which do not fit that criteria dramatically reduces the clutter and makes communities much more distinct. After rerunning the code and making further style tweaks, here&amp;rsquo;s the final result:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/img/breach-network/hibp.png&quot;&gt;&lt;img src=&quot;/img/breach-network/hibp.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(click on the image to view at full resolution)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The interactive version at the top of the page was created with &lt;a href=&quot;https://plot.ly&quot;&gt;Plotly&lt;/a&gt;, via my workflow mentioned in a &lt;a href=&quot;http://minimaxir.com/2016/12/interactive-network/&quot;&gt;previous post&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Analysis&lt;/h2&gt;

&lt;p&gt;The colors of the nodes represent the communities as determined by the &lt;a href=&quot;http://arxiv.org/abs/physics/0512106&quot;&gt;Walktrap community finding algorithm&lt;/a&gt; run on the network. There are a few noteworthy groups: the turquoise group of mainstream social networking services including LinkedIn and MySpace, the brown group of mainstream gaming services like Nexus Mods and XSplit, and mustard-colored services of questionable legality on the right of the cluster. There are clusters of services extended far from the general clusters which represent non-English services: the yellow-green cluster on the left consists of Russian internet services, while the lime-green cluster at the top represents Chinese internet services.&lt;/p&gt;

&lt;p&gt;The size of the nodes represent the degree of the node, or the number of nodes connected to a given node. With that, we can easily see services like LinkedIn and XSplit have strong connections to other breached services. Relatedly, the transparency of the edges in the image is determined by the corresponding weight of the edge, and demonstrates the relative magnitude of the LinkedIn/MySpace breaches. (And it&amp;rsquo;s also why we can&amp;rsquo;t use other metrics like &lt;a href=&quot;https://en.wikipedia.org/wiki/Centrality&quot;&gt;centrality&lt;/a&gt; for sizing the nodes, as the relative weights of those breaches skew the result.)&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s important is that the network is generated &lt;em&gt;entirely from user behavior&lt;/em&gt;, and not by manually establishing the actual relationships between the internet services.&lt;/p&gt;

&lt;p&gt;Given the wildly-varying timing and magnitude of these breaches, along with the numerous potential &lt;em&gt;causes&lt;/em&gt; of breaches which are not always publicly disclosed, it is difficult to make accurate predictive models about future breaches from the HaveIBeenPwned? dataset. The records in HIBP are a &lt;em&gt;very&lt;/em&gt; small sample of all the leaked data worldwide, unfortunately. However, it shows what  relationships can be visualized from simple user fingerprints all around the web, even when the fingerprint &lt;em&gt;itself&lt;/em&gt; is unknown.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;em&gt;You can view all the R and ggplot2 code used to visualize the network data in &lt;a href=&quot;http://minimaxir.com/notebooks/breach-network/&quot;&gt;this R Notebook&lt;/a&gt;. You can also view the images/data used for this post in &lt;a href=&quot;https://github.com/minimaxir/breach-network&quot;&gt;this GitHub repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You are free to use the data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Dec 2016 08:30:00 -0800</pubDate>
        <link>http://minimaxir.com/2016/12/pwned-network/</link>
        <guid isPermaLink="true">http://minimaxir.com/2016/12/pwned-network/</guid>
        
        
        <category>Interactive</category>
        
        <category>Visualization</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>Infinite-Quality Abstract Art and Animations with Primitive</title>
        <description>&lt;p&gt;4k media (&amp;ldquo;&lt;a href=&quot;https://en.wikipedia.org/wiki/Ultra-high-definition_television&quot;&gt;Ultra HD&lt;/a&gt;&amp;rdquo;) is the new big thing. &lt;a href=&quot;https://www.youtube.com&quot;&gt;YouTube&lt;/a&gt; and &lt;a href=&quot;https://www.netflix.com/&quot;&gt;Netflix&lt;/a&gt; now offer 4k video streaming, cell phones such as the &lt;a href=&quot;http://www.apple.com/iphone-7/&quot;&gt;iPhone 7&lt;/a&gt; can now record 4k video, gaming consoles such as the &lt;a href=&quot;https://www.playstation.com/en-us/explore/ps4-pro/&quot;&gt;PlayStation 4 Pro&lt;/a&gt; can play video games in native 4k, and 4k-compatible computer monitors and television sets are now priced reasonably.&lt;/p&gt;

&lt;p&gt;However, the improvements offered by 4k do not come for free. From 1080p, 4k media requires much more resources to store the media on disk, much more resources to read from disk or stream over the Internet, and much more resources to store in memory. And Moore&amp;rsquo;s Law has been slowing down.&lt;/p&gt;

&lt;p&gt;Another option worth considering in light of these potential constraints is vector graphics, which can be scaled to &lt;em&gt;any&lt;/em&gt; resolution at a significant fraction of the storage space. Although graphic formats such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Scalable_Vector_Graphics&quot;&gt;Scalable Vector Graphics&lt;/a&gt; (SVG) have existed &lt;em&gt;long&lt;/em&gt; before 4k media (and even 1080p media), support for it had died out over the years since JPGs and PNGs were more practical for the 1024x768px screens of the day, and older versions of Internet Explorer did not support SVGs. However, creating complex SVGs is not trivial, and there aren&amp;rsquo;t any magic applications that can convert a normal image into vector graphics automatically (the closest is &lt;a href=&quot;http://potrace.sourceforge.net&quot;&gt;potrace&lt;/a&gt;, which only works for one color).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/fogleman/primitive&quot;&gt;Primitive&lt;/a&gt;, an open-source project by &lt;a href=&quot;https://twitter.com/FogleBird&quot;&gt;Michael Fogleman&lt;/a&gt;, is a cross-platform command-line application that creates abstract art by taking simple polygons such as triangles and rectangles and placing them such that the group of polygons is as close as possible to the original image. The Twitter account &lt;a href=&quot;https://twitter.com/PrimitivePic&quot;&gt;@PrimitivePic&lt;/a&gt; posts examples of such images every half hour.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/primitive/git.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://primitive.lol&quot;&gt;Primitive application&lt;/a&gt; on the &lt;a href=&quot;https://itunes.apple.com/us/app/primitive/id1175103038?mt=12&quot;&gt;Mac App Store&lt;/a&gt;, also by Fogleman, provides a simple user interface for the application for $9.99 (worth it for the productivity increase over fiddling with the command-line). With this GUI, you can see exactly &lt;em&gt;how&lt;/em&gt; the art is being made, in real time. The app has also been &lt;a href=&quot;https://medium.com/art-marketing/product-launch-post-mortem-primitive-for-macos-2eee316134ad&quot;&gt;financially successful&lt;/a&gt; for a side project.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/primitive/primitive.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Both interfaces allow for SVG export, which means that we can use Primitive to take small images and scale them &lt;em&gt;indefinitely&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;But what cool things can you do with these fancy SVGs? Let&amp;rsquo;s take a look.&lt;/p&gt;

&lt;h2&gt;Playing with Primitive&lt;/h2&gt;

&lt;p&gt;Here&amp;rsquo;s a photo I took of the &lt;a href=&quot;https://en.wikipedia.org/wiki/San_Francisco_Ferry_Building&quot;&gt;San Francisco Ferry Building&lt;/a&gt; last year, cropped to 16:9 with an attempt to follow the &lt;a href=&quot;https://en.wikipedia.org/wiki/Rule_of_thirds&quot;&gt;rule of thirds&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/primitive/San_Francisco.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;This PNG image is 672 KB. Let&amp;rsquo;s try running it through Primitive to see if we can make it smaller, and make it &lt;em&gt;cooler&lt;/em&gt; by drawing 50 quadrilaterals (with zero transparency), at the 1024px &amp;ldquo;nicest&amp;rdquo; working size for maximum quality.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/primitive/SF_Final_50.svg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Now that is definitely minimalistic. Maybe &lt;em&gt;too&lt;/em&gt; abstract. What happens if we want to add detail? We can do that by adding many quadratic &lt;a href=&quot;https://en.wikipedia.org/wiki/B%C3%A9zier_curve&quot;&gt;B√®zier curves&lt;/a&gt;. Let&amp;rsquo;s add 500 solid curves to this image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/primitive/SF_Final_550.svg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Now that looks fancy, is only 94 KB (about 1/8th the file size of the static image!), and can scale to any screen size.&lt;/p&gt;

&lt;p&gt;You may have noticed that this SVG is the final result from the &lt;a href=&quot;/img/primitive/SF_Final_550_animated.svg&quot;&gt;animation&lt;/a&gt; at the beginning of this article. This particular shenanigan is &lt;a href=&quot;http://minimaxir.com/2016/12/interactive-network/&quot;&gt;another&lt;/a&gt; of my trademark weird-tricks-that-should-not-even-work-but-do. By passing the final SVG through &lt;a href=&quot;https://maxwellito.github.io/vivus/&quot;&gt;Vivus&lt;/a&gt; (specifically, &lt;a href=&quot;https://maxwellito.github.io/vivus-instant/&quot;&gt;Vivus Instant&lt;/a&gt;), we can animate the B√®zier curves such that it looks like they are being drawn &lt;em&gt;in real time&lt;/em&gt; on the quadrilateral background.&lt;/p&gt;

&lt;p&gt;The animations generated from Vivus Instant are pure CSS and require no additional dependencies (although you&amp;rsquo;ll need to &lt;a href=&quot;https://css-tricks.com/scale-svg/&quot;&gt;manually add&lt;/a&gt; the &lt;code&gt;viewBox&lt;/code&gt; attribute to make the SVG mobile-friendly). Even then, the result is 276 KB; still much smaller than the original raster image!&lt;/p&gt;

&lt;p&gt;Of the polygons available in Primitive, Vivus Instant will only animate the B√®zier curves. If you wanted to animate other polygons, you could use &lt;a href=&quot;http://snapsvg.io&quot;&gt;Snap.svg&lt;/a&gt;, which might make a fun project for a future post.&lt;/p&gt;

&lt;p&gt;These SVG animations only work on the web, but it&amp;rsquo;s not difficult to make animations of SVG creation into a share-friendly format.&lt;/p&gt;

&lt;h2&gt;Abstract GIFs&lt;/h2&gt;

&lt;p&gt;GIFs nowadays can be shared anywhere, and Primitive has a few ways of making GIFs. The command-line interface has a GIF-maker built in; essentially, it outputs each frame as an image and stitches them together.&lt;/p&gt;

&lt;p&gt;The Primitive Mac App has another option. Since the polygons are rendered on-screen, you can &lt;em&gt;record the screen&lt;/em&gt; using tools like &lt;a href=&quot;https://support.apple.com/en-us/HT201066&quot;&gt;QuickTime&amp;rsquo;s screen recording&lt;/a&gt; to simultaneously visualize and record the vector image. Once recorded (and after editing the video to speed things up), you can use one of any number of video-to-GIF converters to make a sharable animation.&lt;/p&gt;

&lt;p&gt;For this test, let&amp;rsquo;s try using an &lt;em&gt;extremely&lt;/em&gt; small image and see how well Primitive can extrapolate to an infinite size. Here&amp;rsquo;s a 64x64px sprite of the &lt;a href=&quot;http://www.pokemon.com/us/&quot;&gt;Pok√®mon&lt;/a&gt; Pikachu from the 3rd generation of Pok√®mon games (via &lt;a href=&quot;http://bulbapedia.bulbagarden.net/wiki/Pikachu_(Pok%C3%A9mon)&quot;&gt;Bulbapedia&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/primitive/Spr_3r_025.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;I made a video of Primitive rendering Pikachu using a combination of 1,000 ellipses, rectangles, triangles and curves, and converted it to a GIF using my noncreatively-named &lt;a href=&quot;https://github.com/minimaxir/video-to-gif-osx&quot;&gt;Convert Video to GIF&lt;/a&gt; tool. The result?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/primitive/Pikachu_Record.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Not too bad, given the input. Again, the GIF can be sized to any resolution, bandwidth considerations aside.&lt;/p&gt;

&lt;h2&gt;Abstract 4k Videos&lt;/h2&gt;

&lt;p&gt;So how do you get these infinite-quality images into 4k videos? You can export the Primitive renderings as a super-high-resolution PNG and embed them into the video, but there may still be pixellation if you cinematically zoom into the image. SVGs aren&amp;rsquo;t natively supported by video editing programs such as Final Cut Pro and Adobe Premiere. But &lt;em&gt;PDFs&lt;/em&gt; are, and there are many SVG-to-PDF converters which preserve the vector information (On macOS, &lt;a href=&quot;https://itunes.apple.com/us/app/gapplin/id768053424?mt=12&quot;&gt;Gapplin&lt;/a&gt; works fine for this purpose).&lt;/p&gt;

&lt;p&gt;For vector scaling in Final Cut Pro, you must first &lt;a href=&quot;http://www.fcp.co/forum/4-final-cut-pro-x-fcpx/25544-best-way-to-use-illustrador-files-in-fcpx#77676&quot;&gt;import through Motion&lt;/a&gt; first. Here&amp;rsquo;s a quick 4k/60fps video as proof using the Ferry Building vector image above, with a bit of style.&lt;/p&gt;

&lt;div class=&quot;responsive-video&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/Cv4P6VMvBCw &quot; style=&quot;width: 100%; height: 100%; border: none; padding-bottom: 20px&quot; allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;Again, vector images are not a new concept, but given the changes in the media and technology landscape, it is very worthwhile to give them a second chance. Once technology evolves and the media industries start pushing &lt;em&gt;8k media&lt;/em&gt; (and they will), having a sensible method of producing high-quality assets pays for itself.&lt;/p&gt;

&lt;p&gt;At the least, Primitive will give me some nice background visuals for when I start producing 4k data visualizations videos (yes, that&amp;rsquo;s a thing!).&lt;/p&gt;
</description>
        <pubDate>Mon, 12 Dec 2016 06:30:00 -0800</pubDate>
        <link>http://minimaxir.com/2016/12/primitive/</link>
        <guid isPermaLink="true">http://minimaxir.com/2016/12/primitive/</guid>
        
        
        <category>Visualization</category>
        
        <category>Data</category>
        
      </item>
    
  </channel>
</rss>
