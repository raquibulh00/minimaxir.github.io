<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.3.1"><meta name=author content="Max Woolf"><meta name=description content="Using CPUs instead of GPUs for deep learning training in the cloud is cheaper because of the massive cost differential afforded by preemptible instances."><link rel=alternate hreflang=en-us href=https://minimaxir.com/2017/07/cpu-or-gpu/><meta name=theme-color content=#2962ff><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400italic,700|Source+Code+Pro:400,400italic,700&display=swap"><link rel=stylesheet href=/css/academic.min.4cdedb6ca5fc8a13caf3e26423bf7037.css><link rel=stylesheet href=/css/academic.59da4f61e2de6d8a5935b902fe667ab3.css><link rel=manifest href=/site.webmanifest><link rel=icon type=image/png href=/img/icon.png><link rel=apple-touch-icon type=image/png href=/img/icon-192.png><link rel=canonical href=https://minimaxir.com/2017/07/cpu-or-gpu/><meta property=twitter:card content=summary_large_image><meta property=twitter:site content=@minimaxir><meta property=twitter:creator content=@minimaxir><meta property=article:author content=https://www.facebook.com/max.woolf><meta property=og:site_name content="Max Woolf's Blog"><meta property=og:url content=https://minimaxir.com/2017/07/cpu-or-gpu/><meta property=og:type content=article><meta property=og:title content="Benchmarking TensorFlow on Cloud CPUs: Cheaper Deep Learning than Cloud GPUs"><meta property=og:description content="Using CPUs instead of GPUs for deep learning training in the cloud is cheaper because of the massive cost differential afforded by preemptible instances."><meta property=og:image content=https://minimaxir.com/2017/07/cpu-or-gpu/featured.png><meta property=twitter:image content=https://minimaxir.com/2017/07/cpu-or-gpu/featured.png><meta property=og:locale content=en-us><meta property=article:published_time content=2017-07-05T09:00:00-07:00><meta property=article:modified_time content=2017-07-05T09:00:00-07:00><title>Benchmarking TensorFlow on Cloud CPUs: Cheaper Deep Learning than Cloud GPUs | Max Woolf&#39;s Blog</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id=navbar-main><div class=container><a class=navbar-brand href=/>Max Woolf&#39;s Blog</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/about/><span>About</span></a></li><li class=nav-item><a class=nav-link href=/post/><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/portfolio/><span>Portfolio</span></a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=https://www.patreon.com/minimaxir target=_blank rel=noopener><span><i class="fab fa-patreon mr-1"></i>Patreon</span></a></li><li class=nav-item><a class=nav-link href=https://github.com/minimaxir target=_blank rel=noopener><span><i class="fab fa-github-alt mr-1"></i>GitHub</span></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><article class=article itemscope itemtype=http://schema.org/Article><div class="article-container pt-3"><h1 itemprop=name>Benchmarking TensorFlow on Cloud CPUs: Cheaper Deep Learning than Cloud GPUs</h1><meta content="2017-07-05 09:00:00 -0700 PDT" itemprop=datePublished><meta content="2017-07-05 09:00:00 -0700 PDT" itemprop=dateModified><div class=article-metadata><span class=article-date><time>July 5, 2017</time></span>
<span class=middot-divider></span><span class=article-reading-time>7 min read</span><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://minimaxir.com/2017/07/cpu-or-gpu/&amp;text=Benchmarking%20TensorFlow%20on%20Cloud%20CPUs:%20Cheaper%20Deep%20Learning%20than%20Cloud%20GPUs" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://minimaxir.com/2017/07/cpu-or-gpu/&amp;t=Benchmarking%20TensorFlow%20on%20Cloud%20CPUs:%20Cheaper%20Deep%20Learning%20than%20Cloud%20GPUs" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook-f"></i></a></li><li><a href="mailto:?subject=Benchmarking%20TensorFlow%20on%20Cloud%20CPUs:%20Cheaper%20Deep%20Learning%20than%20Cloud%20GPUs&amp;body=https://minimaxir.com/2017/07/cpu-or-gpu/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://minimaxir.com/2017/07/cpu-or-gpu/&amp;title=Benchmarking%20TensorFlow%20on%20Cloud%20CPUs:%20Cheaper%20Deep%20Learning%20than%20Cloud%20GPUs" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://reddit.com/submit?url=https://minimaxir.com/2017/07/cpu-or-gpu/&amp;title=Benchmarking%20TensorFlow%20on%20Cloud%20CPUs:%20Cheaper%20Deep%20Learning%20than%20Cloud%20GPUs" target=_blank rel=noopener class=share-btn-reddit><i class="fab fa-reddit-alien"></i></a></li></ul></div></div></div><div class=article-container><div class=article-style itemprop=articleBody><p>I&rsquo;ve been working on a few personal deep learning projects with <a href=https://github.com/fchollet/keras target=_blank>Keras</a> and <a href=https://www.tensorflow.org target=_blank>TensorFlow</a>. However, training models for deep learning with cloud services such as <a href=https://aws.amazon.com/ec2/ target=_blank>Amazon EC2</a> and <a href=https://cloud.google.com/compute/ target=_blank>Google Compute Engine</a> isn&rsquo;t free, and as someone who is currently unemployed, I have to keep an eye on extraneous spending and be as cost-efficient as possible (please support my work on <a href=https://www.patreon.com/minimaxir target=_blank>Patreon</a>!). I tried deep learning on the cheaper CPU instances instead of GPU instances to save money, and to my surprise, my model training was only slightly slower. As a result, I took a deeper look at the pricing mechanisms of these two types of instances to see if CPUs are more useful for my needs.</p><p>The <a href=https://cloud.google.com/compute/pricing#gpus target=_blank>pricing of GPU instances</a> on Google Compute Engine starts at <strong>$0.745/hr</strong> (by attaching a $0.700/hr GPU die to a $0.045/hr n1-standard-1 instance). A couple months ago, Google <a href=https://cloudplatform.googleblog.com/2017/05/Compute-Engine-machine-types-with-up-to-64-vCPUs-now-ready-for-your-production-workloads.html target=_blank>announced</a> CPU instances with up to 64 vCPUs on the modern Intel <a href=https://en.wikipedia.org/wiki/Skylake_(microarchitecture) target=_blank>Skylake</a> CPU architecture. More importantly, they can also be used in <a href=https://cloud.google.com/compute/docs/instances/preemptible target=_blank>preemptible CPU instances</a>, which live at most for 24 hours on GCE and can be terminated at any time (very rarely), but cost about <em>20%</em> of the price of a standard instance. A preemptible n1-highcpu-64 instance with 64 vCPUs and 57.6GB RAM plus the premium for using Skylake CPUs is <strong>$0.509/hr</strong>, about 2/3rds of the cost of the GPU instance.</p><p>If the model training speed of 64 vCPUs is comparable to that of a GPU (or even slightly slower), it would be more cost-effective to use the CPUs instead. But that&rsquo;s assuming the deep learning software and the GCE platform hardware operate at 100% efficiency; if they don&rsquo;t (and they likely don&rsquo;t), there may be <em>even more savings</em> by scaling down the number of vCPUs and cost accordingly (a 32 vCPU instance with same parameters is half the price at <strong>$0.254/hr</strong>, 16 vCPU at <strong>$0.127/hr</strong>, etc).</p><p>There aren&rsquo;t any benchmarks for deep learning libraries with tons and tons of CPUs since there&rsquo;s no demand, as GPUs are the <a href=https://en.wikipedia.org/wiki/Occam%27s_razor target=_blank>Occam&rsquo;s razor</a> solution to deep learning hardware. But what might make counterintuitive but economical sense is to use CPUs instead of GPUs for deep learning training because of the massive cost differential afforded by preemptible instances, thanks to Google&rsquo;s <a href=https://en.wikipedia.org/wiki/Economies_of_scale target=_blank>economies of scale</a>.</p><h2 id=setup>Setup</h2><p>I already have <a href=https://github.com/minimaxir/deep-learning-cpu-gpu-benchmark target=_blank>benchmarking scripts</a> of real-world deep learning use cases, <a href=https://github.com/minimaxir/keras-cntk-docker target=_blank>Docker container environments</a>, and results logging from my <a href=http://minimaxir.com/2017/06/keras-cntk/ target=_blank>TensorFlow vs. CNTK article</a>. A few minor tweaks allow the scripts to be utilized for both CPU and GPU instances by setting CLI arguments. I also rebuilt <a href=https://github.com/minimaxir/keras-cntk-docker/blob/master/Dockerfile target=_blank>the Docker container</a> to support the latest version of TensorFlow (1.2.1), and created a <a href=https://github.com/minimaxir/keras-cntk-docker/blob/master/Dockerfile-cpu target=_blank>CPU version</a> of the container which installs the CPU-appropriate TensorFlow library instead.</p><p>There is a notable CPU-specific TensorFlow behavior; if you install from <code>pip</code> (as the<a href=https://www.tensorflow.org/install/ target=_blank> official instructions</a> and tutorials recommend) and begin training a model in TensorFlow, you&rsquo;ll see these warnings in the console:</p><p><img src=/img/cpu-or-gpu/tensorflow-console.png alt></p><p>In order to fix the warnings and benefit from these <a href=https://en.wikipedia.org/wiki/SSE4#SSE4.2 target=_blank>SSE4.2</a>/<a href=https://en.wikipedia.org/wiki/Advanced_Vector_Extensions target=_blank>AVX</a>/<a href=https://en.wikipedia.org/wiki/FMA_instruction_set target=_blank>FMA</a> optimizations, we <a href=https://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions target=_blank>compile TensorFlow from source</a>, and I created a <a href=https://github.com/minimaxir/keras-cntk-docker/blob/master/Dockerfile-cpu-compiled target=_blank>third Docker container</a> to do just that. When training models in the new container, <a href=https://github.com/tensorflow/tensorflow/issues/10689 target=_blank>most</a> of the warnings no longer show, and (spoiler alert) there is indeed a speed boost in training time.</p><p>Therefore, we can test three major cases with Google Compute Engine:</p><ul><li>A Tesla K80 GPU instance.</li><li>A 64 Skylake vCPU instance where TensorFlow is installed via <code>pip</code> (along with testings at 8/16/32 vCPUs).</li><li>A 64 Skylake vCPU instance where TensorFlow is compiled (<code>cmp</code>) with CPU instructions (+ 8/16/32 vCPUs).</li></ul><h2 id=results>Results</h2><p>For each model architecture and software/hardware configuration, I calculate the <strong>total training time relative to the GPU instance training</strong> for running the model training for the provided test script. In all cases, the GPU <em>should</em> be the fastest training configuration, and systems with more processors should train faster than those with fewer processors.</p><p>Let&rsquo;s start using the <a href=http://yann.lecun.com/exdb/mnist/ target=_blank>MNIST dataset</a> of handwritten digits plus the common multilayer perceptron (MLP) architecture, with dense fully-connected layers. Lower training time is better. All configurations below the horizontal dotted line are better than GPUs; all configurations above the dotted line are worse than GPUs.</p><p><img src=/img/cpu-or-gpu/dl-cpu-gpu-5.png alt></p><p>Here, the GPU is the fastest out of all the platform configurations, but there are other curious trends: the performance between 32 vCPUs and 64 vCPUs is similar, and the compiled TensorFlow library is indeed a significant improvement in training speed <em>but only for 8 and 16 vCPUs</em>. Perhaps there are overheads negotiating information between vCPUs that eliminate the performance advantages of more vCPUs, and perhaps these overheads are <em>different</em> with the CPU instructions of the compiled TensorFlow. In the end, it&rsquo;s a <a href=https://en.wikipedia.org/wiki/Black_box target=_blank>black box</a>, which is why I prefer black box benchmarking all configurations of hardware instead of theorycrafting.</p><p>Since the difference between training speeds of different vCPU counts is minimal, there is definitely an advantage by scaling down. For each model architecture and configuration, I calculate a <strong>normalized training cost relative to the cost of GPU instance training</strong>. Because GCE instance costs are prorated (unlike Amazon EC2), we can simply calculate experiment cost by multiplying the total number of seconds the experiment runs by the cost of the instance (per second). Ideally, we want to <em>minimize</em> cost.</p><p><img src=/img/cpu-or-gpu/dl-cpu-gpu-6.png alt></p><p>Lower CPU counts are <em>much</em> more cost-effective for this problem, when going as low as possible is better.</p><p>Now, let&rsquo;s look at the same dataset with a convolutional neural network (CNN) approach for digit classification:</p><p><img src=/img/cpu-or-gpu/dl-cpu-gpu-7.png alt></p><p><img src=/img/cpu-or-gpu/dl-cpu-gpu-8.png alt></p><p>GPUs are unsurprisingly more than twice as fast as any CPU approach at CNNs, but cost structures are still the same, except that 64 vCPUs are <em>worse</em> than GPUs cost-wise, with 32 vCPUs training even faster than with 64 vCPUs.</p><p>Let&rsquo;s go deeper with CNNs and look at the <a href=https://www.cs.toronto.edu/%7Ekriz/cifar.html target=_blank>CIFAR-10</a> image classification dataset, and a model which utilizes a deep covnet + a multilayer perceptron and ideal for image classification (similar to the <a href=https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3 target=_blank>VGG-16</a> architecture).</p><p><img src=/img/cpu-or-gpu/dl-cpu-gpu-9.png alt></p><p><img src=/img/cpu-or-gpu/dl-cpu-gpu-10.png alt></p><p>Similar behaviors as in the simple CNN case, although in this instance all CPUs perform better with the compiled TensorFlow library.</p><p>The fasttext algorithm, used here on the <a href=http://ai.stanford.edu/%7Eamaas/data/sentiment/ target=_blank>IMDb reviews dataset</a> to determine whether a review is positive or negative, classifies text extremely quickly relative to other methods.</p><p><img src=/img/cpu-or-gpu/dl-cpu-gpu-3.png alt></p><p><img src=/img/cpu-or-gpu/dl-cpu-gpu-4.png alt></p><p>In this case, GPUs are much, much faster than CPUs. The benefit of lower numbers of CPU isn&rsquo;t as dramatic; although as an aside, the <a href=https://github.com/facebookresearch/fastText target=_blank>official fasttext implementation</a> is <em>designed</em> for large amounts of CPUs and handles parallelization much better.</p><p>The Bidirectional long-short-term memory (LSTM) architecture is great for working with text data like IMDb reviews, but after my previous benchmark article, <a href="https://news.ycombinator.com/item?id=14538086" target=_blank>commenters on Hacker News</a> noted that TensorFlow uses an inefficient implementation of the LSTM on the GPU, so perhaps the difference will be more notable.</p><p><img src=/img/cpu-or-gpu/dl-cpu-gpu-1.png alt></p><p><img src=/img/cpu-or-gpu/dl-cpu-gpu-2.png alt></p><p>Wait, what? GPU training of Bidirectional LSTMs is <em>twice as slow</em> as any CPU configuration? Wow. (In fairness, the benchmark uses the Keras LSTM default of <code>implementation=0</code> which is better on CPUs while <code>implementation=2</code> is better on GPUs, but it shouldn&rsquo;t result in that much of a differential)</p><p>Lastly, LSTM text generation of <a href=https://en.wikipedia.org/wiki/Friedrich_Nietzsche target=_blank>Nietzsche&rsquo;s</a> <a href=https://s3.amazonaws.com/text-datasets/nietzsche.txt target=_blank>writings</a> follows similar patterns to the other architectures, but without the drastic hit to the GPU.</p><p><img src=/img/cpu-or-gpu/dl-cpu-gpu-11.png alt></p><p><img src=/img/cpu-or-gpu/dl-cpu-gpu-12.png alt></p><h2 id=conclusion>Conclusion</h2><p>As it turns out, using 64 vCPUs is <em>bad</em> for deep learning as current software/hardware architectures can&rsquo;t fully utilize all of them, and it often results in the exact same performance (or <em>worse</em>) than with 32 vCPUs. In terms balancing both training speed and cost, training models with <strong>16 vCPUs + compiled TensorFlow</strong> seems like the winner. The 30%-40% speed boost of the compiled TensorFlow library was an unexpected surprise, and I&rsquo;m shocked Google doesn&rsquo;t offer a precompiled version of TensorFlow with these CPU speedups since the gains are nontrivial.</p><p>It&rsquo;s worth nothing that the cost advantages shown here are <em>only</em> possible with preemptible instances; regular high-CPU instances on Google Compute Engine are about 5x as expensive, and as a result eliminate the cost benefits completely. Hooray for economies of scale!</p><p>A major implicit assumption with the cloud CPU training approach is that you don&rsquo;t need a trained model ASAP. In professional use cases, time may be too valuable to waste, but in personal use cases where someone can just leave a model training overnight, it&rsquo;s a very, very good and cost-effective option, and one that I&rsquo;ll now utilize.</p><hr><p><em>All scripts for running the benchmark are available in <a href=https://github.com/minimaxir/deep-learning-cpu-gpu-benchmark target=_blank>this GitHub repo</a>. You can view the R/ggplot2 code used to process the logs and create the visualizations in <a href=http://minimaxir.com/notebooks/deep-learning-cpu-gpu/ target=_blank>this R Notebook</a>.</em></p><div class="alert alert-note"><div>If you liked this blog post, I have set up a <a href=https://www.patreon.com/minimaxir target=_blank>Patreon</a> to fund my machine learning/deep learning/software/hardware needs for my future crazy yet cool projects, and any monetary contributions to the Patreon are appreciated and will be put to good creative use.</div></div></div><div class=article-tags><a class="badge badge-light" href=/tags/tensorflow/>TensorFlow</a></div><div class="media author-card" itemscope itemtype=http://schema.org/Person><img class="portrait mr-3" src="https://s.gravatar.com/avatar/28f09e3deff62333b3f32f19d3971d46?s=200')" itemprop=image alt=Avatar><div class=media-body><h5 class=card-title itemprop=name><a href=https://minimaxir.com/>Max Woolf</a></h5><h6 class=card-subtitle>Data Scientist at BuzzFeed</h6><p class=card-text itemprop=description>Ex-Apple. Carnegie Mellon graduate. Plotter of pretty charts. Former TechCrunch comment troll.</p><ul class=network-icon aria-hidden=true><li><a itemprop=sameAs href=https://twitter.com/minimaxir target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a itemprop=sameAs href=https://linkedin.com/in/minimaxir target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a itemprop=sameAs href=https://youtube.com/minimaxir target=_blank rel=noopener><i class="fab fa-youtube"></i></a></li><li><a itemprop=sameAs href=https://twitch.tv/minimaxir target=_blank rel=noopener><i class="fab fa-twitch"></i></a></li><li><a itemprop=sameAs href=mailto:max@minimaxir.com><i class="fas fa-envelope"></i></a></li></ul></div></div><div class=article-widget><div class=hr-light></div><h3>Related</h3><ul><li><a href=/2017/06/reddit-deep-learning/>Predicting the Success of a Reddit Submission with Deep Learning and Keras</a></li></ul></div><div class=article-widget><div class=post-nav><div class=post-nav-item><div class=meta-nav>Next</div><a href=/2017/08/ggplot2-web/ rel=next>How to Make High Quality Data Visualizations for Websites With R and ggplot2</a></div><div class=post-nav-item><div class=meta-nav>Previous</div><a href=/2017/06/reddit-deep-learning/ rel=prev>Predicting the Success of a Reddit Submission with Deep Learning and Keras</a></div></div></div><section id=comments><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"minimaxir"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js></script><script>hljs.initHighlightingOnLoad();</script><script src=/js/academic.min.fa2e27444bc8d51f81714869209e3287.js></script><div class=container><footer class=site-footer><p class=powered-by>Copyright Max Woolf &copy; 2020 &middot;
Powered by the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a>.
<span class=float-right aria-hidden=true><a href=# id=back_to_top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&times;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>