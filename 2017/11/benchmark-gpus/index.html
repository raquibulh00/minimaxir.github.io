<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.3.1"><meta name=author content="Max Woolf"><meta name=description content="A 36% price cut to GPU instances, in addition to the potential new benefits offered by software and GPU updates, however, might be enough to tip the cost-efficiency scales back in favor of GPUs."><link rel=alternate hreflang=en-us href=https://minimaxir.com/2017/11/benchmark-gpus/><meta name=theme-color content=#2962ff><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400italic,700|Source+Code+Pro:400,400italic,700&display=swap"><link rel=stylesheet href=/css/academic.min.a5099be3e049a7eefdcd4143a4af338a.css><link rel=stylesheet href=/css/academic.80785e08a563f430e47a4d753c8d9f2d.css><link rel=manifest href=/site.webmanifest><link rel=icon type=image/png href=/img/icon.png><link rel=apple-touch-icon type=image/png href=/img/icon-192.png><link rel=canonical href=https://minimaxir.com/2017/11/benchmark-gpus/><meta property=twitter:card content=summary_large_image><meta property=twitter:site content=@minimaxir><meta property=twitter:creator content=@minimaxir><meta property=article:author content=https://www.facebook.com/max.woolf><meta property=og:site_name content="Max Woolf's Blog"><meta property=og:url content=https://minimaxir.com/2017/11/benchmark-gpus/><meta property=og:type content=article><meta property=og:title content="Benchmarking Modern GPUs for Maximum Cloud Cost Efficiency in Deep Learning"><meta property=og:description content="A 36% price cut to GPU instances, in addition to the potential new benefits offered by software and GPU updates, however, might be enough to tip the cost-efficiency scales back in favor of GPUs."><meta property=og:image content=https://minimaxir.com/2017/11/benchmark-gpus/featured.png><meta property=twitter:image content=https://minimaxir.com/2017/11/benchmark-gpus/featured.png><meta property=og:locale content=en-us><meta property=article:published_time content=2017-11-28T08:30:00-07:00><meta property=article:modified_time content=2017-11-28T08:30:00-07:00><title>Benchmarking Modern GPUs for Maximum Cloud Cost Efficiency in Deep Learning | Max Woolf&#39;s Blog</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id=navbar-main><div class=container><a class=navbar-brand href=/>Max Woolf&#39;s Blog</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/about/><span>About</span></a></li><li class=nav-item><a class=nav-link href=/post/><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/portfolio/><span>Portfolio</span></a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=https://www.patreon.com/minimaxir target=_blank rel=noopener><span><i class="fab fa-patreon mr-1"></i>Patreon</span></a></li><li class=nav-item><a class=nav-link href=https://github.com/minimaxir target=_blank rel=noopener><span><i class="fab fa-github-alt mr-1"></i>GitHub</span></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><article class=article itemscope itemtype=http://schema.org/Article><div class="article-container pt-3"><h1 itemprop=name>Benchmarking Modern GPUs for Maximum Cloud Cost Efficiency in Deep Learning</h1><meta content="2017-11-28 08:30:00 -0700 -0700" itemprop=datePublished><meta content="2017-11-28 08:30:00 -0700 -0700" itemprop=dateModified><div class=article-metadata><span class=article-date><time>November 28, 2017</time></span>
<span class=middot-divider></span><span class=article-reading-time>8 min read</span>
<span class=middot-divider></span><span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/categories/ai/>AI</a>, <a href=/categories/cost-savings/>Cost Savings</a></span><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://minimaxir.com/2017/11/benchmark-gpus/&amp;text=Benchmarking%20Modern%20GPUs%20for%20Maximum%20Cloud%20Cost%20Efficiency%20in%20Deep%20Learning" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://minimaxir.com/2017/11/benchmark-gpus/&amp;t=Benchmarking%20Modern%20GPUs%20for%20Maximum%20Cloud%20Cost%20Efficiency%20in%20Deep%20Learning" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook-f"></i></a></li><li><a href="mailto:?subject=Benchmarking%20Modern%20GPUs%20for%20Maximum%20Cloud%20Cost%20Efficiency%20in%20Deep%20Learning&amp;body=https://minimaxir.com/2017/11/benchmark-gpus/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://minimaxir.com/2017/11/benchmark-gpus/&amp;title=Benchmarking%20Modern%20GPUs%20for%20Maximum%20Cloud%20Cost%20Efficiency%20in%20Deep%20Learning" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://reddit.com/submit?url=https://minimaxir.com/2017/11/benchmark-gpus/&amp;title=Benchmarking%20Modern%20GPUs%20for%20Maximum%20Cloud%20Cost%20Efficiency%20in%20Deep%20Learning" target=_blank rel=noopener class=share-btn-reddit><i class="fab fa-reddit-alien"></i></a></li></ul></div></div></div><div class=article-container><div class=article-style itemprop=articleBody><p>A few months ago, I <a href=http://minimaxir.com/2017/06/keras-cntk/ target=_blank>performed benchmarks</a> of deep learning frameworks in the cloud, with a <a href=http://minimaxir.com/2017/07/cpu-or-gpu/ target=_blank>followup</a> focusing on the cost difference between using GPUs and CPUs. And just a few months later, the landscape has changed, with significant updates to the low-level <a href=https://developer.nvidia.com/cudnn target=_blank>NVIDIA cuDNN</a> library which powers the raw learning on the GPU, the <a href=https://www.tensorflow.org target=_blank>TensorFlow</a> and <a href=https://github.com/Microsoft/CNTK target=_blank>CNTK</a> deep learning frameworks, and the higher-level <a href=https://github.com/fchollet/keras target=_blank>Keras</a> framework which uses TensorFlow/CNTK as backends for easy deep learning model training.</p><p>As a bonus to the framework updates, Google <a href=https://cloudplatform.googleblog.com/2017/09/introducing-faster-GPUs-for-Google-Compute-Engine.html target=_blank>recently released</a> the newest generation of NVIDIA cloud GPUs, the Pascal-based P100, onto <a href=https://cloud.google.com/compute/ target=_blank>Google Compute Engine</a> which touts an up-to-10x performance increase to the current K80 GPUs used in cloud computing. As a bonus bonus, Google recently <a href=https://cloudplatform.googleblog.com/2017/11/new-lower-prices-for-GPUs-and-preemptible-Local-SSDs.html target=_blank>cut the prices</a> of both K80 and P100 GPU instances by up to 36%.</p><p>The results of my earlier benchmarks favored <a href=https://cloud.google.com/preemptible-vms/ target=_blank>preemptible</a> instances with many CPUs as the most cost efficient option (where a preemptable instance can only last for up to 24 hours and could end prematurely). A 36% price cut to GPU instances, in addition to the potential new benefits offered by software and GPU updates, however, might be enough to tip the cost-efficiency scales back in favor of GPUs. It&rsquo;s a good idea to rerun the experiment with updated VMs and see what happens.</p><h2 id=benchmark-setup>Benchmark Setup</h2><p>As with the original benchmark, I set up a <a href=https://github.com/minimaxir/keras-cntk-docker target=_blank>Docker container</a> containing the deep learning frameworks (based on cuDNN 6, the latest version of cuDNN natively supported by the frameworks) that can be used to train each model independently. The <a href=https://github.com/minimaxir/keras-cntk-benchmark/tree/master/v2/test_files target=_blank>Keras benchmark scripts</a> run on the containers are based off of <em>real world</em> use cases of deep learning.</p><p>The 6 hardware/software configurations and Google Compute Engine <a href=https://cloud.google.com/compute/pricing target=_blank>pricings</a> for the tests are:</p><ul><li>A K80 GPU (attached to a <code>n1-standard-1</code> instance), tested with both TensorFlow (1.4) and CNTK (2.2): <strong>$0.4975 / hour</strong>.</li><li>A P100 GPU (attached to a <code>n1-standard-1</code> instance), tested with both TensorFlow and CNTK: <strong>$1.5075 / hour</strong>.</li><li>A preemptable <code>n1-highcpu-32</code> instance, with 32 vCPUs based on the Intel Skylake architecture, tested with TensorFlow only: <strong>$0.2400 / hour</strong></li><li>A preemptable <code>n1-highcpu-16</code> instance, with 16 vCPUs based on the Intel Skylake architecture, tested with TensorFlow only: <strong>$0.1200 / hour</strong></li></ul><p>A single K80 GPU uses &frac12; a GPU board while a single P100 uses a full GPU board, which in an ideal world would suggest that the P100 is twice as fast at the K80 at minimum. But even so, the P100 configuration is about 3 times as expensive, so even if a model is trained in half the time, it may not necessarily be cheaper with the P100.</p><p>Also, the CPU tests use TensorFlow <em>as installed via the recommended method</em> through pip, since compiling the TensorFlow binary from scratch to take advantage of CPU instructions as <a href=http://minimaxir.com/2017/07/cpu-or-gpu/ target=_blank>with my previous test</a> is not a pragmatic workflow for casual use.</p><h2 id=benchmark-results>Benchmark Results</h2><p>When a fresh-out-of-a-AI-MOOC engineer wants to experiment with deep learning in the cloud, typically they use a K80 + TensorFlow setup, so we&rsquo;ll use that as the <em>base configuration</em>.</p><p>For each model architecture and software/hardware configuration, I calculate the <strong>total training time relative to the base configuration instance training</strong> for running the model training for the provided test script. In all cases, the P100 GPU <em>should</em> perform better than the K80, and 32 vCPUs <em>should</em> train faster than 16 vCPUs. The question is how <em>much</em> faster?</p><p>Let&rsquo;s start using the <a href=http://yann.lecun.com/exdb/mnist/ target=_blank>MNIST dataset</a> of handwritten digits plus the common multilayer perceptron (MLP) architecture, with dense fully-connected layers. Lower training time is better.</p><p><img src=/img/benchmark-gpus/dl-cpu-gpu-5.png alt></p><p>For this task, CNTK appears to be more effective than TensorFlow. Indeed, the P100 is faster than the K80 for the corresponding framework, although it&rsquo;s not a dramatic difference. However, since the task is simple, the CPU performance is close to that of the GPU, which implies that the GPU is not as cost effective for a simple architecture.</p><p>For each model architecture and configuration, I calculate a <strong>normalized training cost relative to the cost of the base configuration training</strong>. Because GCE instance costs are prorated, we can simply calculate experiment cost by multiplying the total number of seconds the experiment runs by the cost of the instance (per second).</p><p><img src=/img/benchmark-gpus/dl-cpu-gpu-6.png alt></p><p>Unsurprisingly, CPUs are more cost effective. However, the P100 is more cost <em>ineffective</em> for this task than the K80.</p><p>Now, let&rsquo;s look at the same dataset with a convolutional neural network (CNN) approach for digit classification. Since CNNs are typically used for computer vision tasks, new graphic card architectures are optimized for CNN workflows, so it will be interesting to see how the P100 performs compared to the K80:</p><p><img src=/img/benchmark-gpus/dl-cpu-gpu-7.png alt></p><p><img src=/img/benchmark-gpus/dl-cpu-gpu-8.png alt></p><p>Indeed, the P100 is twice as fast and the K80, but due to the huge cost premium, it&rsquo;s not cost effective for this simple task. However, CPUs do not perform well on this task either, so notably the base configuration is the best configuration.</p><p>Let&rsquo;s go deeper with CNNs and look at the <a href=https://www.cs.toronto.edu/%7Ekriz/cifar.html target=_blank>CIFAR-10</a> image classification dataset, and a model which utilizes a deep covnet + a multilayer perceptron and ideal for image classification (similar to the <a href=https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3 target=_blank>VGG-16</a> architecture).</p><p><img src=/img/benchmark-gpus/dl-cpu-gpu-9.png alt></p><p><img src=/img/benchmark-gpus/dl-cpu-gpu-10.png alt></p><p>Similar results to that of a normal MLP. Nothing fancy.</p><p>The Bidirectional long-short-term memory (LSTM) architecture is great for working with text data like IMDb reviews. When I did <a href=http://minimaxir.com/2017/06/keras-cntk/ target=_blank>my first benchmark article</a>, I noticed that CNTK performed significantly better than TensorFlow, as <a href="https://news.ycombinator.com/item?id=14538086" target=_blank>commenters on Hacker News</a> noted that TensorFlow uses an inefficient implementation of the LSTM on the GPU.</p><p><img src=/img/benchmark-gpus/cntk-old.png alt></p><p>However, with Keras&rsquo;s <a href=https://keras.io/layers/recurrent/#cudnnlstm target=_blank>new CuDNNRNN layers</a> which leverage cuDNN, this inefficiency may be fixed, so for the K80/P100 TensorFlow GPU configs, I use a CuDNNLSTM layer instead of a normal LSTM layer. So let&rsquo;s take another look:</p><p><img src=/img/benchmark-gpus/dl-cpu-gpu-1.png alt></p><p><img src=/img/benchmark-gpus/dl-cpu-gpu-2.png alt></p><p><em>WOAH.</em> TensorFlow is now more than <em>three times as fast</em> than CNTK! (And compared against my previous benchmark, TensorFlow on the K80 w/ the CuDNNLSTM is about <em>7x as fast</em> as it once was!) Even the CPU-only versions of TensorFlow are faster than CNTK on the GPU now, which implies significant improvements in the ecosystem outside of the CuDNNLSTM layer itself. (And as a result, CPUs are still more cost efficient)</p><p>Lastly, LSTM text generation of <a href=https://en.wikipedia.org/wiki/Friedrich_Nietzsche target=_blank>Nietzsche&rsquo;s</a> <a href=https://s3.amazonaws.com/text-datasets/nietzsche.txt target=_blank>writings</a> follows similar patterns to the other architectures, but without the drastic hit to the GPU.</p><p><img src=/img/benchmark-gpus/dl-cpu-gpu-11.png alt></p><p><img src=/img/benchmark-gpus/dl-cpu-gpu-12.png alt></p><h2 id=conclusions>Conclusions</h2><p>The biggest surprise of these new benchmarks is that there is no configuration where the P100 is the most cost-effective option, even though the P100 is indeed faster than the K80 in all tests. Although per <a href=https://developer.nvidia.com/cudnn target=_blank>the cuDNN website</a>, there is apparently only a 2x speed increase between the performance of the K80 and P100 using cuDNN 6, which is mostly consistent with the results of my benchmarks:</p><p><img src=/img/benchmark-gpus/cudnn.png alt></p><p>I did not include a multi-GPU configuration in the benchmark data visualizations above using Keras&rsquo;s new <code>multi_gpu_model</code> <a href=https://keras.io/utils/#multi_gpu_model target=_blank>function</a> because in my testing, the multi-GPU training <em>was equal to or worse than a single GPU</em> in all tests.</p><p>Taking these together, it&rsquo;s possible that the overhead introduced by parallel, advanced architectures <em>eliminates the benefits</em> for &ldquo;normal&rdquo; deep learning workloads which do not fully saturate the GPU. Rarely do people talk about diminishing returns in GPU performance with deep learning.</p><p>In the future, I want to benchmark deep learning performance against more advanced deep learning use cases such as <a href=https://en.wikipedia.org/wiki/Reinforcement_learning target=_blank>reinforcement learning</a> and deep CNNs like <a href=https://github.com/tensorflow/models/tree/master/research/inception target=_blank>Inception</a>. But that doesn&rsquo;t mean these benchmarks are not relevant; as stated during the benchmark setup, the GPUs were tested against typical deep learning use cases, and now we see the tradeoffs that result.</p><p>In all, with the price cuts on GPU instances, cost-performance is often <em>on par</em> with preemptable CPU instances, which is an advantage if you want to train models faster and not risk the instance being killed unexpectedly. And there is still a lot of competition in this space: <a href=https://www.amazon.com target=_blank>Amazon</a> offers a <code>p2.xlarge</code> <a href=https://aws.amazon.com/ec2/spot/ target=_blank>Spot Instance</a> with a K80 GPU for $0.15-$0.20 an hour, less than half of the corresponding Google Compute Engine K80 GPU instance, although with <a href=https://aws.amazon.com/ec2/spot/details/ target=_blank>a few bidding caveats</a> which I haven&rsquo;t fully explored yet. Competition will drive GPU prices down even further, and training deep learning models will become even easier.</p><p>And as the cuDNN chart above shows, things will get <em>very</em> interesting once Volta-based GPUs like the V100 are generally available and the deep learning frameworks support cuDNN 7 by default.</p><p><strong>UPDATE 12/17</strong>: <em>As pointed out by <a href="https://news.ycombinator.com/item?id=15941682" target=_blank>dantiberian on Hacker News</a>, Google Compute Engine now supports <a href=https://cloud.google.com/compute/docs/instances/preemptible#preemptible_with_gpu target=_blank>preemptible GPUs</a>, which was apparently added after this post went live. Preemptable GPUs are exactly half the price of normal GPUs (for both K80s and P100s; $0.73/hr and $0.22/hr respectively), so they&rsquo;re about double the cost efficiency (when factoring in the cost of the base preemptable instance), which would put them squarely ahead of CPUs in all cases. (and since the CPU instances used here were also preemptable, it&rsquo;s apples-to-apples)</em></p><hr><p><em>All scripts for running the benchmark are available in <a href=https://github.com/minimaxir/keras-cntk-benchmark/tree/master/v2 target=_blank>this GitHub repo</a>. You can view the R/ggplot2 code used to process the logs and create the visualizations in <a href=http://minimaxir.com/notebooks/benchmark-gpus/ target=_blank>this R Notebook</a>.</em></p><div class="alert alert-note"><div>If you liked this blog post, I have set up a <a href=https://www.patreon.com/minimaxir target=_blank>Patreon</a> to fund my machine learning/deep learning/software/hardware needs for my future crazy yet cool projects, and any monetary contributions to the Patreon are appreciated and will be put to good creative use.</div></div></div><div class=article-tags><a class="badge badge-light" href=/tags/python/>Python</a></div><div class="media author-card" itemscope itemtype=http://schema.org/Person><img class="portrait mr-3" src="https://s.gravatar.com/avatar/28f09e3deff62333b3f32f19d3971d46?s=200')" itemprop=image alt=Avatar><div class=media-body><h5 class=card-title itemprop=name><a href=https://minimaxir.com/>Max Woolf</a></h5><h6 class=card-subtitle>Data Scientist at BuzzFeed</h6><p class=card-text itemprop=description>Ex-Apple. Carnegie Mellon graduate. Plotter of pretty charts. Former TechCrunch comment troll.</p><ul class=network-icon aria-hidden=true><li><a itemprop=sameAs href=https://twitter.com/minimaxir target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a itemprop=sameAs href=https://linkedin.com/in/minimaxir target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a itemprop=sameAs href=https://youtube.com/minimaxir target=_blank rel=noopener><i class="fab fa-youtube"></i></a></li><li><a itemprop=sameAs href=https://twitch.tv/minimaxir target=_blank rel=noopener><i class="fab fa-twitch"></i></a></li><li><a itemprop=sameAs href=mailto:max@minimaxir.com><i class="fas fa-envelope"></i></a></li></ul></div></div><div class=article-widget><div class=hr-light></div><h3>Related</h3><ul><li><a href=/2017/11/magic-the-gifening/>Making Magic: the GIFening</a></li><li><a href=/2017/06/reddit-deep-learning/>Predicting the Success of a Reddit Submission with Deep Learning and Keras</a></li><li><a href=/2017/06/r-notebooks/>Advantages of Using R Notebooks For Data Analysis Instead of Jupyter Notebooks</a></li><li><a href=/2016/05/wordclouds/>Creating Stylish, High-Quality Word Clouds Using Python and Font Awesome Icons</a></li><li><a href=/2015/07/facebook-scraper/>How to Scrape Data From Facebook Page Posts for Statistical Analysis</a></li></ul></div><div class=article-widget><div class=post-nav><div class=post-nav-item><div class=meta-nav>Next</div><a href=/2018/02/stack-overflow-questions/ rel=next>A Visual Overview of Stack Overflow&#39;s Question Tags</a></div><div class=post-nav-item><div class=meta-nav>Previous</div><a href=/2017/11/magic-the-gifening/ rel=prev>Making Magic: the GIFening</a></div></div></div><section id=comments><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"minimaxir"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js></script><script>hljs.initHighlightingOnLoad();</script><script src=/js/academic.min.bc1d5e4f014b8d38d75521ad1ae2ab18.js></script><div class=container><footer class=site-footer><p class=powered-by>Copyright Max Woolf &copy; 2019 &middot;
Powered by the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a>.
<span class=float-right aria-hidden=true><a href=# id=back_to_top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&times;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>