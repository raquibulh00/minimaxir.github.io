<!DOCTYPE html>
<html lang="en" class="no-js">

  <!-- HEAD -->
<head>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>Pretrained Character Embeddings for Deep Learning and Automatic Text Generation</title>
<meta property="article:author" content="https://www.facebook.com/max.woolf" />
<meta name="description" content="Deep learning is the biggest, often misapplied buzzword nowadays for getting pageviews on blogs. As a result, there have been a lot of shenanigans lately wit...">
<link href="/rss.xml" rel="alternate" title="minimaxir | Max Woolf's Blog" type="application/atom+xml">
<link rel="canonical" href="http://minimaxir.com/2017/04/char-embeddings/">

<!-- Mobile Metas -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">


 <meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@minimaxir">
<meta name="twitter:creator" content="@minimaxir">

  <meta name="twitter:title" content="Pretrained Character Embeddings for Deep Learning and Automatic Text Generation">


  <meta name="twitter:url" content="http://minimaxir.com/2017/04/char-embeddings/">


  <meta name="twitter:description" content="Keras + TensorFlow + Pretrained character embeddings makes text generation a breeze.">


  <meta name="twitter:image:src" content="http://minimaxir.com/img/char-tsne-embed.png">

 
 <meta content="minimaxir | Max Woolf's Blog" property="og:site_name">

  <meta content="Pretrained Character Embeddings for Deep Learning and Automatic Text Generation" property="og:title">


  <meta content="article" property="og:type">


  <meta content="Keras + TensorFlow + Pretrained character embeddings makes text generation a breeze." property="og:description">


  <meta content="http://minimaxir.com/2017/04/char-embeddings/" property="og:url">



  <meta content="http://minimaxir.com/img/char-tsne-embed.png" property="og:image">


  
  <meta content="Visualization" property="article:section">
  


  



      <!-- Add to homescreen for Chrome on Android -->
    <meta name="mobile-web-app-capable" content="yes">

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="white">
    <meta name="apple-mobile-web-app-title" content="minimaxir">
    <link rel="apple-touch-icon-precomposed" href="apple-touch-icon-precomposed.png">

<link rel="apple-touch-icon" sizes="57x57" href="http://minimaxir.com/favicon/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="http://minimaxir.com/favicon/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="http://minimaxir.com/favicon/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="http://minimaxir.com/favicon/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="http://minimaxir.com/favicon/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="http://minimaxir.com/favicon/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="http://minimaxir.com/favicon/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="http://minimaxir.com/favicon/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="http://minimaxir.com/favicon/apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="http://minimaxir.com/favicon/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://minimaxir.com/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="http://minimaxir.com/favicon/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="http://minimaxir.com/favicon/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="http://minimaxir.com/favicon/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<!-- BEGIN GLOBAL MANDATORY STYLES -->
  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,400,400italic,700' rel='stylesheet' type='text/css'>

<link href="/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/css/font-awesome.min.css" rel="stylesheet" type="text/css"/>
<link href="/css/minimaxir.css" rel="stylesheet" type="text/css"/>
<!-- END GLOBAL MANDATORY STYLES -->

<!-- BEGIN THEME STYLES -->
<link href="/css/global.css" rel="stylesheet" type="text/css"/>
<!-- END THEME STYLES -->

<!-- BEGIN JQUERY -->
<script type="text/javascript" src="/js/jquery.min.js"></script>
<!-- END JQUERY -->



  <script>
   $(document).ready(function() {
 
  // LinkedIn
	$( ".social.linkedin" ).children().each(function(index,value) {
	var linkedin_button = $(this);
	var url = linkedin_button.attr("data");

  jQuery.getJSON("http://www.linkedin.com/countserv/count/share?url="+url+"&format=jsonp&callback=?", function(data) {
  linkedin_button.html("<i class=\"fa fa-linkedin\"></i><span class=\"count hidden-xs\"> " + data.count + "</span>");
  });
  });
  });
  </script>
  
</head>
<!-- END HEAD -->
  <!-- WRAPPER -->
<div class="wrapper">
    <!--========== HEADER TRANSPARENT ==========-->
    <header class="header-transparent header-transparent-bb navbar-fixed-top header-sticky">
        <!-- Navbar -->
        <nav class="navbar mega-menu" role="navigation">
            <div class="container">
                <!-- Brand and toggle get grouped for better mobile display -->
                <div class="menu-container">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="toggle-icon"></span>
                    </button>

                    <!-- Navbar Actions -->
                    <div class="navbar-actions">

                    </div>
                    <!-- End Navbar Actions -->

                    <!-- Logo -->
                    <div class="navbar-logo">
                        <a class="navbar-logo-wrap" href="/">
                            <img class="navbar-logo-img navbar-logo-img-white" src="/minimaxir-light.png" alt="minimaxir">
                            <img class="navbar-logo-img navbar-logo-img-dark" src="/minimaxir-dark.png" alt="minimaxir">
                        </a>
                    </div>
                    <!-- End Logo -->
                </div>

                <!-- Collect the nav links, forms, and other content for toggling -->
                <div class="collapse navbar-collapse nav-collapse">
                    <div class="menu-container">
                        <ul class="nav navbar-nav">

                            <li class="nav-item">
                                <a class="nav-item-child" href="/about">
                                    About
                                </a>
                            </li>

                            <li class="nav-item">
                                <a class="nav-item-child" href="/portfolio">
                                    Code Portfolio
                                </a>


                            </li>
                            <li class="nav-item">
                                <a class="nav-item-child" href="/data-portfolio">
                                    Data Portfolio
                                </a>


                            </li>
                             <li class="nav-item">
                                <a class="nav-item-child" href="https://www.patreon.com/minimaxir" target="_blank">
                                    Patreon
                                </a>


                            </li>

                        </ul>
                    </div>
                </div>
                <!-- End Navbar Collapse -->
            </div>
            <!--// End Container-->
        </nav>
        <!-- Navbar -->
    </header>
    <!--========== END HEADER TRANSPARENT ==========-->

        <!--========== BREADCRUMBS V5 ==========-->
    
    
	
	
    
    
    
    <section class="breadcrumbs-v5" style="background: url('/img/char-tsne-embed.png') no-repeat center center; background-size: cover;">
    
        <div class="container">
            <h1 class="breadcrumbs-v5-title">Pretrained Character Embeddings for Deep Learning and Automatic Text Generation</h1>
            <span class="breadcrumbs-v5-subtitle">April 4, 2017
            
    
              <span class="sharing index" style="float: right">
  
		  <span class="social facebook"><a href="https://www.facebook.com/share.php?u=http://minimaxir.com/2017/04/char-embeddings/"  onclick="javascript:window.open(this.href,
  '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=350,width=600');return false;" data="http://minimaxir.com/2017/04/char-embeddings/"><i class="fa fa-facebook"></i><span class="count hidden-xs"></span></a></span>
  
		<span class="social twitter"><a href="http://twitter.com/home/?status=Pretrained Character Embeddings for Deep Learning and Automatic Text Generation - http://minimaxir.com/2017/04/char-embeddings/ - via @minimaxir"  onclick="javascript:window.open(this.href,
  '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=350,width=600');return false;" data=http://minimaxir.com/2017/04/char-embeddings/><i class="fa fa-twitter"></i><span class="count hidden-xs"></span></a> </span>

  <span  class="social linkedin"><a href="http://www.linkedin.com/shareArticle?mini=true&title=&url=http://minimaxir.com/2017/04/char-embeddings/"  onclick="javascript:window.open(this.href,
  '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=450,width=600');return false;" data=http://minimaxir.com/2017/04/char-embeddings/><i class="fa fa-linkedin"></i><span class="count hidden-xs"> -</span></a> </span>
   
			</span>
            
            </span>
        </div>
    </section>
    
    
    <!--========== END BREADCRUMBS V5 ==========-->
  
  <!-- BEGIN BODY -->
<body>

  
      <!--========== BACKGROUND COLOR SKY LIGHT ==========-->
    <div class="bg-color-sky-light">

        
        <!--========== PAGE CONTENT ==========-->
        <div class="content-sm container">
        
            
			<div class="row">
			<div class="col-md-12 md-margin-b-50">
			
    <div style="row">
        <div class="col-md-8 col-md-offset-2 col-sm-12 col-sm-offset-0 margin-b-20">
            <img src="/img/char-embeddings/batch-losses.png" width="100%" style="border: 1px solid #eaeaea;"></img>
        </div>
    </div>

			</div>
			</div>
			
		
        
            <div class="row">
                <div class="col-md-9 md-margin-b-50">
                    <!-- Blog Grid -->
                    <article class="blog-grid margin-b-30">
                        <!-- Image -->
                        <!--<img class="img-responsive" src="/assets/img/1920x1080/20.jpg" alt="">-->
                        <!-- End Image -->

                        <!-- Blog Grid Content -->
						<div class="blog-grid-content">
						<p>Deep learning is the biggest, <a href="http://approximatelycorrect.com/2017/03/28/the-ai-misinformation-epidemic/">often misapplied</a> buzzword nowadays for getting pageviews on blogs. As a result, there have been a lot of shenanigans lately with deep learning thought pieces and how deep learning can solve <em>anything</em> and make childhood sci-fi dreams come true.</p>

<p>I&rsquo;m not a fan of <a href="http://tvtropes.org/pmwiki/pmwiki.php/Main/ClarkesThirdLaw">Clarke&rsquo;s Third Law</a>, so I spent some time checking out deep learning myself. As it turns out, with modern deep learning tools like <a href="https://github.com/fchollet/keras">Keras</a>, a higher-level framework on top of the popular <a href="https://www.tensorflow.org">TensorFlow</a> framework, deep learning is <strong>easy to learn and understand</strong>. Yes, easy. And it <em>definitely</em> does not require a PhD, or even a Computer Science undergraduate degree, to implement models or make decisions based on the output.</p>

<p>However, let&rsquo;s try something more expansive than the stereotypical deep learning tutorials.</p>

<h2>Characters Welcome</h2>

<p>Word embeddings have been a popular machine learning trick nowadays. By using an algorithm such as <a href="https://en.wikipedia.org/wiki/Word2vec">Word2vec</a>, you can obtain a numeric representation of a word, and use those values to create numeric representations of higher-level representations like sentences/paragraphs/documents/etc.</p>

<p><img src="/img/char-embeddings/word-vectors.png" alt=""></p>

<p>However, generating word vectors for datasets can be computationally expensive (see <a href="http://minimaxir.com/2016/08/clickbait-cluster/">my earlier post</a> which uses Apache Spark/Word2vec to create sentence vectors at scale quickly). The academic way to work around this is to use pretrained word embeddings, such as <a href="https://nlp.stanford.edu/projects/glove/">the GloVe vectors</a> collected by researchers at Stanford NLP. However, GloVe vectors are huge; the largest one (840 billion tokens at 300D) is 5.65 GB on disk and may hit issues when loaded into memory on less-powerful computers.</p>

<p>Why not work <em>backwards</em> and calculate <em>character</em> embeddings? Then you could calculate a relatively few amount of vectors which would easily fit into memory, and use those to derive word vectors, which can then be used to derive the sentence/paragraph/document/etc vectors. But training character embeddings traditionally is significantly more computationally expensive since there are 5-6x the amount of tokens, and I don&rsquo;t have access to the supercomputing power of Stanford researchers.</p>

<p>Why not use the <em>existing</em> pretrained word embeddings to extrapolate the corresponding character embeddings within the word? Think &ldquo;<a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag-of-words</a>,&rdquo; except &ldquo;bag-of-characters.&rdquo; For example, from the embeddings from the word &ldquo;the&rdquo;, we can infer the embeddings for &ldquo;t&rdquo;, &ldquo;h,&rdquo; and &ldquo;e&rdquo; from the parent word, and average the t/h/e vectors from <em>all</em> words/tokens in the dataset corpus. (For this post, I will only look at the 840B/300D dataset since that is the only one with capital letters, which are rather important. If you want to use a dataset with smaller dimensionality, apply <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a> on the final results)</p>

<p>I wrote a <a href="https://github.com/minimaxir/char-embeddings/blob/master/create_embeddings.py">simple Python script</a> that takes in the specified pretrained word embeddings and does just that, <a href="https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt">outputting the character embeddings</a> in the same format. (for simplicity, only ASCII characters are included; the <a href="https://en.wikipedia.org/wiki/Extended_ASCII">extended ASCII characters</a> are  intentionally omitted due to compatibility reasons. Additionally, by construction, space and newline characters are not represented in the derived dataset.)</p>

<p><img src="/img/char-embeddings/char-embeddings.png" alt=""></p>

<p>You may be thinking that I&rsquo;m cheating. So let&rsquo;s set a point-of-reference. Colin Morris <a href="http://colinmorris.github.io/blog/1b-words-char-embeddings">found</a> that when 16D character embeddings from a model used in Google&rsquo;s <a href="https://arxiv.org/abs/1312.3005">One Billion Word Benchmark</a> are projected into a 2D space via t-SNE, patterns emerge: digits are close, lowercase and uppercase letters are often paired, and punctuation marks are loosely paired.</p>

<p><img src="/img/char-embeddings/tsne_embeddings.png" alt=""></p>

<p>Let&rsquo;s do that for my derived character embeddings, but with <a href="https://www.r-project.org">R</a> and <a href="http://docs.ggplot2.org/current/">ggplot2</a>. t-SNE is <a href="http://distill.pub/2016/misread-tsne/">difficult to use</a> for high-dimensional vectors as combinations of parameters can result in wildly different output, so let&rsquo;s try a couple projections. Here&rsquo;s what happens when my pretrained projections are preprojected from 300D to 16D via <a href="http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/">PCA whitening</a>, and setting perplexity (number of optimal neighbors) to 7.</p>

<p><img src="/img/char-embeddings/char-tsne.png" alt=""></p>

<p>The algorithm manages to separate and group lowercase, uppercase, and numerals rather distinctly. Quadrupling the dimensionality of the preprocessing step to 64D and changing perplexity to 2 generates a depiction closer to the Google model projection:</p>

<p><img src="/img/char-embeddings/char-tsne-2.png" alt=""></p>

<p>My pretrained character embeddings trick isn&rsquo;t academic, but it&rsquo;s successfully identifying realistic relationships. There might be something here worthwhile.</p>

<h2>The Coolness of Deep Learning</h2>

<p>Keras, maintained by Google employee <a href="https://twitter.com/fchollet">François Chollet</a>, is so good that it is effectively cheating in the field of machine learning, where even TensorFlow tutorials can be replaced with a single line of code. (which is important for iteration; Keras layers are effectively Lego blocks). A simple read of the <a href="https://github.com/fchollet/keras/tree/master/examples">Keras examples</a> and <a href="https://keras.io/">documentation</a> will let you reverse-engineer most the revolutionary deep learning clickbait thought pieces. Some create entire startups by changing the source dataset of the Keras examples and pitch them to investors none-the-wiser, or make very light wrappers on top the examples for teaching tutorial videos and get thousands of subscribers on YouTube.</p>

<p>I prefer to parse documentation/examples as a proof-of-concept, but never as gospel. Examples are often not the most efficient ways to implement a solution to a problem, just merely a start. In the case of Keras&rsquo;s <a href="https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py">text generator example</a>, the initial code was likely modeled after the 2015 blog post <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> by Andrej Karpathy and the corresponding project <a href="https://github.com/karpathy/char-rnn">char-rnn</a>. There have been many new developments in neural network architecture since 2015 that can improve both speed and performance of the text generation model as a whole.</p>

<h2>What Text to Generate?</h2>

<p>The Keras example uses <a href="https://en.wikipedia.org/wiki/Friedrich_Nietzsche">Nietzsche</a> writings as a data source, which I&rsquo;m not fond of because it&rsquo;s difficult to differentiate bad autogenerated Nietzsche rants from actual Nietzsche rants. What I want to generate is text with <em>rules</em>, with the algorithm being judged by how well it follows an inherent structure. My idea is to create <a href="http://magic.wizards.com/en">Magic: The Gathering</a> cards.</p>

<p><img src="/img/char-embeddings/dragon-whelp.jpg" alt=""></p>

<p>Inspired by the <a href="https://twitter.com/RoboRosewater">@RoboRosewater</a> Twitter account by Reed Milewicz and the <a href="http://www.mtgsalvation.com/forums/creativity/custom-card-creation/612057-generating-magic-cards-using-deep-recurrent-neural">corresponding research</a> and <a href="https://motherboard.vice.com/en_us/article/the-ai-that-learned-magic-the-gathering">articles</a>, I aim to see if it&rsquo;s possible to recreate the structured design creativity for myself.</p>

<p>Even if you are not familiar with Magic and its rules, you can still find the <a href="https://twitter.com/RoboRosewater/status/756198572282949632">card text</a> of RoboRosewater cards hilarious:</p>

<p><img src="/img/char-embeddings/horse.jpeg" alt=""></p>

<p>Occasionally RoboRosewater, using a weaker model, produces amusing <a href="https://twitter.com/RoboRosewater/status/689184317721960448">neural network trainwrecks</a>:</p>

<p><img src="/img/char-embeddings/carl.png" alt=""></p>

<p>More importantly, all Magic cards have an explicit structure; they have a name, mana cost in the upper-right, card type, card text, and usually a power and toughness in the bottom-right.</p>

<p>I wrote <a href="https://github.com/minimaxir/char-embeddings/blob/master/create_magic_text.py">another Python script</a> to parse all Magic card data from <a href="https://mtgjson.com">MTG JSON</a> into an encoding which matches this architecture, where each section transition has its own symbol delimiter, along with other encoding simplicities. For example, here is the card <a href="http://gatherer.wizards.com/Pages/Card/Details.aspx?multiverseid=247314">Dragon Whelp</a> in my encoding:</p>
<div class="highlight"><pre><code class="language-" data-lang="">[Dragon Whelp@{2}{R}{R}#Creature — Dragon$Flying|{R}: ~ gets +1/+0 until end of turn. If this ability has been activated four or more times this turn, sacrifice ~ at the beginning of the next end step.%2^3]
</code></pre></div>
<p>These card encodings are all combined into one .txt file, which will be fed into the model.</p>

<h2>Building and Training the Model</h2>

<p>The Keras text generation example operates by breaking a given .txt file into 40-character sequences, and the model tries to predict the 41st character by outputting a probability for each possible character (108 in this dataset). For example, if the input based on the above example is <code>[&#39;D&#39;, &#39;r&#39;, &#39;a&#39;, &#39;g&#39;, ..., &#39;D&#39;, &#39;r&#39;, &#39;a&#39;, &#39;g&#39;]</code> (with the latter Drag being part of the creature type), the model will optimize for outputting a probability of 1.0 of <code>o</code>; per the <a href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression">categorical crossentropy</a> loss function, the model is rewarded for assigning correct guesses with 1.0 probability and incorrect guesses with 0.0 probabilities, penalizing half-guesses and wrong guesses.</p>

<p>Each possible 40-character sequence is collected, however only every other third sequence is kept; this prevents the model from being able to learn card text verbatim, plus it also makes training faster. (for this model, there are about <strong>1 million</strong> sequences for the final training). The example uses only a 128-node <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">long-short-term-memory</a> (LSTM) <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural network</a> (RNN) layer, popular for incorporating a &ldquo;memory&rdquo; into a neural network model, but the example notes at the beginning it can take awhile to train before generated text is coherent.</p>

<p>There are a few optimizations we can make. Instead of supplying the characters directly to the RNN, we can first encode them using an <a href="https://keras.io/layers/embeddings/">Embedding layer</a> so the model can train character context. We can stack more layers on the RNN by adding a 2-level <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">multilayer perceptron</a>: a <a href="https://www.reddit.com/r/ProgrammerHumor/comments/5si1f0/machine_learning_approaches/">meme</a>, yes, but it helps, as the network must learn latent representations of the data. Thanks to recent developments such as <a href="https://arxiv.org/abs/1502.03167">batch normalization</a> and <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">rectified linear activations</a> for these <a href="https://keras.io/layers/core/#dense">Dense layers</a>, they can both be trained without as much computational overhead, and thanks to Keras, both can be added to a layer with a single line of code each. Lastly, we can add an auxiliary output via Keras&rsquo;s <a href="https://keras.io/models/model/">functional API</a> where the network makes a prediction based on only the output from the RNN in addition to the main output, which forces it to work smarter and ends up resulting in a <em>significant</em> improvement in loss for the main path.</p>

<p>The final architecture ends up looking like this:</p>

<p><img src="/img/char-embeddings/model.png" alt=""></p>

<p>And because we added an Embedding layer, we can load the pretrained 300D character embeds I made earlier, giving the model a good start in understanding character relationships.</p>

<p>The goal of the training is to minimize the total loss of the model. (but for evaluating model performance, we only look at the loss of the main output). The model is trained in <strong>epochs</strong>, where the model sees all the input data atleast once. During each epoch, batches of size 128 are loaded into the model and evaluated, calculating a <strong>batch loss</strong> for each; the gradients from the batch are backpropagated into the previous layers to improve them. While training with Keras, the console reports an <strong>epoch loss</strong>, which is the average of all the batch losses so far in the current epoch, allowing the user to see in real time how the model improves, and it&rsquo;s addicting.</p>

<p><img src="/img/char-embeddings/keras-training.gif" alt=""></p>

<p>Keras/TensorFlow works just fine on the CPU, but for models with a RNN, you&rsquo;ll want to consider using a GPU for performance, specifically one by nVidia. Amazon has cloud GPU instances for $0.90/hr (<a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html">not prorated</a>), but very recently, Google announced <a href="https://cloud.google.com/compute/docs/gpus/add-gpus">GPU instances</a> of the same caliber for ~$0.75/hr (prorated to the minute), which is what I used to train this model, although Google Compute Engine requires configuring the GPU drivers first. For 20 epochs, it took about 4 hours and 20 minutes to train the model while spending $3.26, which isn&rsquo;t bad as far as deep learning goes.</p>

<h2>Making Magic</h2>

<p>After each epoch, the original Keras text generation example takes a sentence from the input data as a seed and predicts the next character in the sequence according to the model, then uses the last 40 characters generated for the next character, etc. The sampling incorporates a diversity/temperature parameter which allows the model to make suboptimal decisions and select characters with lower natural probabilities, which allows for the romantic &ldquo;creativity&rdquo; popular with neural network text generation.</p>

<p>With the Magic card dataset and my tweaked model architecture, generated text is coherent <a href="https://github.com/minimaxir/char-embeddings/blob/master/output/iter-01-0_9204.txt">after the 1st epoch</a>! After about 20 epochs, training becomes super slow, but the predicted text becomes super interesting. Here are a few fun examples from a <a href="https://github.com/minimaxir/char-embeddings/blob/master/output/text_sample.txt">list of hundreds of generated cards</a>. (Note: the power/toughness values at the end of the card have issues; more on that later).</p>

<p>With low diversity, the neural network generated cards that are oddly biased toward card names which include the letter &ldquo;S&rdquo;. The card text also conforms to the rules of the game very well.</p>
<div class="highlight"><pre><code class="language-" data-lang="">[Reality Spider@{3}{G}#Creature — Elf Warrior$Whenever ~ deals combat damage to a player, put a +1/+1 counter on it.%^]
[Dark Soul@{2}{R}#Instant$~ deals 2 damage to each creature without flying.%^]
[Standing Stand@{2}{G}#Creature — Elf Shaman${1}{G}, {T}: Draw a card, then discard a card.%^]
</code></pre></div>
<p>In contrast, cards generated with high diversity hit the uncanny valley of coherence and incoherence in both text and game mechanic abuse, which is what makes them interesting. </p>
<div class="highlight"><pre><code class="language-" data-lang="">[Portrenline@{2}{R}#Sorcery$As an additional cost to cast ~, exile ~.%^]
[Clocidian Lorid@{W}{W}{W}#Instant$Regenerate each creature with flying and each player.%^]
[Icomic Convermant@{3}{G}#Sorcery$Search your library for a land card in your graveyard.%1^1]
</code></pre></div>
<p>The best-of-both-worlds cards are generated from diversity parameters between both extremes, and often have funny names.</p>
<div class="highlight"><pre><code class="language-" data-lang="">[Seal Charm@{W}{W}#Instant$Exile target creature. Its controller loses 1 life.%^]
[Shambling Assemblaster@{4}{W}#Creature — Human Cleric$When ~ enters the battlefield, destroy target nonblack creature.%1^1]
[Lightning Strength@{3}{R}#Enchantment — Aura$Enchant creature|Enchanted creature gets +3/+3 and has flying, flying, trample, trample, lifelink, protection from black and votile all damage unless you return that card to its owner's hand.%2^2]
[Skysor of Shadows@{7}{B}{B}{B}#Enchantment$As ~ enters the battlefield, choose one —|• Put a -1/-1 counter on target creature.%2^2]
[Glinding Stadiers@{4}{W}#Creature — Spirit$Protection from no creatures can't attack.%^]
[Dragon Gault@{3}{G}{U}{U}#Creature — Kraven$~'s power and toughness are 2.%2^2]
</code></pre></div>
<p>All Keras/Python code used in this blog post, along with sample Magic card output and the trained model itself, is available open-source <a href="https://github.com/minimaxir/char-embeddings">in this GitHub repository</a>. The repo additionally contains <a href="https://github.com/minimaxir/char-embeddings/blob/master/text_generator_keras_sample.py">a Python script</a> which lets you generate new cards using the model, too!</p>

<h2>Visualizing Model Performance</h2>

<p>One thing deep learning tutorials rarely mention is <em>how</em> to collect the loss data and visualize the change in loss over time. Thanks to Keras&rsquo;s <a href="https://keras.io/callbacks/">utility functions</a>, I wrote a custom model callback which collects the batch losses and epoch losses and writes them to a CSV file.</p>

<p>Using R and ggplot2, I can plot the batch loss at every 50th batch to visualize how the model converges over time.</p>

<p><img src="/img/char-embeddings/batch-losses.png" alt=""></p>

<p>After 20 epochs, the model loss ends up at about <strong>0.30</strong> which is more-than-low-enough for coherent text. As you can see, there are large diminishing returns after a few epochs, which is the hard part of training deep learning models.</p>

<p>Plotting the epoch loss over the batches makes the trend more clear.</p>

<p><img src="/img/char-embeddings/epoch-losses.png" alt=""></p>

<p>In order to prevent early convergence, we can make the model more complex (i.e. stack more layers unironically), but that has trade-offs, both in training <em>and</em> predictive speed, the latter of which is important if using deep learning in a production application.</p>

<p>Lastly, as with the Google One Billion Words benchmark, we can extract the <a href="https://github.com/minimaxir/char-embeddings/blob/master/output/char-embeddings.txt">trained character embeddings</a> from the model (now augmented with Magic card context!) and plot them again to see what has changed.</p>

<p><img src="/img/char-embeddings/char-tsne-embed.png" alt=""></p>

<p>There are more pairs of uppercase/lowercase characters, although  interestingly there isn&rsquo;t much grouping with the special characters added as section breaks in the encoding, or mechanical uppercase characters such as W/U/B/R/G/C/T.</p>

<h2>Next Steps</h2>

<p>After building the model, I did a little more research to see if others solved the power/toughness problem. Since the sentences are only 40 characters and Magic cards are much longer than 40 characters, it&rsquo;s likely that power/toughness are out-of-scope for the model and it cannot learn their exact values. Turns out that the intended solution is to use a <a href="https://github.com/billzorn/mtgencode">completely different encoding</a>, such as this one for Dragon Whelp:</p>
<div class="highlight"><pre><code class="language-" data-lang="">|5creature|4|6dragon|7|8&amp;^^/&amp;^^^|9flying\{RR}: @ gets +&amp;^/+&amp; until end of turn. if this ability has been activated four or more times this turn, sacrifice @ at the beginning of the next end step.|3{^^RRRR}|0N|1dragon whelp|
</code></pre></div>
<p>Power/toughness are generated near the <em>beginning</em> of the card. Sections are delimited by pipes, with a numeral designating the corresponding section. Instead of numerals being used card values, carets are used, which provides a more accurate <em>quantification</em> of values. With this encoding, each character has a <em>singular purpose</em> in the global card context, and their embeddings would likely generate more informative visualizations. (But as a consequence, the generated cards are harder to parse at a glance).</p>

<p>The secondary encoding highlights a potential flaw in my methodology using pretrained character embeddings. Trained machine learning models must be used apples-to-apples on similar datasets; for example, you can&rsquo;t accurately perform Twitter <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">sentiment analysis</a> on a dataset using a model trained on professional movie reviews since Tweets do not follow <a href="https://owl.english.purdue.edu/owl/resource/735/02/">AP Style</a> guidelines. In my case, the <a href="http://commoncrawl.org">Common Crawl</a>, the source of the pretrained embeddings, follows more natural text usage and would not work analogously with the atypical character usages in <em>either</em> of the Magic card encodings.</p>

<p>There&rsquo;s still a <em>lot</em> of work to be done in terms of working with both pretrained character embeddings and improving Magic card generation, but I believe there is promise. The better way to make character embeddings than my script is to do it the hard way and train then manually, maybe even at a higher dimensionality like 500D or 1000D. Likewise, for Magic model building, the <a href="https://github.com/billzorn/mtgencode#training-a-neural-net">mtg-rnn instructions</a> repo uses a large LSTM stacked on a LSTM along with 120/200-character sentences, both of which combined make training <strong>VERY</strong> slow (notably, this was the architecture of the <a href="https://github.com/fchollet/keras/commit/d2b229df2ea0bab712379c418115bc44508bc6f9#diff-904d72bcf9fa38b32f9c1f868ff59367">very first commit</a> for the Keras text generation example, and <a href="https://github.com/fchollet/keras/commit/01d5e7bc4782daafcfa99e035c1bdbe13a985145">was changed</a> to the easily-trainable architecture). There is also promise in a <a href="http://kvfrans.com/variational-autoencoders-explained/">variational autoencoder</a> approach, such as with <a href="https://arxiv.org/abs/1702.02390">textvae</a>.</p>

<p>This work is potentially very expensive and I am strongly considering setting up a <a href="https://www.patreon.com">Patreon</a> in lieu of excess venture capital to subsidize my machine learning/deep learning tasks in the future.</p>

<p>At minimum, working with this example gave me a sufficient application of practical work with Keras, and another tool in my toolbox for data analysis and visualization. Keras makes the model-construction aspect of deep learning trivial and not scary. Hopefully, this article justifies the use of the &ldquo;deep learning&rdquo; buzzword in the headline.</p>

<p>It&rsquo;s also worth mentioning that I actually started working on automatic text generation 6 months ago using a different, non-deep-learning approach, but hit a snag and abandoned that project. With my work on Keras, I found a way around that snag, and on the same Magic dataset with the same input construction, I obtained a model loss of <strong>0.03</strong> at <strong>20% of the cloud computing cost</strong> in about the same amount of time. More on that later.</p>

<hr>

<p><em>The code for generating the R/ggplot2 data visualizations is available in this <a href="http://minimaxir.com/notebooks/char-tsne/">R Notebook</a>, and open-sourced in <a href="https://github.com/minimaxir/char-tsne-visualization">this GitHub Repository.</a></em></p>

<p><em>You are free to use the automatic text generation scripts and data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!</em></p>

                        
                        
                        <div class="alert alert-warning" role="alert">

     <p>If you liked this blog post, I have set up a <a href="https://www.patreon.com/minimaxir">Patreon</a> to fund my machine learning/deep learning/software/hardware needs for my future crazy yet cool projects, and any monetary contributions to the Patreon are appreciated and will be put to good creative use.</p>

    
</div>
                        
						
            			 <div class="breadcrumbs-v5-subtitle sharing-post" style="color: #999 !important; padding-top:10px; border-top: 1px solid #eee">Share this article!


              <span class="sharing index" style="float: right; color: #999 !important;">
  
		  <span class="social facebook"><a href="https://www.facebook.com/share.php?u=http://minimaxir.com/2017/04/char-embeddings/"  onclick="javascript:window.open(this.href,
  '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=350,width=600');return false;" data="http://minimaxir.com/2017/04/char-embeddings/"><i class="fa fa-facebook"></i><span class="count hidden-xs"></span></a></span>
  
		<span class="social twitter"><a href="http://twitter.com/home/?status=Pretrained Character Embeddings for Deep Learning and Automatic Text Generation - http://minimaxir.com/2017/04/char-embeddings/ - via @minimaxir"  onclick="javascript:window.open(this.href,
  '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=350,width=600');return false;" data=http://minimaxir.com/2017/04/char-embeddings/><i class="fa fa-twitter"></i><span class="count hidden-xs"></span></a> </span>

  <span  class="social linkedin"><a href="http://www.linkedin.com/shareArticle?mini=true&title=&url=http://minimaxir.com/2017/04/char-embeddings/"  onclick="javascript:window.open(this.href,
  '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=450,width=600');return false;" data=http://minimaxir.com/2017/04/char-embeddings/><i class="fa fa-linkedin"></i><span class="count hidden-xs"> -</span></a> </span>

   </span>
   </div>
						</div>
                        <!-- End Blog Grid Content -->
                        
                        
                        
                    </article>
                    <!-- End Blog Grid -->

 					

                    <!-- Blog Comment -->
                    <div class="blog-comments bg-color-white margin-b-30">
                        <div class="blog-single-post-content">

						

<script type="text/javascript">
      var disqus_shortname = 'minimaxir';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://minimaxir.com/2017/04/char-embeddings/';
        var disqus_url = 'http://minimaxir.com/2017/04/char-embeddings/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>

<div id="disqus_thread"></div>



                        </div>
                    </div>
                    <!-- End Blog Comment -->
                    
                    
                </div>

			<!--========== BLOG SIDEBAR ==========-->
<div class="col-md-3">



    <!-- Blog Author -->
    <div class="blog-sidebar margin-b-30">
        <div class="blog-sidebar-heading">
            <h4 class="blog-sidebar-heading-title">Author</h4>
        </div>

        <img src="/MaxFB.jpg" title="Stop mousing over me! I'm not a XKCD comic!" class="img-responsive hidden-xs hidden-sm"></img>

        <div class="blog-sidebar-content">
            <!-- Blog Author Bio -->
             <p><strong>Max Woolf (@minimaxir)</strong> is an Associate Data Scientist at BuzzFeed in San Francisco. He is also an ex-Apple employee and Carnegie Mellon University graduate.</p>

<p>In his spare time, Max uses <a href="https://www.python.org/">Python</a> to gather data from public APIs and <a href="http://ggplot2.org/">ggplot2</a> to plot plenty of pretty charts from that data. On special occasions, he uses <a href="https://github.com/fchollet/keras">Keras</a> for fancy deep learning projects.</p>

<p>You can learn more about Max <a href="/about">here</a>, view his data analysis portfolio <a href="/data-portfolio">here</a>, or view his coding portfolio <a href="/portfolio">here</a>. </p>


            <ul class="list-inline" style="padding-top:10px; margin-top: 20px; border-top: 1px solid #ebeef6">
                <li class="theme-icons-wrap"><a title="Facebook" href="https://facebook.com/max.woolf" target="_blank"><i class="theme-icons theme-icons-grey-light-bg theme-icons-xs radius-circle fa fa-facebook"></i></a></li>
                <li class="theme-icons-wrap"><a title="Twitter" href="https://twitter.com/minimaxir" target="_blank"><i class="theme-icons theme-icons-grey-light-bg theme-icons-xs radius-circle fa fa-twitter"></i></a></li>
                <li class="theme-icons-wrap"><a title="LinkedIn" href="https://linkedin.com/in/minimaxir" target="_blank"><i class="theme-icons theme-icons-grey-light-bg theme-icons-xs radius-circle fa fa-linkedin"></i></a></li>
                <li class="theme-icons-wrap"><a title="E-Mail" href="mailto:max@minimaxir.com"><i class="theme-icons theme-icons-grey-light-bg theme-icons-xs radius-circle fa fa-envelope"></i></a></li>
                <li class="theme-icons-wrap"><a title="GitHub" href="https://github.com/minimaxir" target="_blank"><i class="theme-icons theme-icons-grey-light-bg theme-icons-xs radius-circle fa fa-github"></i></a></li>
                <li class="theme-icons-wrap"><a title="YouTube" href="https://youtube.com/minimaxir" target="_blank"><i class="theme-icons theme-icons-grey-light-bg theme-icons-xs radius-circle fa fa-youtube"></i></a></li>
                <li class="theme-icons-wrap"><a title="RSS" href="/rss.xml"><i class="theme-icons theme-icons-grey-light-bg theme-icons-xs radius-circle fa fa-rss"></i></a></li>
            </ul>
            <!-- End Blog Author Bio -->
        </div>
    </div>
    <!-- End Blog Author -->

    <!-- Blog Recent Posts -->
    <div class="blog-sidebar margin-b-30">
        <div class="blog-sidebar-heading">
            <h4 class="blog-sidebar-heading-title">Recent Posts</h4>
        </div>
        <div class="blog-sidebar-content" style="position: relative; overflow: visible;">

            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="/2018/03/basketball-shots/">Visualizing One Million NCAA Basketball Shots</a></h5>
                    <small class="latest-tuts-content-time"> Mar 19, 2018</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="/2018/02/stack-overflow-questions/">A Visual Overview of Stack Overflow's Question Tags</a></h5>
                    <small class="latest-tuts-content-time"> Feb 9, 2018</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="/2017/11/benchmark-gpus/">Benchmarking Modern GPUs for Maximum Cloud Cost Efficiency in Deep Learning</a></h5>
                    <small class="latest-tuts-content-time"> Nov 28, 2017</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="/2017/11/magic-the-gifening/">Making Magic: the GIFening</a></h5>
                    <small class="latest-tuts-content-time"> Nov 7, 2017</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="/2017/08/ggplot2-web/">How to Make High Quality Data Visualizations for Websites With R and ggplot2</a></h5>
                    <small class="latest-tuts-content-time"> Aug 14, 2017</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="/2017/07/cpu-or-gpu/">Benchmarking TensorFlow on Cloud CPUs: Cheaper Deep Learning than Cloud GPUs</a></h5>
                    <small class="latest-tuts-content-time"> Jul 5, 2017</small>
                </div>
            </article>
            
        </div>
        <!-- End Blog Recent Posts -->
    </div>

    <!-- GitHub Top Repositories -->
    <div class="blog-sidebar margin-b-30">
        <div class="blog-sidebar-heading">
            <h4 class="blog-sidebar-heading-title">GitHub Code Repositories</h4>
        </div>
        <div class="blog-sidebar-content" style="position: relative; overflow: visible;">

            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="http://github.com/minimaxir/big-list-of-naughty-strings" target="_blank">Big List of Naughty Strings</a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-star"></i> 21,523+</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="http://github.com/minimaxir/facebook-page-post-scraper" target="_blank">Facebook Page Post Scraper</a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-star"></i> 1,436+</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="http://github.com/minimaxir/textgenrnn" target="_blank">textgenrnn</a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-star"></i> 234+</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="http://github.com/minimaxir/reactionrnn" target="_blank">reactionrnn</a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-star"></i> 201+</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="http://github.com/minimaxir/system-dashboard" target="_blank">Multiplatform System Dashboard</a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-star"></i> 100+</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="http://github.com/minimaxir/tritonize" target="_blank">Tritonize</a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-star"></i> 9+</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="http://github.com/minimaxir/stylistic-word-clouds" target="_blank">Stylistic Word Clouds</a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-star"></i> 30+</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="http://github.com/minimaxir/char-embeddings" target="_blank">Pretrained Character Embeddings for Deep Learning and Automatic Text Generation</a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-star"></i> 8+</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="http://github.com/minimaxir/get-profile-data-of-repo-stargazers" target="_blank">Get GitHub Profile Data of All Stargazers of a GitHub Repo</a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-star"></i> 15+</small>
                </div>
            </article>
            
        </div>
    </div>

    <!-- End GitHub Top Repositories -->

    <!-- Data Top Repositories -->
    <div class="blog-sidebar margin-b-30">
        <div class="blog-sidebar-heading">
            <h4 class="blog-sidebar-heading-title">Data Analysis Notebooks</h4>
        </div>
        <div class="blog-sidebar-content" style="position: relative; overflow: visible;">

            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="http://minimaxir.com/notebooks/imgur-decline/" target="_blank">The Decline of Imgur on Reddit and the Rise of Reddit's Native Image Hosting</a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-cogs"></i> R, ggplot2</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="http://minimaxir.com/notebooks/keras-cntk/" target="_blank">Benchmarking CNTK on Keras: is it Better at Deep Learning than TensorFlow?</a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-cogs"></i> R, ggplot2</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="http://minimaxir.com/notebooks/char-tsne/" target="_blank">Pretrained Character Embeddings for Deep Learning and Automatic Text Generation</a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-cogs"></i> R, ggplot2</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="http://minimaxir.com/notebooks/predicting-arrests/" target="_blank">Predicting And Mapping Arrest Types in San Francisco</a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-cogs"></i> R, ggplot2, LightGBM</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="http://minimaxir.com/notebooks/amazon-spark/" target="_blank">Playing with 80 Million Amazon Product Review Ratings</a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-cogs"></i> R, ggplot2, Spark</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="http://minimaxir.com/notebooks/interactive-network/" target="_blank">How to Create an Interactive WebGL Network Graph</a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-cogs"></i> R, ggplot2, plotly</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="http://minimaxir.com/notebooks/first-comment/" target="_blank">What Percent of the Top-Voted Comments in Reddit Threads Were Also 1st Comment?</a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-cogs"></i> R, ggplot2</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="https://github.com/minimaxir/clickbait-cluster/blob/master/fb_news_53D_spark.ipynb" target="_blank">Processing Clusters of Clickbait Headlines </a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-cogs"></i> Python, Spark, word2vec</small>
                </div>
            </article>
            
            <article class="latest-tuts">
                <div class="latest-tuts-content">
                    <h5 class="latest-tuts-content-title"><a href="https://github.com/minimaxir/clickbait-cluster/blob/master/fb_news_53D_plotly.ipynb" target="_blank">Visualizing Clusters of Clickbait Headlines </a></h5>
                    <small class="latest-tuts-content-time"><i class="fa fa-cogs"></i> R, plotly</small>
                </div>
            </article>
            
        </div>
    </div>

    <!-- End GitHub Top Repositories -->



</div>
<!--========== END BLOG SIDEBAR ==========-->
			</div>
            <!--// end row -->
        </div>
        <!--========== END PAGE CONTENT ==========-->

    </div>
    <!--========== END BACKGROUND COLOR SKY LIGHT ==========-->
    



  
              <div class="wrapper">
    
	
	
    	 <section class="breadcrumbs-v5 bg-position-fixed" style="background: #2c3e50;  padding: 20px 0 20px !important;">
        <div class="container">
            <div class="row hero-index">
            <div class="col-sm-6 sm-margin-b-30 hidden-xs">
                <img src="/MaxFBInvert.png" title="Stop mousing over me! I'm not a XKCD comic!" class="img-responsive" style="border-radius: 30px; margin: 0 auto; max-height: 400px"></img>
            </div>
            <div class="col-sm-6 col-xs-12">

			
			<p><strong>Max Woolf (@minimaxir)</strong> is an Associate Data Scientist at BuzzFeed in San Francisco. He is also an ex-Apple employee and Carnegie Mellon University graduate.</p>

<p>In his spare time, Max uses <a href="https://www.python.org/">Python</a> to gather data from public APIs and <a href="http://ggplot2.org/">ggplot2</a> to plot plenty of pretty charts from that data. On special occasions, he uses <a href="https://github.com/fchollet/keras">Keras</a> for fancy deep learning projects.</p>

<p>You can learn more about Max <a href="/about">here</a>, view his data analysis portfolio <a href="/data-portfolio">here</a>, or view his coding portfolio <a href="/portfolio">here</a>. </p>

  
  
  <ul class="list-inline" style="padding-top:20px; margin-top: 40px; border-top: 1px solid white;">
                        <li class="theme-icons-wrap"><a title="Facebook" href="https://facebook.com/max.woolf" target="_blank"><i class="theme-icons theme-icons-white-brd theme-icons-md radius-circle fa fa-facebook"></i></a></li>
                        <li class="theme-icons-wrap"><a title="Twitter" href="https://twitter.com/minimaxir" target="_blank"><i class="theme-icons theme-icons-white-brd theme-icons-md radius-circle fa fa-twitter"></i></a></li>
                        <li class="theme-icons-wrap"><a title="LinkedIn" href="https://linkedin.com/in/minimaxir" target="_blank"><i class="theme-icons theme-icons-white-brd theme-icons-md radius-circle fa fa-linkedin"></i></a></li>
                        <li class="theme-icons-wrap"><a title="E-Mail" href="mailto:max@minimaxir.com"><i class="theme-icons theme-icons-white-brd theme-icons-md radius-circle fa fa-envelope"></i></a></li>
                        <li class="theme-icons-wrap"><a title="GitHub" href="https://github.com/minimaxir" target="_blank"><i class="theme-icons theme-icons-white-brd theme-icons-md radius-circle fa fa-github"></i></a></li>
                        <li class="theme-icons-wrap"><a title="YouTube" href="https://youtube.com/minimaxir" target="_blank"><i class="theme-icons theme-icons-white-brd theme-icons-md radius-circle fa fa-youtube"></i></a></li>
                        <li class="theme-icons-wrap"><a title="RSS" href="/rss.xml"><i class="theme-icons theme-icons-white-brd theme-icons-md radius-circle fa fa-rss"></i></a></li>

                    </ul>
            </div>
        </div>
        </div>
    </section>
    
	

	
    

            </div>
    
    <!--========== FOOTER V8 ==========-->
    <footer id="footer" class="footer-v8">



        <!-- Copyright -->
        <div class="footer-v8-copyright">
            <div class="container text-center">
				Copyright &#169; 2018 Max Woolf. All Rights Reserved. 
                minimaxir.com is powered by Jekyll and GitHub Pages.
            </div>
        </div>
        <!-- Copyright -->
    </footer>
    <!--========== END FOOTER V8 ==========-->
</div>
<!-- END WRAPPER -->

<!-- Sidebar Content Overlay -->
<div class="sidebar-content-overlay"></div>
<!-- End Sidebar Content Overlay -->

<!-- Back To Top -->
<a href="javascript:void(0);" class="js-back-to-top back-to-top-theme"></a>
<!-- End Back To Top -->

<!--========== JAVASCRIPTS(Load javascripts at bottom, this will reduce page load time) ==========-->
<!-- BEGIN CORE PLUGINS -->
<!--[if lt IE 9]>
<script src="/assets/plugins/html5shiv.js"></script>
<script src="/assets/plugins/respond.min.js"></script>
<![endif]-->
<script type="text/javascript" src="/js/jquery.migrate.min.js"></script>
<script type="text/javascript" src="/js/bootstrap.min.js"></script>
<!-- END CORE PLUGINS -->

<!-- BEGIN PAGE LEVEL PLUGINS -->
<script type="text/javascript" src="/js/jquery.back-to-top.js"></script>
<!-- END PAGE LEVEL PLUGINS -->

<!-- BEGIN PAGE LEVEL SCRIPTS -->
<script type="text/javascript" src="/js/app.min.js"></script>
<script type="text/javascript" src="/js/header-sticky.js"></script>
<!-- END PAGE LEVEL SCRIPTS -->


<!--========== END JAVASCRIPTS ==========-->
  
</body>
<!-- END BODY -->

</html>