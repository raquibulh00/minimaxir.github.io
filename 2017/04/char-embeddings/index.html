<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.3.1"><meta name=author content="Max Woolf"><meta name=description content="Keras &#43; TensorFlow &#43; Pretrained character embeddings makes text generation a breeze."><link rel=alternate hreflang=en-us href=https://minimaxir.com/2017/04/char-embeddings/><meta name=theme-color content=#2962ff><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400italic,700|Source+Code+Pro:400,400italic,700&display=swap"><link rel=stylesheet href=/css/academic.min.4cdedb6ca5fc8a13caf3e26423bf7037.css><link rel=stylesheet href=/css/academic.59da4f61e2de6d8a5935b902fe667ab3.css><link rel=manifest href=/site.webmanifest><link rel=icon type=image/png href=/img/icon.png><link rel=apple-touch-icon type=image/png href=/img/icon-192.png><link rel=canonical href=https://minimaxir.com/2017/04/char-embeddings/><meta property=twitter:card content=summary_large_image><meta property=twitter:site content=@minimaxir><meta property=twitter:creator content=@minimaxir><meta property=article:author content=https://www.facebook.com/max.woolf><meta property=og:site_name content="Max Woolf's Blog"><meta property=og:url content=https://minimaxir.com/2017/04/char-embeddings/><meta property=og:type content=article><meta property=og:title content="Pretrained Character Embeddings for Deep Learning and Automatic Text Generation"><meta property=og:description content="Keras &#43; TensorFlow &#43; Pretrained character embeddings makes text generation a breeze."><meta property=og:image content=https://minimaxir.com/2017/04/char-embeddings/featured.png><meta property=twitter:image content=https://minimaxir.com/2017/04/char-embeddings/featured.png><meta property=og:locale content=en-us><meta property=article:published_time content=2017-04-04T06:30:00-07:00><meta property=article:modified_time content=2017-04-04T06:30:00-07:00><title>Pretrained Character Embeddings for Deep Learning and Automatic Text Generation | Max Woolf&#39;s Blog</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id=navbar-main><div class=container><a class=navbar-brand href=/>Max Woolf&#39;s Blog</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/about/><span>About</span></a></li><li class=nav-item><a class=nav-link href=/post/><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/portfolio/><span>Portfolio</span></a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=https://www.patreon.com/minimaxir target=_blank rel=noopener><span><i class="fab fa-patreon mr-1"></i>Patreon</span></a></li><li class=nav-item><a class=nav-link href=https://github.com/minimaxir target=_blank rel=noopener><span><i class="fab fa-github-alt mr-1"></i>GitHub</span></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><article class=article itemscope itemtype=http://schema.org/Article><div class="article-container pt-3"><h1 itemprop=name>Pretrained Character Embeddings for Deep Learning and Automatic Text Generation</h1><meta content="2017-04-04 06:30:00 -0700 PDT" itemprop=datePublished><meta content="2017-04-04 06:30:00 -0700 PDT" itemprop=dateModified><div class=article-metadata><span class=article-date><time>April 4, 2017</time></span>
<span class=middot-divider></span><span class=article-reading-time>15 min read</span>
<span class=middot-divider></span><span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/categories/data-science/>Data Science</a></span><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://minimaxir.com/2017/04/char-embeddings/&amp;text=Pretrained%20Character%20Embeddings%20for%20Deep%20Learning%20and%20Automatic%20Text%20Generation" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://minimaxir.com/2017/04/char-embeddings/&amp;t=Pretrained%20Character%20Embeddings%20for%20Deep%20Learning%20and%20Automatic%20Text%20Generation" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook-f"></i></a></li><li><a href="mailto:?subject=Pretrained%20Character%20Embeddings%20for%20Deep%20Learning%20and%20Automatic%20Text%20Generation&amp;body=https://minimaxir.com/2017/04/char-embeddings/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://minimaxir.com/2017/04/char-embeddings/&amp;title=Pretrained%20Character%20Embeddings%20for%20Deep%20Learning%20and%20Automatic%20Text%20Generation" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://reddit.com/submit?url=https://minimaxir.com/2017/04/char-embeddings/&amp;title=Pretrained%20Character%20Embeddings%20for%20Deep%20Learning%20and%20Automatic%20Text%20Generation" target=_blank rel=noopener class=share-btn-reddit><i class="fab fa-reddit-alien"></i></a></li></ul></div></div></div><div class=article-container><div class=article-style itemprop=articleBody><p>Deep learning is the biggest, <a href=http://approximatelycorrect.com/2017/03/28/the-ai-misinformation-epidemic/ target=_blank>often misapplied</a> buzzword nowadays for getting pageviews on blogs. As a result, there have been a lot of shenanigans lately with deep learning thought pieces and how deep learning can solve <em>anything</em> and make childhood sci-fi dreams come true.</p><p>I&rsquo;m not a fan of <a href=http://tvtropes.org/pmwiki/pmwiki.php/Main/ClarkesThirdLaw target=_blank>Clarke&rsquo;s Third Law</a>, so I spent some time checking out deep learning myself. As it turns out, with modern deep learning tools like <a href=https://github.com/fchollet/keras target=_blank>Keras</a>, a higher-level framework on top of the popular <a href=https://www.tensorflow.org target=_blank>TensorFlow</a> framework, deep learning is <strong>easy to learn and understand</strong>. Yes, easy. And it <em>definitely</em> does not require a PhD, or even a Computer Science undergraduate degree, to implement models or make decisions based on the output.</p><p>However, let&rsquo;s try something more expansive than the stereotypical deep learning tutorials.</p><h2 id=characters-welcome>Characters Welcome</h2><p>Word embeddings have been a popular machine learning trick nowadays. By using an algorithm such as <a href=https://en.wikipedia.org/wiki/Word2vec target=_blank>Word2vec</a>, you can obtain a numeric representation of a word, and use those values to create numeric representations of higher-level representations like sentences/paragraphs/documents/etc.</p><p><img src=/img/char-embeddings/word-vectors.png alt></p><p>However, generating word vectors for datasets can be computationally expensive (see <a href=http://minimaxir.com/2016/08/clickbait-cluster/ target=_blank>my earlier post</a> which uses Apache Spark/Word2vec to create sentence vectors at scale quickly). The academic way to work around this is to use pretrained word embeddings, such as <a href=https://nlp.stanford.edu/projects/glove/ target=_blank>the GloVe vectors</a> collected by researchers at Stanford NLP. However, GloVe vectors are huge; the largest one (840 billion tokens at 300D) is 5.65 GB on disk and may hit issues when loaded into memory on less-powerful computers.</p><p>Why not work <em>backwards</em> and calculate <em>character</em> embeddings? Then you could calculate a relatively few amount of vectors which would easily fit into memory, and use those to derive word vectors, which can then be used to derive the sentence/paragraph/document/etc vectors. But training character embeddings traditionally is significantly more computationally expensive since there are 5-6x the amount of tokens, and I don&rsquo;t have access to the supercomputing power of Stanford researchers.</p><p>Why not use the <em>existing</em> pretrained word embeddings to extrapolate the corresponding character embeddings within the word? Think &ldquo;<a href=https://en.wikipedia.org/wiki/Bag-of-words_model target=_blank>bag-of-words</a>,&rdquo; except &ldquo;bag-of-characters.&rdquo; For example, from the embeddings from the word &ldquo;the&rdquo;, we can infer the embeddings for &ldquo;t&rdquo;, &ldquo;h,&rdquo; and &ldquo;e&rdquo; from the parent word, and average the t/h/e vectors from <em>all</em> words/tokens in the dataset corpus. (For this post, I will only look at the 840B/300D dataset since that is the only one with capital letters, which are rather important. If you want to use a dataset with smaller dimensionality, apply <a href=https://en.wikipedia.org/wiki/Principal_component_analysis target=_blank>PCA</a> on the final results)</p><p>I wrote a <a href=https://github.com/minimaxir/char-embeddings/blob/master/create_embeddings.py target=_blank>simple Python script</a> that takes in the specified pretrained word embeddings and does just that, <a href=https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt target=_blank>outputting the character embeddings</a> in the same format. (for simplicity, only ASCII characters are included; the <a href=https://en.wikipedia.org/wiki/Extended_ASCII target=_blank>extended ASCII characters</a> are intentionally omitted due to compatibility reasons. Additionally, by construction, space and newline characters are not represented in the derived dataset.)</p><p><img src=/img/char-embeddings/char-embeddings.png alt></p><p>You may be thinking that I&rsquo;m cheating. So let&rsquo;s set a point-of-reference. Colin Morris <a href=http://colinmorris.github.io/blog/1b-words-char-embeddings target=_blank>found</a> that when 16D character embeddings from a model used in Google&rsquo;s <a href=https://arxiv.org/abs/1312.3005 target=_blank>One Billion Word Benchmark</a> are projected into a 2D space via t-SNE, patterns emerge: digits are close, lowercase and uppercase letters are often paired, and punctuation marks are loosely paired.</p><p><img src=/img/char-embeddings/tsne_embeddings.png alt></p><p>Let&rsquo;s do that for my derived character embeddings, but with <a href=https://www.r-project.org target=_blank>R</a> and <a href=http://docs.ggplot2.org/current/ target=_blank>ggplot2</a>. t-SNE is <a href=http://distill.pub/2016/misread-tsne/ target=_blank>difficult to use</a> for high-dimensional vectors as combinations of parameters can result in wildly different output, so let&rsquo;s try a couple projections. Here&rsquo;s what happens when my pretrained projections are preprojected from 300D to 16D via <a href=http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/ target=_blank>PCA whitening</a>, and setting perplexity (number of optimal neighbors) to 7.</p><p><img src=/img/char-embeddings/char-tsne.png alt></p><p>The algorithm manages to separate and group lowercase, uppercase, and numerals rather distinctly. Quadrupling the dimensionality of the preprocessing step to 64D and changing perplexity to 2 generates a depiction closer to the Google model projection:</p><p><img src=/img/char-embeddings/char-tsne-2.png alt></p><p>My pretrained character embeddings trick isn&rsquo;t academic, but it&rsquo;s successfully identifying realistic relationships. There might be something here worthwhile.</p><h2 id=the-coolness-of-deep-learning>The Coolness of Deep Learning</h2><p>Keras, maintained by Google employee <a href=https://twitter.com/fchollet target=_blank>François Chollet</a>, is so good that it is effectively cheating in the field of machine learning, where even TensorFlow tutorials can be replaced with a single line of code. (which is important for iteration; Keras layers are effectively Lego blocks). A simple read of the <a href=https://github.com/fchollet/keras/tree/master/examples target=_blank>Keras examples</a> and <a href=https://keras.io/ target=_blank>documentation</a> will let you reverse-engineer most the revolutionary deep learning clickbait thought pieces. Some create entire startups by changing the source dataset of the Keras examples and pitch them to investors none-the-wiser, or make very light wrappers on top the examples for teaching tutorial videos and get thousands of subscribers on YouTube.</p><p>I prefer to parse documentation/examples as a proof-of-concept, but never as gospel. Examples are often not the most efficient ways to implement a solution to a problem, just merely a start. In the case of Keras&rsquo;s <a href=https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py target=_blank>text generator example</a>, the initial code was likely modeled after the 2015 blog post <a href=http://karpathy.github.io/2015/05/21/rnn-effectiveness/ target=_blank>The Unreasonable Effectiveness of Recurrent Neural Networks</a> by Andrej Karpathy and the corresponding project <a href=https://github.com/karpathy/char-rnn target=_blank>char-rnn</a>. There have been many new developments in neural network architecture since 2015 that can improve both speed and performance of the text generation model as a whole.</p><h2 id=what-text-to-generate>What Text to Generate?</h2><p>The Keras example uses <a href=https://en.wikipedia.org/wiki/Friedrich_Nietzsche target=_blank>Nietzsche</a> writings as a data source, which I&rsquo;m not fond of because it&rsquo;s difficult to differentiate bad autogenerated Nietzsche rants from actual Nietzsche rants. What I want to generate is text with <em>rules</em>, with the algorithm being judged by how well it follows an inherent structure. My idea is to create <a href=http://magic.wizards.com/en target=_blank>Magic: The Gathering</a> cards.</p><p><img src=/img/char-embeddings/dragon-whelp.jpg alt></p><p>Inspired by the <a href=https://twitter.com/RoboRosewater target=_blank>@RoboRosewater</a> Twitter account by Reed Milewicz and the <a href=http://www.mtgsalvation.com/forums/creativity/custom-card-creation/612057-generating-magic-cards-using-deep-recurrent-neural target=_blank>corresponding research</a> and <a href=https://motherboard.vice.com/en_us/article/the-ai-that-learned-magic-the-gathering target=_blank>articles</a>, I aim to see if it&rsquo;s possible to recreate the structured design creativity for myself.</p><p>Even if you are not familiar with Magic and its rules, you can still find the <a href=https://twitter.com/RoboRosewater/status/756198572282949632 target=_blank>card text</a> of RoboRosewater cards hilarious:</p><p><img src=/img/char-embeddings/horse.jpeg alt></p><p>Occasionally RoboRosewater, using a weaker model, produces amusing <a href=https://twitter.com/RoboRosewater/status/689184317721960448 target=_blank>neural network trainwrecks</a>:</p><p><img src=/img/char-embeddings/carl.png alt></p><p>More importantly, all Magic cards have an explicit structure; they have a name, mana cost in the upper-right, card type, card text, and usually a power and toughness in the bottom-right.</p><p>I wrote <a href=https://github.com/minimaxir/char-embeddings/blob/master/create_magic_text.py target=_blank>another Python script</a> to parse all Magic card data from <a href=https://mtgjson.com target=_blank>MTG JSON</a> into an encoding which matches this architecture, where each section transition has its own symbol delimiter, along with other encoding simplicities. For example, here is the card <a href="http://gatherer.wizards.com/Pages/Card/Details.aspx?multiverseid=247314" target=_blank>Dragon Whelp</a> in my encoding:</p><pre><code>[Dragon Whelp@{2}{R}{R}#Creature — Dragon$Flying|{R}: ~ gets +1/+0 until end of turn. If this ability has been activated four or more times this turn, sacrifice ~ at the beginning of the next end step.%2^3]
</code></pre><p>These card encodings are all combined into one .txt file, which will be fed into the model.</p><h2 id=building-and-training-the-model>Building and Training the Model</h2><p>The Keras text generation example operates by breaking a given .txt file into 40-character sequences, and the model tries to predict the 41st character by outputting a probability for each possible character (108 in this dataset). For example, if the input based on the above example is <code>['D', 'r', 'a', 'g', ..., 'D', 'r', 'a', 'g']</code> (with the latter Drag being part of the creature type), the model will optimize for outputting a probability of 1.0 of <code>o</code>; per the <a href=https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression target=_blank>categorical crossentropy</a> loss function, the model is rewarded for assigning correct guesses with 1.0 probability and incorrect guesses with 0.0 probabilities, penalizing half-guesses and wrong guesses.</p><p>Each possible 40-character sequence is collected, however only every other third sequence is kept; this prevents the model from being able to learn card text verbatim, plus it also makes training faster. (for this model, there are about <strong>1 million</strong> sequences for the final training). The example uses only a 128-node <a href=https://en.wikipedia.org/wiki/Long_short-term_memory target=_blank>long-short-term-memory</a> (LSTM) <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network target=_blank>recurrent neural network</a> (RNN) layer, popular for incorporating a &ldquo;memory&rdquo; into a neural network model, but the example notes at the beginning it can take awhile to train before generated text is coherent.</p><p>There are a few optimizations we can make. Instead of supplying the characters directly to the RNN, we can first encode them using an <a href=https://keras.io/layers/embeddings/ target=_blank>Embedding layer</a> so the model can train character context. We can stack more layers on the RNN by adding a 2-level <a href=https://en.wikipedia.org/wiki/Multilayer_perceptron target=_blank>multilayer perceptron</a>: a <a href=https://www.reddit.com/r/ProgrammerHumor/comments/5si1f0/machine_learning_approaches/ target=_blank>meme</a>, yes, but it helps, as the network must learn latent representations of the data. Thanks to recent developments such as <a href=https://arxiv.org/abs/1502.03167 target=_blank>batch normalization</a> and <a href=https://en.wikipedia.org/wiki/Rectifier_(neural_networks) target=_blank>rectified linear activations</a> for these <a href=https://keras.io/layers/core/#dense target=_blank>Dense layers</a>, they can both be trained without as much computational overhead, and thanks to Keras, both can be added to a layer with a single line of code each. Lastly, we can add an auxiliary output via Keras&rsquo;s <a href=https://keras.io/models/model/ target=_blank>functional API</a> where the network makes a prediction based on only the output from the RNN in addition to the main output, which forces it to work smarter and ends up resulting in a <em>significant</em> improvement in loss for the main path.</p><p>The final architecture ends up looking like this:</p><p><img src=/img/char-embeddings/model.png alt></p><p>And because we added an Embedding layer, we can load the pretrained 300D character embeds I made earlier, giving the model a good start in understanding character relationships.</p><p>The goal of the training is to minimize the total loss of the model. (but for evaluating model performance, we only look at the loss of the main output). The model is trained in <strong>epochs</strong>, where the model sees all the input data atleast once. During each epoch, batches of size 128 are loaded into the model and evaluated, calculating a <strong>batch loss</strong> for each; the gradients from the batch are backpropagated into the previous layers to improve them. While training with Keras, the console reports an <strong>epoch loss</strong>, which is the average of all the batch losses so far in the current epoch, allowing the user to see in real time how the model improves, and it&rsquo;s addicting.</p><p><img src=/img/char-embeddings/keras-training.gif alt></p><p>Keras/TensorFlow works just fine on the CPU, but for models with a RNN, you&rsquo;ll want to consider using a GPU for performance, specifically one by nVidia. Amazon has cloud GPU instances for $0.90/hr (<a href=http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html target=_blank>not prorated</a>), but very recently, Google announced <a href=https://cloud.google.com/compute/docs/gpus/add-gpus target=_blank>GPU instances</a> of the same caliber for ~$0.75/hr (prorated to the minute), which is what I used to train this model, although Google Compute Engine requires configuring the GPU drivers first. For 20 epochs, it took about 4 hours and 20 minutes to train the model while spending $3.26, which isn&rsquo;t bad as far as deep learning goes.</p><h2 id=making-magic>Making Magic</h2><p>After each epoch, the original Keras text generation example takes a sentence from the input data as a seed and predicts the next character in the sequence according to the model, then uses the last 40 characters generated for the next character, etc. The sampling incorporates a diversity/temperature parameter which allows the model to make suboptimal decisions and select characters with lower natural probabilities, which allows for the romantic &ldquo;creativity&rdquo; popular with neural network text generation.</p><p>With the Magic card dataset and my tweaked model architecture, generated text is coherent <a href=https://github.com/minimaxir/char-embeddings/blob/master/output/iter-01-0_9204.txt target=_blank>after the 1st epoch</a>! After about 20 epochs, training becomes super slow, but the predicted text becomes super interesting. Here are a few fun examples from a <a href=https://github.com/minimaxir/char-embeddings/blob/master/output/text_sample.txt target=_blank>list of hundreds of generated cards</a>. (Note: the power/toughness values at the end of the card have issues; more on that later).</p><p>With low diversity, the neural network generated cards that are oddly biased toward card names which include the letter &ldquo;S&rdquo;. The card text also conforms to the rules of the game very well.</p><pre><code>[Reality Spider@{3}{G}#Creature — Elf Warrior$Whenever ~ deals combat damage to a player, put a +1/+1 counter on it.%^]
[Dark Soul@{2}{R}#Instant$~ deals 2 damage to each creature without flying.%^]
[Standing Stand@{2}{G}#Creature — Elf Shaman${1}{G}, {T}: Draw a card, then discard a card.%^]
</code></pre><p>In contrast, cards generated with high diversity hit the uncanny valley of coherence and incoherence in both text and game mechanic abuse, which is what makes them interesting.</p><pre><code>[Portrenline@{2}{R}#Sorcery$As an additional cost to cast ~, exile ~.%^]
[Clocidian Lorid@{W}{W}{W}#Instant$Regenerate each creature with flying and each player.%^]
[Icomic Convermant@{3}{G}#Sorcery$Search your library for a land card in your graveyard.%1^1]
</code></pre><p>The best-of-both-worlds cards are generated from diversity parameters between both extremes, and often have funny names.</p><pre><code>[Seal Charm@{W}{W}#Instant$Exile target creature. Its controller loses 1 life.%^]
[Shambling Assemblaster@{4}{W}#Creature — Human Cleric$When ~ enters the battlefield, destroy target nonblack creature.%1^1]
[Lightning Strength@{3}{R}#Enchantment — Aura$Enchant creature|Enchanted creature gets +3/+3 and has flying, flying, trample, trample, lifelink, protection from black and votile all damage unless you return that card to its owner's hand.%2^2]
[Skysor of Shadows@{7}{B}{B}{B}#Enchantment$As ~ enters the battlefield, choose one —|• Put a -1/-1 counter on target creature.%2^2]
[Glinding Stadiers@{4}{W}#Creature — Spirit$Protection from no creatures can't attack.%^]
[Dragon Gault@{3}{G}{U}{U}#Creature — Kraven$~'s power and toughness are 2.%2^2]
</code></pre><p>All Keras/Python code used in this blog post, along with sample Magic card output and the trained model itself, is available open-source <a href=https://github.com/minimaxir/char-embeddings target=_blank>in this GitHub repository</a>. The repo additionally contains <a href=https://github.com/minimaxir/char-embeddings/blob/master/text_generator_keras_sample.py target=_blank>a Python script</a> which lets you generate new cards using the model, too!</p><h2 id=visualizing-model-performance>Visualizing Model Performance</h2><p>One thing deep learning tutorials rarely mention is <em>how</em> to collect the loss data and visualize the change in loss over time. Thanks to Keras&rsquo;s <a href=https://keras.io/callbacks/ target=_blank>utility functions</a>, I wrote a custom model callback which collects the batch losses and epoch losses and writes them to a CSV file.</p><p>{% comment %}
In addition to being able to generate images of neural network models as above, Keras has many useful utility functions which I added to the example, such as a callback to save the model while training, and a callback to log the losses to <a href=https://github.com/minimaxir/char-embeddings/blob/master/output/log.csv target=_blank>a CSV file</a>.
{% endcomment %}</p><p>Using R and ggplot2, I can plot the batch loss at every 50th batch to visualize how the model converges over time.</p><p><img src=/img/char-embeddings/batch-losses.png alt></p><p>After 20 epochs, the model loss ends up at about <strong>0.30</strong> which is more-than-low-enough for coherent text. As you can see, there are large diminishing returns after a few epochs, which is the hard part of training deep learning models.</p><p>Plotting the epoch loss over the batches makes the trend more clear.</p><p><img src=/img/char-embeddings/epoch-losses.png alt></p><p>In order to prevent early convergence, we can make the model more complex (i.e. stack more layers unironically), but that has trade-offs, both in training <em>and</em> predictive speed, the latter of which is important if using deep learning in a production application.</p><p>Lastly, as with the Google One Billion Words benchmark, we can extract the <a href=https://github.com/minimaxir/char-embeddings/blob/master/output/char-embeddings.txt target=_blank>trained character embeddings</a> from the model (now augmented with Magic card context!) and plot them again to see what has changed.</p><p><img src=/img/char-embeddings/char-tsne-embed.png alt></p><p>There are more pairs of uppercase/lowercase characters, although interestingly there isn&rsquo;t much grouping with the special characters added as section breaks in the encoding, or mechanical uppercase characters such as W/U/B/R/G/C/T.</p><h2 id=next-steps>Next Steps</h2><p>After building the model, I did a little more research to see if others solved the power/toughness problem. Since the sentences are only 40 characters and Magic cards are much longer than 40 characters, it&rsquo;s likely that power/toughness are out-of-scope for the model and it cannot learn their exact values. Turns out that the intended solution is to use a <a href=https://github.com/billzorn/mtgencode target=_blank>completely different encoding</a>, such as this one for Dragon Whelp:</p><pre><code>|5creature|4|6dragon|7|8&amp;^^/&amp;^^^|9flying\{RR}: @ gets +&amp;^/+&amp; until end of turn. if this ability has been activated four or more times this turn, sacrifice @ at the beginning of the next end step.|3{^^RRRR}|0N|1dragon whelp|
</code></pre><p>Power/toughness are generated near the <em>beginning</em> of the card. Sections are delimited by pipes, with a numeral designating the corresponding section. Instead of numerals being used card values, carets are used, which provides a more accurate <em>quantification</em> of values. With this encoding, each character has a <em>singular purpose</em> in the global card context, and their embeddings would likely generate more informative visualizations. (But as a consequence, the generated cards are harder to parse at a glance).</p><p>The secondary encoding highlights a potential flaw in my methodology using pretrained character embeddings. Trained machine learning models must be used apples-to-apples on similar datasets; for example, you can&rsquo;t accurately perform Twitter <a href=https://en.wikipedia.org/wiki/Sentiment_analysis target=_blank>sentiment analysis</a> on a dataset using a model trained on professional movie reviews since Tweets do not follow <a href=https://owl.english.purdue.edu/owl/resource/735/02/ target=_blank>AP Style</a> guidelines. In my case, the <a href=http://commoncrawl.org target=_blank>Common Crawl</a>, the source of the pretrained embeddings, follows more natural text usage and would not work analogously with the atypical character usages in <em>either</em> of the Magic card encodings.</p><p>There&rsquo;s still a <em>lot</em> of work to be done in terms of working with both pretrained character embeddings and improving Magic card generation, but I believe there is promise. The better way to make character embeddings than my script is to do it the hard way and train then manually, maybe even at a higher dimensionality like 500D or 1000D. Likewise, for Magic model building, the <a href=https://github.com/billzorn/mtgencode#training-a-neural-net target=_blank>mtg-rnn instructions</a> repo uses a large LSTM stacked on a LSTM along with 120/200-character sentences, both of which combined make training <strong>VERY</strong> slow (notably, this was the architecture of the <a href=https://github.com/fchollet/keras/commit/d2b229df2ea0bab712379c418115bc44508bc6f9#diff-904d72bcf9fa38b32f9c1f868ff59367 target=_blank>very first commit</a> for the Keras text generation example, and <a href=https://github.com/fchollet/keras/commit/01d5e7bc4782daafcfa99e035c1bdbe13a985145 target=_blank>was changed</a> to the easily-trainable architecture). There is also promise in a <a href=http://kvfrans.com/variational-autoencoders-explained/ target=_blank>variational autoencoder</a> approach, such as with <a href=https://arxiv.org/abs/1702.02390 target=_blank>textvae</a>.</p><p>This work is potentially very expensive and I am strongly considering setting up a <a href=https://www.patreon.com target=_blank>Patreon</a> in lieu of excess venture capital to subsidize my machine learning/deep learning tasks in the future.</p><p>At minimum, working with this example gave me a sufficient application of practical work with Keras, and another tool in my toolbox for data analysis and visualization. Keras makes the model-construction aspect of deep learning trivial and not scary. Hopefully, this article justifies the use of the &ldquo;deep learning&rdquo; buzzword in the headline.</p><p>It&rsquo;s also worth mentioning that I actually started working on automatic text generation 6 months ago using a different, non-deep-learning approach, but hit a snag and abandoned that project. With my work on Keras, I found a way around that snag, and on the same Magic dataset with the same input construction, I obtained a model loss of <strong>0.03</strong> at <strong>20% of the cloud computing cost</strong> in about the same amount of time. More on that later.</p><hr><p><em>The code for generating the R/ggplot2 data visualizations is available in this <a href=http://minimaxir.com/notebooks/char-tsne/ target=_blank>R Notebook</a>, and open-sourced in <a href=https://github.com/minimaxir/char-tsne-visualization target=_blank>this GitHub Repository.</a></em></p><p><em>You are free to use the automatic text generation scripts and data visualizations from this article however you wish, but it would be greatly appreciated if proper attribution is given to this article and/or myself!</em></p><div class="alert alert-note"><div>If you liked this blog post, I have set up a <a href=https://www.patreon.com/minimaxir target=_blank>Patreon</a> to fund my machine learning/deep learning/software/hardware needs for my future crazy yet cool projects, and any monetary contributions to the Patreon are appreciated and will be put to good creative use.</div></div></div><div class=article-tags><a class="badge badge-light" href=/tags/r/>R</a>
<a class="badge badge-light" href=/tags/ggplot2/>ggplot2</a></div><div class="media author-card" itemscope itemtype=http://schema.org/Person><img class="portrait mr-3" src="https://s.gravatar.com/avatar/28f09e3deff62333b3f32f19d3971d46?s=200')" itemprop=image alt=Avatar><div class=media-body><h5 class=card-title itemprop=name><a href=https://minimaxir.com/>Max Woolf</a></h5><h6 class=card-subtitle>Data Scientist at BuzzFeed</h6><p class=card-text itemprop=description>Ex-Apple. Carnegie Mellon graduate. Plotter of pretty charts. Former TechCrunch comment troll.</p><ul class=network-icon aria-hidden=true><li><a itemprop=sameAs href=https://twitter.com/minimaxir target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a itemprop=sameAs href=https://linkedin.com/in/minimaxir target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a itemprop=sameAs href=https://youtube.com/minimaxir target=_blank rel=noopener><i class="fab fa-youtube"></i></a></li><li><a itemprop=sameAs href=https://twitch.tv/minimaxir target=_blank rel=noopener><i class="fab fa-twitch"></i></a></li><li><a itemprop=sameAs href=mailto:max@minimaxir.com><i class="fas fa-envelope"></i></a></li></ul></div></div><div class=article-widget><div class=hr-light></div><h3>Related</h3><ul><li><a href=/2017/02/predicting-arrests/>Predicting And Mapping Arrest Types in San Francisco with LightGBM, R, ggplot2</a></li><li><a href=/2017/01/amazon-spark/>Playing with 80 Million Amazon Product Review Ratings Using Apache Spark</a></li><li><a href=/2016/11/first-comment/>What Percent of the Top-Voted Comments in Reddit Threads Were Also 1st Comment?</a></li><li><a href=/2016/07/stack-overflow/>Visualizing How Developers Rate Their Own Programming Skills</a></li><li><a href=/2016/06/reddit-related-subreddits/>Methods for Finding Related Reddit Subreddits with Simple Set Theory</a></li></ul></div><div class=article-widget><div class=post-nav><div class=post-nav-item><div class=meta-nav>Next</div><a href=/2017/05/leaving-apple/ rel=next>Leaving Apple Inc.</a></div><div class=post-nav-item><div class=meta-nav>Previous</div><a href=/2017/02/predicting-arrests/ rel=prev>Predicting And Mapping Arrest Types in San Francisco with LightGBM, R, ggplot2</a></div></div></div><section id=comments><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"minimaxir"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js></script><script>hljs.initHighlightingOnLoad();</script><script src=/js/academic.min.fa2e27444bc8d51f81714869209e3287.js></script><div class=container><footer class=site-footer><p class=powered-by>Copyright Max Woolf &copy; 2020 &middot;
Powered by the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a>.
<span class=float-right aria-hidden=true><a href=# id=back_to_top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&times;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>