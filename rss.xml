<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[minimaxir | Max Woolf's Blog]]></title>
  <link href="http://minimaxir.com/rss.xml" rel="self"/>
  <link href="http://minimaxir.com/"/>
  <updated>2016-04-06T08:16:05-07:00</updated>
  <id>http://minimaxir.com/</id>
  <author>
    <name><![CDATA[Max Woolf]]></name>
    <email><![CDATA[max@minimaxir.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The Importance of Sanity-Checking Datasets Before Analysis]]></title>
    <link href="http://minimaxir.com/2016/04/trust-but-verify/"/>
    <updated>2016-04-06T08:00:00-07:00</updated>
    <id>http://minimaxir.com/2016/04/trust-but-verify</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve done some cool things with movie data using a dataset from <a href="http://www.omdbapi.com">OMDb API</a>, which is sourced from <a href="http://www.imdb.com">IMDb</a> and <a href="http://www.rottentomatoes.com">Rotten Tomatoes</a> data. In my <a href="http://minimaxir.com/2016/01/movie-revenue-ratings/">previous article</a> on the dataset, I plotted the relationship between the domestic box office revenue of movies and their Rotten Tomatoes scores.</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-1.png" alt=""></p>

<p>I want to take another look at domestic Box Office Revenues with aggregate statistics such as means/medians on categorical variables such as MPAA rating and release month. For this type of analysis in particular, I&rsquo;ll also need to implement code in <a href="https://www.r-project.org">R</a> for inflation adjustment.</p>

<p>However, I ran into a few unexpectedly silly issues.</p>

<h2>Seeing Double</h2>

<p>There are many similarities between data validation and the Quality Assurance process of product development, which is why this particular area appeals to me personally as a Software QA Engineer. Whenever a cool dataset is released publicly, I play around with it to look for any obvious flaws and to get a good all-around benchmark on the robustness of the data (this is a separate procedure from the traditional &ldquo;data cleaning&rdquo; phase necessary to begin quantification on some poorly-structured datasets).</p>

<p>Do the extreme values in the data make sense? Is the data encoded in a sane format? Are there any obvious gaps or logical contradictions in summary representations of the data, especially when compared to other canonical sources? </p>

<p>These concerns are also some of the reasons I&rsquo;ve switched to the <a href="http://jupyter.org">Jupyter Notebook</a> as my primary data science IDE. After each block of code which transforms data, I can print the data frame inline to immediately see the results of the code execution, and refer back to them if anything odd happens in the future.</p>

<p>Let&rsquo;s say I have a data frame of Movies using the latest data dump (3/26/16) from OMDb. This data set contains 1,160,273 movies, including both IMDb and Rotten Tomatoes data. After cleaning the data (not shown), I can use the R package <code>dplyr</code> by Hadley Wickham to sort the data frame by Box Office Revenue descending, and print the <code>head</code> (top) of the data.</p>
<div class="highlight"><pre><code class="language-R" data-lang="R"><span class="kp">print</span><span class="p">(</span>df <span class="o">%&gt;%</span> select<span class="p">(</span>imdbID<span class="p">,</span> Title<span class="p">,</span> Year<span class="p">,</span> BoxOffice<span class="p">)</span> <span class="o">%&gt;%</span> arrange<span class="p">(</span>desc<span class="p">(</span>BoxOffice<span class="p">))</span> <span class="o">%&gt;%</span> <span class="kp">head</span><span class="p">(</span><span class="m">25</span><span class="p">),</span> n <span class="o">=</span> <span class="m">25</span><span class="p">)</span>
</code></pre></div>
<p><img src="/img/trust-but-verify/data-2.png" alt=""></p>

<p>Those movies being the best <em>makes sense</em>. For <a href="http://www.rottentomatoes.com/m/star_wars_episode_vii_the_force_awakens/">Star Wars: The Force Awakens</a>, I can compare it to the Box Office reported on the corresponding Rotten Tomatoes page, which in turn matches the <a href="http://www.boxofficemojo.com/movies/?id=starwars7.htm">domestic Box Office Revenue</a> on <a href="http://www.boxofficemojo.com">Box Office Mojo</a>.</p>

<p>But wait, <a href="https://en.wikipedia.org/wiki/The_Dark_Knight_%28film%29">The Dark Knight</a> appears <em>twice</em>? How?!</p>

<p>There&rsquo;s no way I would have missed something this obvious during the sanity-check for my previous article. In order to make sure that I&rsquo;m not going insane, I double-checked the December 2015 data dump I used for that post, derived the top movies with the same methodology for the modern data dump, and the duplicate movies <em>were not present</em>. Weird.</p>

<p>There are 2 different IDs for
The Dark Knight, and for some other movies near the top (<a href="http://www.imdb.com/title/tt4817264/">Inside Out</a>, &ldquo;<a href="http://www.imdb.com/title/tt3138972/">The Gravity</a>&rdquo;). Fortunately, duplicate data like this is easy to debug. The second data entry for The Dark Knight has a greater IMDb ID (1774602) which means it was likely added to the site later. Let&rsquo;s look up the <a href="http://www.imdb.com/title/tt1774602/">corresponding IMDb page</a>:</p>

<p><img src="/img/trust-but-verify/dark-knight.png" alt=""></p>

<p>Huh. Apparently someone put a filler movie entry with the same name and release year as a blockbuster movie in hopes that people search for it by accident (and since it received 50 ratings and an average score of 8.6, this tactic was successful).</p>

<p>Using the Rotten Tomatoes <a href="http://developer.rottentomatoes.com/docs/read/json/v10/Movie_Alias">IMDb Lookup API</a>, we find that &ldquo;The Dark Knight&rdquo; page on Rotten Tomatoes&hellip;<a href="http://api.rottentomatoes.com/api/public/v1.0/movie_alias.json?type=imdb&amp;id=1774602">doesn&rsquo;t exist</a>.</p>

<p>We can run a safe deduplicate by removing entries with the same title (excluding the &ldquo;The&rdquo; if present) and release year.</p>
<div class="highlight"><pre><code class="language-r" data-lang="r">df_dup <span class="o">&lt;-</span> df <span class="o">%&gt;%</span> select<span class="p">(</span>Title<span class="p">,</span> Year<span class="p">)</span> <span class="o">%&gt;%</span> mutate<span class="p">(</span>Title <span class="o">=</span> <span class="kp">gsub</span><span class="p">(</span><span class="s">&quot;The &quot;</span><span class="p">,</span> <span class="s">&quot;&quot;</span><span class="p">,</span> Title<span class="p">))</span>
dup <span class="o">&lt;-</span> <span class="kp">duplicated</span><span class="p">(</span>df_dup<span class="p">)</span>   <span class="c1"># find entry indices which are duplicates</span>
<span class="kp">rm</span><span class="p">(</span>df_dup<span class="p">)</span>   <span class="c1"># remove temp dataframe</span>

df_dedup <span class="o">&lt;-</span> df <span class="o">%&gt;%</span> filter<span class="p">(</span><span class="o">!</span>dup<span class="p">)</span>   <span class="c1"># keep entries which are *not* dups</span>
<span class="kp">print</span><span class="p">(</span>df_dedup <span class="o">%&gt;%</span> select<span class="p">(</span>imdbID<span class="p">,</span> Title<span class="p">,</span> Year<span class="p">,</span> BoxOffice<span class="p">)</span> <span class="o">%&gt;%</span> arrange<span class="p">(</span>desc<span class="p">(</span>BoxOffice<span class="p">))</span> <span class="o">%&gt;%</span> <span class="kp">head</span><span class="p">(</span><span class="m">25</span><span class="p">),</span> n <span class="o">=</span> <span class="m">25</span><span class="p">)</span>
</code></pre></div>
<p><img src="/img/trust-but-verify/data-1.png" alt=""></p>

<p>There we go! The de-duped dataset has 1,114,431 movies, impliying that there were 45,842 of these duplicate entries.</p>

<p>I&rsquo;m not sure <em>whose</em> fault it is that duplicate movies suddenly became present in the data dump: OMDb or Rotten Tomatoes. <em>But it doesn&rsquo;t matter</em>: the wrong entries still need to be addressed, and it&rsquo;s good to have a test case for the future too.</p>

<h2>Inflation Station</h2>

<p>A <a href="http://stackoverflow.com/a/26068058">Stack Overflow answer</a> from <a href="http://stackoverflow.com/users/1048757/brash-equilibrium">Ben Hanowell</a> has a good R implementation and rationale for implementing inflation adjustment using the <a href="https://research.stlouisfed.org/fred2/data/CPIAUCSL.txt">historical Consumer Price Index data</a> from the <a href="https://www.stlouisfed.org">Federal Reserve Bank of St. Louis</a>.</p>

<p>Take the index for each year (averaging each month for simplicity) and create an adjustment factor to convert historical dollar amounts into present-day dollar amounts. Much better than plugging hundreds of thousands of values into an online calculator. Here&rsquo;s the SO code made <code>dplyr</code>-friendly for this purpose, with the requisite sanity-checks.</p>
<div class="highlight"><pre><code class="language-r" data-lang="r">inflation <span class="o">&lt;-</span> read_csv<span class="p">(</span><span class="s">&quot;http://research.stlouisfed.org/fred2/data/CPIAUCSL.csv&quot;</span><span class="p">)</span> <span class="o">%&gt;%</span>
                    group_by<span class="p">(</span>Year <span class="o">=</span> <span class="kp">as.integer</span><span class="p">(</span><span class="kp">substr</span><span class="p">(</span>DATE<span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">4</span><span class="p">)))</span> <span class="o">%&gt;%</span>
                    summarize<span class="p">(</span>Avg_Value <span class="o">=</span> <span class="kp">mean</span><span class="p">(</span>VALUE<span class="p">))</span> <span class="o">%&gt;%</span>   <span class="c1"># average across all months</span>
                    mutate<span class="p">(</span>Adjust <span class="o">=</span> <span class="kp">tail</span><span class="p">(</span>Avg_Value<span class="p">,</span> <span class="m">1</span><span class="p">)</span> <span class="o">/</span> Avg_Value<span class="p">)</span>   <span class="c1"># normalize by most-recent year</span>

<span class="kp">print</span><span class="p">(</span>inflation <span class="o">%&gt;%</span> <span class="kp">head</span><span class="p">())</span>
<span class="kp">print</span><span class="p">(</span>inflation <span class="o">%&gt;%</span> <span class="kp">tail</span><span class="p">())</span>
</code></pre></div>
<p><img src="/img/trust-but-verify/inf.png" alt=""></p>

<p>For example, to get the inflation-adjusted Box Office Revenue for a movie released in 1949 in 2016 dollars, we multiply the reported revenue by 10. That sounds about right (and matches closely enough to the output of the <a href="http://data.bls.gov/cgi-bin/cpicalc.pl?cost1=1&amp;year1=1949&amp;year2=2016">Bureau of Labor Statistics inflation calculator</a>).</p>

<p>Now map each inflation adjustment factor to each movie by merging the two datasets (on the <code>Year</code> column), then multiply the Box Office revenue by the adjustment factor to get the inflation-adjusted revenue. Plus another sanity-check for good measure. </p>
<div class="highlight"><pre><code class="language-r" data-lang="r">df_dedup_join <span class="o">&lt;-</span> df_dedup <span class="o">%&gt;%</span> inner_join<span class="p">(</span>inflation<span class="p">)</span> <span class="o">%&gt;%</span> mutate<span class="p">(</span>AdjBoxOffice <span class="o">=</span> BoxOffice <span class="o">*</span> Adjust<span class="p">)</span>

<span class="kp">print</span><span class="p">(</span>df_dedup_join <span class="o">%&gt;%</span> select<span class="p">(</span>Title<span class="p">,</span> Year<span class="p">,</span> AdjBoxOffice<span class="p">)</span> <span class="o">%&gt;%</span> arrange<span class="p">(</span>desc<span class="p">(</span>AdjBoxOffice<span class="p">))</span> <span class="o">%&gt;%</span> <span class="kp">head</span><span class="p">(</span><span class="m">25</span><span class="p">),</span> n<span class="o">=</span><span class="m">25</span><span class="p">)</span>
</code></pre></div>
<p><img src="/img/trust-but-verify/data-3.png" alt=""></p>

<p>Uh-oh.</p>

<p>I mean, <a href="https://en.wikipedia.org/wiki/The_Lorax_(TV_special)">The Lorax</a> probably earned $1.2 billion in VHS sales for Earth Day education <em>alone</em>, but the TV special was never released in theaters. There was a <a href="https://en.wikipedia.org/wiki/The_Lorax_(film)">CGI remake of The Lorax</a> a few years ago which was reasonably popular. Could it be that someone at Rotten Tomatoes or Box Office Mojo confused the two media?</p>

<p>That is exactly what happened. On Rotten Tomatoes, The <a href="http://www.rottentomatoes.com/m/the-lorax/">1972 Lorax</a> was encoded with similar box office revenue as the <a href="http://www.rottentomatoes.com/m/the_lorax/">2012 Lorax</a>; then the inflation factor sextupled it. For this type of data fidelity issue, it&rsquo;s considerably more obvious whose at fault.</p>

<p>Unfortunately, that&rsquo;s not the end of problems with the dataset. I compared my results with <a href="http://www.vox.com/2016/4/4/11351788/batman-v-superman-terrible-reviews#undefined">Vox&rsquo;s dataset</a> on worldwide historical box office revenues. In the Top 200 Movies by inflation-adjusted revenue, there are noted historical movie omissions such as <a href="http://www.rottentomatoes.com/m/jaws/">Jaws</a> and <a href="http://www.rottentomatoes.com/m/star_wars/">Star Wars: A New Hope</a>. It turns out Rotten Tomatoes does not have Box Office Revenue data for these movies at all. </p>

<p>That is a very serious problem which I&rsquo;ll have to think about if it blocks any analysis on aggregate box office data completely. In the end, sanity-checking third party data is important because you never know <em>how</em> the data will surprise you, until it&rsquo;s too late.</p>

<hr>

<p><em>You can view the Top 200 movies by domestic box office revenue for each of the 12/15 source dataset, the 3/16 dataset, the 3/16 deduped dataset, and the 3/16 deduced inflation-adjusted data <a href="https://github.com/minimaxir/movie-data-sanity-checking">in this GitHub repository</a>, along with the Jupyter notebook.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Unlimited Data Storage Using Image Steganography and Cat GIFs]]></title>
    <link href="http://minimaxir.com/2016/03/gif-unlimited/"/>
    <updated>2016-03-29T08:00:00-07:00</updated>
    <id>http://minimaxir.com/2016/03/gif-unlimited</id>
    <content type="html"><![CDATA[<p><span><style>
.suspicious-pregnant-pause {
  padding: 60px 0;
}
</style></span></p>

<p>In 2016, animated GIFs are absurdly popular. <a href="https://www.messenger.com">Facebook Messenger</a>, <a href="https://twitter.com">Twitter</a>, and <a href="https://slack.com">Slack</a> now natively support GIF sharing, and startups which help facilitate GIF sharing such as <a href="http://giphy.com">Giphy</a> and <a href="https://www.riffsy.com">Riffsy</a> have received <a href="https://www.crunchbase.com/organization/giphy"><em>tens of millions</em> of dollars</a> in venture capital. I even built my own tool to <a href="https://github.com/minimaxir/video-to-gif-osx">convert a video to a GIF</a> on OSX because <a href="http://minimaxir.com/2015/08/gif-to-video-osx/">I wanted to be cool</a> too.</p>

<p>On <a href="https://www.reddit.com">Reddit</a>, GIFs are also popular for reactions, often containing submission titles with the phrase MRW (&ldquo;my reaction when&rdquo;). Here is a wordcloud of the titles of all GIF submissions to Reddit in 2015:</p>

<p><img src="/img/gif-unlimited/reddit-gif-wordcloud-e.png" alt=""></p>

<p>Completely unrelated, <a href="https://en.wikipedia.org/wiki/Steganography">image steganography</a> is a technique that allows the embedding of messages into the pixels of images themselves, allowing for the transmission of secret messages. Image steganography was popular on the internet in the early 2000&rsquo;s, but died out before image-sharing between peers was commonplace and information encryption became a hot political issue.</p>

<h2>It&rsquo;s a Secret to Everybody</h2>

<p>In 2012, Zach Oakes created <a href="https://sekao.net/pixeljihad/">PixelJihad</a>, an <a href="https://github.com/oakes/PixelJihad">open-source</a>, client-side tool for image steganography using JavaScript and the HTML5 Canvas. This implementation embeds text messages by converting them into raw binary and modifying the least-significant bit (Aaron Miller wrote <a href="http://www.aaronmiller.in/thesis/">an informative paper</a> on LSB encoding) of the red, blue, and green binary pixel values of the specified image to match the message (e.g. a white pixel with red, blue, and green values of <code>[255, 255, 255]</code> can be represented in binary form as <code>[11111111, 11111111, 11111111]</code>; LSB encoding changes the right-most bit to 0 or 1 for each pixel channel). Changing the last bit of a color channel will not result in a detectable visual change by the human eye.</p>

<p>The image pixels to-be-modified during encoding are selected according to a semirandom schedule. The message can then be decoded by reading the LSB of the changed pixels from the schedule and reconstructing the binary message, then converting back into text.</p>

<p>Since we can set 3 bits per physical pixel, the maximum amount of data (in bytes) that can be stored is equivalent to the width of an image (in pixels) multiplied by the height times 3, divided by 8.</p>

<p>Take the Reddit GIF wordcloud I posted above. If you download the image and load it into PixelJihad, you can see I hid a secret message showing how to get the Reddit word data using <a href="https://cloud.google.com/bigquery/">BigQuery</a>. The visualization is 600px by 400px, therefore it can store up to:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">600 * 400 * 3 bits / 8 = 90000 Bytes / 1000 = 90 kB
</code></pre></div>
<p>The image itself is 257 kB, but a bonus 35% in data storage isn&rsquo;t bad.</p>

<p>PixelJihad only supports text messages with an artificial limit of 1,000 characters (~1 kB) for <em>sanity</em>, but I&rsquo;m not fond of being sane. There is <em>nothing</em> from a technical perspective stopping users from encoding larger text messages, or even raw file data as pure binary into images.</p>

<p>But embedding data into a <em>static</em> image is boring. What happens when you embed data into <em>each frame of a GIF</em>? Just a thought.</p>

<h2>Meow!</h2>

<p>Now, back to what you&rsquo;re reading this article for, who cares about all this stegosaurus crap. Here is a cute cat GIF <a href="https://www.reddit.com/r/gifs/comments/256bir/abduction_cat_re_flying_cat_re_so_big/">from Reddit</a>. Look at its cuteness.</p>

<p><img src="/img/gif-unlimited/aliencat.gif" alt=""></p>

<p>Aww.</p>

<p>The GIF file size is 2.44 MB (sorry, readers on mobile devices!). The dimensions of the GIF are 300px by 399px. But if you open the GIF in Preview on OSX, you can see that the GIF is comprised of 72 frames.</p>

<p><img src="/img/gif-unlimited/gif-finder-17.png" alt=""></p>

<p>Performing the same math:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">300 * 399 * 72 frames * 3 / 8 = 3231900 / 1000000 = 3.23 MB
</code></pre></div>
<p>The potential amount of encodable data is larger than the file size of the image itself! Screw secret messages, we&rsquo;re literally <em>creating</em> data storage out of nothing!</p>

<p>You know what this means? We can <em>embed the cat GIF into itself</em> steganographically by splitting it over all 72 frames, and get 780 kB left over to store whatever we want.</p>

<p>And then we can <em>do it again</em> to the newly-embedded image.</p>

<p>Infinitely.</p>

<p>780 kB free with each iteration.</p>

<p>Cat GIF recursion.</p>

<h2>Purrcursion</h2>

<p>Let&rsquo;s say we want to embed a 10 MB music file into a GIF. Since 10000 kB / 780 kB = 12.82, we re-embed the cat GIF 13 times, including 1/13th of the music file as sidecar data with each iteration. And now we have a cat GIF that contains some pretty sweet tunes!</p>

<p>And not only do we have a magical cat GIF, the data is completely portable; you can simply e-mail the cat GIF to your friend and they can decode the high-fidelity music file out of the GIF using an app that implements this schema. Or you can post the cat GIF w/ music to Reddit for <em>double</em> meaningless internet points.</p>

<p>The only limitation to the &ldquo;infinitely&rdquo; part is the increasing amount of CPU processing power for each decoding, but that&rsquo;s fixable since it&rsquo;s 2016 and smartphones have quad-core processors. Worth every ounce of battery life, in my opinion.</p>

<p>*<em>tl;dr *</em> I was bored and decided to create infinite data in a way that makes people feel fuzzy inside. No big deal. Of course, I can&rsquo;t show <em>you</em> a proof-of-concept of how this works. That would be silly. But if you&rsquo;re a venture capitalist who wants to invest more tens of millions of dollars into a clearly-sustainable GIF startup, feel free to email me at <a href="mailto:max@minimaxir.com">max@minimaxir.com</a>.</p>

<div class="suspicious-pregnant-pause">&hellip;</div>

<div class="suspicious-pregnant-pause">&hellip;</div>

<div class="suspicious-pregnant-pause">&hellip;</div>

<p>Yeah, infinite data via image steganography doesn&rsquo;t actually work. But the reasons it doesn&rsquo;t work are not obvious unless you have deep knowledge of how image compression reduces the file size of images, and conversely, how modifying images could cause them to become <em>larger</em> in file size  (I may or may not have spent several hours forking PixelJihad and adding modern features before realizing that I am not a data storage magician).</p>

<p>I will write a follow-up post soon detailing the steganographic gotchas. Until then, keep in mind that there is no such thing as a free cat GIF.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Facebook Reactions and the Problems With Quantifying Likes Differently]]></title>
    <link href="http://minimaxir.com/2016/02/facebook-reactions/"/>
    <updated>2016-02-29T08:00:00-08:00</updated>
    <id>http://minimaxir.com/2016/02/facebook-reactions</id>
    <content type="html"><![CDATA[<p>Facebook added <a href="http://newsroom.fb.com/news/2016/02/reactions-now-available-globally/">Facebook Reactions</a>, allowing users to do more than just &ldquo;Like&rdquo; posts and statuses as they have done for the past decade. Likes were the universal symbol of approval on social media. Now, Facebook users can apply more granular responses, from positive emotions like <strong>Love</strong>, to negative emotions such as <strong>Angry</strong>. This was widely believed to be Facebook&rsquo;s compromise instead of adding a Dislike button.</p>

<p><img src="/img/facebook-reactions/facebook_react.png" alt=""></p>

<p>Of course, there&rsquo;s an ulterior motive. The use of reactions provides organic data on the sentiment of a status, which is helpful for numerous marketing and statistical applications. As <a href="http://www.buzzfeed.com/alexkantrowitz/facebook-reactions-launch-today">BuzzFeed notes</a>, Facebook ads may be able &ldquo;to write one product message for someone who mostly uses <strong>Sad</strong> and another who mostly uses <strong>Wow</strong> or <strong>Love.</strong>&rdquo;</p>

<p>However, this isn&rsquo;t the first time a big social network has tried implementing reactions alongside Likes/Dislikes. Four years ago, YouTube added <a href="http://googlesystem.blogspot.com/2011/06/youtube-reactions.html">Reaction buttons</a> to their comments section:</p>

<p><img src="/img/facebook-reactions/youtube-reactions.png" alt=""></p>

<p>&hellip;and removed them sometime after without fanfare, replacing it with the simple Like/Dislike bar.</p>

<p>Presumably, YouTube implemented the buttons for the similar reason as Facebook. What makes things different now, if anything?</p>

<h2>A Quantitative Approach to Feeling</h2>

<p>Even after YouTube&rsquo;s failure, another data-driven website implemented reaction buttons: BuzzFeed (who else?). At the end of each article (in most categories), registered users can select a quirky reaction to indicate how they felt about the article.</p>

<p><img src="/img/facebook-reactions/buzzfeedreactions.png" alt=""></p>

<p>The heart represents <strong>Love</strong> internally and is by-far the most-used reaction on BuzzFeed posts. When I started scraping BuzzFeed data in 2014 <a href="http://minimaxir.com/2015/01/linkbait/">to analyze clickbait</a>, I made sure to grab the reaction data of other reactions as well to see if there are any interesting trends or correlations between reactions. A cursory glance at the scraped reaction data revealed a problem that forced me to disregard it.</p>

<p>An important part of variable selection for analysis and modeling is avoiding <em>redundant</em> features, as that can cause issues such as <a href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a> and <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>. For Facebook, avoiding adding redundant Reactions was an <a href="https://medium.com/facebook-design/reactions-not-everything-in-life-is-likable-5c403de72a3f">explicit design goal</a> of the feature, but the positive emotions such as <strong>Like</strong> and <strong>Wow</strong> might be overly similar regardless (I believe it fair to compare the behavior of BuzzFeed users with the average Facebook user, given that they hit the same demographics). Do BuzzFeed readers use specific positive reactions differently? Did they use specific negative reactions?</p>

<p>I rechecked my 2014 data in light of Facebook Reactions. The scraped dataset contains reaction data from 9,883 BuzzFeed articles in the Celebrity, Animals, Books, Longform, and Business categories. From that, I made a <a href="http://vita.had.co.nz/papers/gpp.pdf">pairs plot</a> for the counts of all the <em>positive</em> reactions on the articles to illustrate all bivariate relationships:</p>

<ul>
<li>The lower half of the pairs plot is a scatterplot for the two reactions; the axes represent the number of votes for a given reaction on a BuzzFeed article (both axes are scaled logarithmically), color intensity indicates the number of articles at that X/Y combo, and the line is a linear trendline of least-squares.</li>
<li>The diagonal of the pairs plot represents the density distribution of reaction vote counts for that reaction. (also logarithmically scaled on the X axis)</li>
<li>The upper half of the pairs plot illustrates the Pearson correlation between the non-log quantities of the two reaction variables. The stars represent statistical significance of the correlation test; since the data set is large, all correlations are statistically significant (rejection of null hypothesis of no correlation) at p &lt; 0.001.</li>
</ul>

<p><img src="/img/facebook-reactions/buzzfeed-pos.png" alt=""></p>

<p>All of the bivariate correlations of positive reactions are <em>moderately or strongly positively correlated</em>, which is problematic for analysis (except one: apparently, there is little statistical relationship between things that are cute and things that make you go YAAASS). So why not just use the <strong>Love</strong> reaction, since articles tend to get about 100 Loves, while other reactions get around 10?</p>

<p>Does the same hold for negative reactions? Relatedly, we would also expect a negative correlation between the number of <strong>Love</strong> reactions and negative reactions, right?</p>

<p><img src="/img/facebook-reactions/buzzfeed-neg.png" alt=""></p>

<p>All negative reactions are positively correlated, as expected, but there is a weak <em>positive</em> correlation between <strong>Love</strong> and <strong>Hate</strong>, which is definitely not right. There isn&rsquo;t an ideal &ldquo;negative&rdquo; reaction, since all have similar distributions.</p>

<p>Why does Facebook have 6 different responses to gauge positivity or negativity when one reaction for each would be both more accurate and more intuitive for the user?</p>

<h2>Conceal, Don&rsquo;t Feel</h2>

<p>There are other qualitative issues with Facebook&rsquo;s current implementation of Reactions. Apparently, Likes and Reactions are treated <em>differently internally</em>. As a result, you get separate notifications for Likes and Reactions.</p>

<p><img src="/img/facebook-reactions/facebook_react2.png" alt=""></p>

<p>Why? No idea. There is enough Notification spam on Facebook, I don&rsquo;t need <em>double notifications</em> in my Notification feed for every status I make.</p>

<p>What&rsquo;s important to note is that a user cannot both Like and React to a status; only one or the other. As a result, the number of Likes on statuses overall will drop, and this is a <em>major</em> problem for businesses who are dependent on measuring the number of Likes for engagement.</p>

<p>I took a look at the Facebook Graph API endpoint for <a href="https://developers.facebook.com/docs/graph-api/reference/v2.5/post">Facebook Page Posts</a> (same endpoint I use for my <a href="https://github.com/minimaxir/facebook-page-post-scraper">Facebook Page Data Scraper</a>), and I can confirm that the API can only report the number of Likes on a status; not the number of Likes + Reactions, or number of Likes + number of each Reaction.</p>

<p><img src="/img/facebook-reactions/cnn_fb.jpg" alt=""></p>

<p>There is no way currently to automate the retrieval of Reactions data from Facebook posts, which is an unfortunate oversight (especially considering how Twitter <a href="https://blog.twitter.com/2015/hearts-for-developers">handled the transition</a> from Favorites to Likes easily).</p>

<p>The example <a href="https://www.facebook.com/cnn/posts/10154506885211509">CNN story</a> I used for that screenshot is anecdotally one of the very few examples I&rsquo;ve noticed where the number of Likes is <em>almost equal</em> to negative emotions, a relationship which should be weakly correlated and therefore this knowledge may be useful to isolate the story as unusual (and serve ads accordingly). At Facebook&rsquo;s immense scale, identifying a relatively small proportion of unusual stories might be enough to justify adding Reactions.</p>

<p>Or maybe this feature is just the harbinger of a new generation of emotionally-charged linkbait. Perhaps there is more to this Facebook Reactions data than what meets the eye, and I&rsquo;ll update my scripts and do further statistical analysis when able. But given what has happened with Reactions data before with YouTube, I am unconvinced and I still believe the functionality as a whole is a usability regression that won&rsquo;t last.</p>

<p>A Dislike button would have been better, just saying.</p>

<hr>

<p><em>You can view the code and data used to generate the BuzzFeed Reaction data visualizations <a href="https://github.com/minimaxir/facebook-reactions/blob/master/buzzfeed_reactions.ipynb">in this Jupyter notebook</a>, <a href="https://github.com/minimaxir/facebook-reactions">open-sourced on GitHub</a>, or you can <a href="https://github.com/minimaxir/facebook-reactions/raw/master/reactions_pdf.pdf">view as a PDF</a>, which is better if you are on a mobile device.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[You're Not Allowed to Criticize Startups, You Stupid Hater]]></title>
    <link href="http://minimaxir.com/2016/01/startup-haters/"/>
    <updated>2016-01-25T08:00:00-08:00</updated>
    <id>http://minimaxir.com/2016/01/startup-haters</id>
    <content type="html"><![CDATA[<p>A couple weeks ago, <a href="https://itunes.apple.com/us/app/peach-a-space-for-friends/id1067891186?mt=8">Peach</a>, a messaging app, was released on iOS. Not only did Peach trend on Twitter on the day of release, but there was also an <em>unusual</em> amount of media coverage for app, with both <a href="http://techcrunch.com/2016/01/08/peach-is-a-slick-new-messaging-app-from-the-founder-of-vine/">TechCrunch</a> and <a href="http://www.buzzfeed.com/katienotopoulos/do-i-dare-to-post-on-peach">BuzzFeed</a> posting glowing reviews for it, noting that the app is &ldquo;slick&rdquo; and &ldquo;blowing up&rdquo; respectively (commenters in the BuzzFeed article thought it was paid advertising). Usually, reports for apps from both sites are more neutral, which made me curious.</p>

<p><img src="/img/startup-haters/peach-1.png" alt=""></p>

<p>There are <em>already</em> an absurd amount of messaging apps out there, and as a result, product differentiation is the primary value proposition. (e.g. security and <a href="https://itunes.apple.com/us/app/telegram-messenger/id686449807?mt=8">Telegram Messenger</a>) What is Peach&rsquo;s differentiation? &ldquo;Magic Words&rdquo;, apparently, which let you perform contextual actions with words! These Magic Words are implemented in a manner similar to a <a href="https://en.wikipedia.org/wiki/Command-line_interface">command-line interface</a> (CLI). <a href="http://nymag.com/following/2016/01/how-the-command-line-became-mainstream-again.html">NYMag</a> goes into extreme detail about the feature. Nerds love CLIs, so you can tell the product was made by smart people!</p>

<p><img src="/img/startup-haters/peach-2.png" alt=""></p>

<p>The <a href="https://news.ycombinator.com/item?id=10883698">Hacker News thread</a> about the NYMag article was interesting. One commenter (jobu) that the Magic Words are similar to those of <a href="https://slack.com">Slack</a>, which allow for &ldquo;/&rdquo; commands (again, messaging app differentiation). A HN user (pbreit) argued in response:</p>

<blockquote>
<p>You&rsquo;re missing the whole thing. For starters, Slack is for work and Peach is for personal. But I think before you dismiss something as &ldquo;isn&rsquo;t it just x plus y?&rdquo; you need to take a little bit more time and thinking to try and figure out what the product designers are trying to achieve and how.</p>
</blockquote>

<p>Wait, what? Are the users of Peach supposed to be mind readers? Are people stupid for not understanding the purpose of Peach?</p>

<p>Another user (thwarted) replied to that response with such:</p>

<blockquote>
<p>Maybe the product designers and product providers should be more explicit about how their product should be used and who their target demographic is, if they want a successful product, rather than making everyone &ldquo;figure out&rdquo; what they are trying to achieve and how.</p>
</blockquote>

<p>pbreit replied:</p>

<blockquote>
<p>&ldquo;Figuring it out&rdquo; might be the point. You do realize that you are referring to some of the very few people who have actually already built an amazingly successful product.</p>
</blockquote>

<p>I should have mentioned earlier that Peach was made by <a href="http://byte.co">Byte</a>, which is ran by <a href="https://twitter.com/dhof">Dom Hofmann</a>, a founder of Vine which was purchased by Twitter before launch (it also appears Peach is a pivot from the failed Byte app; something left out by the blog articles). Yes, if a founder has had a previously successful startup, the previous success implies a higher probability for future startup success than for a startup run by an unknown founder. However, it&rsquo;s still not a 100% guarantee, and startups should not be immune to criticism because of asymmetric information between the users and the startup itself. Peach is impeachable. (pun intended)</p>

<p>TechCrunch did a <a href="http://techcrunch.com/2016/01/11/hype-or-not-peach-hit-the-top-10-social-networking-app-list-fast/">follow-up article</a> three days after release stating (paraphrased) &ldquo;Peach is #9 in the App Store in the Social Networking category, despite the haters! Now they will be a success! Haters gonna hate!&rdquo; Social networking is a double-edged sword in terms of network effects: they can grow incredibly fast as friends-of-friends register, and they can <em>die</em> incredibly fast once they all realize the app is a fad. As of publishing, Peach is ranked #110 in the Social Networking category, and off-the-charts in Overall. And <a href="https://www.appannie.com/apps/ios/app/peach-a-space-for-friends/rank-history/">still dropping</a>, with no apparent recovery.</p>

<p><img src="/img/startup-haters/peach-3.png" alt=""></p>

<p>I seriously wish the tech media would admit they were wrong, but they never will. The startup world is in dire need of cautionary tales as valuations inflate to absurd levels.</p>

<p>But hey, isn&rsquo;t any startup ran by a previously-funded founder a sure thing? They know what it takes to create a company, even if it&rsquo;s in a different and highly competitive environment years later than their first success! One success is all a founder needs.</p>

<p>A startup&rsquo;s release is unpolished and buggy? It&rsquo;s just an MVP, and startups are expected to have issues out of the gate, so you have to forgive them! They don&rsquo;t need QA engineers.</p>

<p>A startup uses growth hacking to get early traction? It&rsquo;s hustle, and they should be applauded for their creativity! Whoever complains is just in the minority, anyways.</p>

<p>A startup gets a lot of points on Product Hunt? That&rsquo;s validation, especially since it was submitted by one of the friends/investors of the startup for all their network to see and upvote! A strong personal network is the only thing you need for business success.</p>

<p>A startup raises a Series A? That means venture capitalists did due diligence, and if the idea sucked, they would not have invested! Millions of dollars is a lot of money to those people.</p>

<p>A startup loses traction and buzz? It&rsquo;s not dead, it can come back as long as it has money in the bank! Your lack of faith demotivates entrepreneurs.</p>

<p>A startup <em>dies</em>? They can just try again! It was bad luck anyways and they likely did nothing wrong.</p>

<p>A startup is mercy-killed through an acquisition/acquihire? Just another step in their incredible journey! The founders still win, and now they have the &ldquo;exited&rdquo; label to leverage for future employment.</p>

<p>Sarcastic rhetorical questions aside, it is very annoying that it is considered socially unacceptable in Silicon Valley to criticize a startup. Even though the majority of startups fail, actually <em>saying so</em> is sacrilegious. In Peach&rsquo;s particular case, the fact that the app is from a previous exited founder should invite <em>more</em> scrutiny instead of giving it a free pass.</p>

<p><strong>EDIT</strong>: Fixed joke so that it is funny.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Video Games and Charity: Analyzing Awesome Games Done Quick 2016 Donations]]></title>
    <link href="http://minimaxir.com/2016/01/agdq-2016/"/>
    <updated>2016-01-11T08:00:00-08:00</updated>
    <id>http://minimaxir.com/2016/01/agdq-2016</id>
    <content type="html"><![CDATA[<p><a href="https://gamesdonequick.com">Awesome Games Done Quick</a>, and its sister event Summer Games Done Quick, are a fundraising events that livestreams video game speedruns <a href="http://www.twitch.tv/gamesdonequick/profile">live on Twitch</a> for charity. Beginning in January 2011, before Twitch was launched out from Justin.tv, <a href="https://en.wikipedia.org/wiki/Awesome_Games_Done_Quick_and_Summer_Games_Done_Quick#List_of_marathons">AGDQ was very small</a> and only raised $52,519.83 for the <a href="http://preventcancer.org">Prevent Cancer Foundation</a>; now, in 2016, from January 3rd to January 10th, AGDQ <a href="https://gamesdonequick.com/tracker/index/agdq2016">successfully raised</a> about $1.2 <em>million</em> for the charity.</p>

<p>A <a href="https://en.wikipedia.org/wiki/Speedrun">speedrun</a>, as the name suggests, is the process of completing a video game as fast as possible, optionally with self-imposed challenges to make things more interesting. Speedruns can emphasize extreme player skill and/or clever glitch abuse. And unexpected mistakes which make the results hilarious.</p>

<p>One of the first runs of AGDQ 2016, <a href="https://www.youtube.com/watch?v=jLlian3g7Gg">Super Monkey Ball</a>, demonstrates all of these. (run starts at 5:57)</p>

<div class="responsive-video"><iframe src="http://www.youtube.com/embed/jLlian3g7Gg " style="width: 100%; height: 100%; border: none; padding-bottom: 20px" allowfullscreen></iframe></div>

<p>AGDQ 2016 also has fun with the concept of speedrunning. One of the best events of AGDQ 2016 was a blind speedrun of user-created <a href="https://www.youtube.com/watch?v=8qC584MWXO4">Super Mario Maker</a> levels from top designers, in which hilarity ensued. (run starts at 27:41)</p>

<div class="responsive-video"><iframe src="http://www.youtube.com/embed/8qC584MWXO4 " style="width: 100%; height: 100%; border: none; padding-bottom: 20px" allowfullscreen></iframe></div>

<p>It might be interesting to know <em>which</em> video games lead to the achievement of over $1M donated to charity and the nature of the donations in general.</p>

<h2>Gaming Data</h2>

<p>With a few quick scripts on Kimono to scrape data from the <a href="https://gamesdonequick.com/tracker/donations/agdq2016">AGDQ 2016 donation page</a> (+ a <em>lot</em> of postprocessing in R!), I obtained a dataset of all 30,528 donations, their donors, when they donated, during what speedrun they donated, and <em>why</em> they donated. (<a href="https://docs.google.com/spreadsheets/d/1yyfkS0jvRK1cWrQesYiBn1TMGC93lo1MqahcU3XeGIU/edit?usp=sharing">Google Sheets link</a> for all the data)</p>

<p>Here are the cumulative donations during AGDQ, color coded by day:</p>

<p><img src="/img/agdq-2016/agdq-1.png" alt=""></p>

<p>Cumulative donations were strong the entire run. On the second-to-last day, the donations rallied and increased exponentially, clearing $1M handily on the last day.</p>

<p>The donation amount minimum is $5, but the average is significantly higher at $39.62. What is the distribution of donations?</p>

<p>Here is a distribution of donations from $5 to $100 (for ease of visualization/interpretation), which account for 97% of all donations:</p>

<p><img src="/img/agdq-2016/agdq-2.png" alt=""></p>

<p>The median donation amount is $20. What&rsquo;s interesting is that donations occur at clear break points: not only are there many donations at multiple of $10, but there are many donations at $25 and $75 as well. The $50 and $75 points also potentially benefited for being the threshold for entry into a <a href="https://gamesdonequick.com/tracker/prizes/agdq2016">grand prize raffle</a>. I&rsquo;ll note off-chart that there is a spike in $1,000 donations, the threshold for the audience-clapping in celebration and the <a href="https://gamesdonequick.com/tracker/donation/212588">top single donation</a> was made by an AGDQ sponsor, <a href="https://www.theyetee.com/">The Yetee</a>, at $18,225. The <a href="https://gamesdonequick.com/tracker/donation/209613">top donation by a non-sponsor</a> is from Minecraft creator Notch at $8,000, which he <a href="https://gamesdonequick.com/tracker/donation/234071">did twice</a>.</p>

<p>Which games are the most popular and generated the most amount of money for the Prevent Cancer Foundation?</p>

<p><img src="/img/agdq-2016/agdq-4.png" alt=""></p>

<p>Unsurprisingly, Nintendo games are the most popular due to the nostalgia factor. In fairness, the top runs on this chart occur during the last two days of AGDQ 2016, which as mentioned previously may have been affected by a rally, so we cannot assert causality. The appearance of <a href="http://yachtclubgames.com/shovel-knight/">Shovel Knight</a> and <a href="https://en.wikipedia.org/wiki/Bloodborne">Bloodborne</a> as leading donation games, both relatively recently released, shows that speedrunning has more appeal than just retro games.</p>

<p>A popular technique in charity drives is donation incentives, which help bolster the number of donations total. The <a href="https://gamesdonequick.com/tracker/bids/agdq2016">AGDQ bid incentives</a> can include bonus game segments, or certain game decisions, such as what name to give to a main character.</p>

<p>Any donation can optionally be assigned as a donation toward an incentive. Which run received the most money toward incentives?</p>

<p><img src="/img/agdq-2016/agdq-5.png" alt=""></p>

<p>Super Metroid donations incentives accounted for nearly <em>&frac14;th</em> of all the money raised at AGDQ. Final Fantasy IV accounted for a large amount as well, however, in both cases, the rally effect may apply. </p>

<p>Speaking of the Super Metroid donation incentives, it should be noted that this particular incentive is one of the most culturally-important incentives in the show. Super Metroid has an optional objective to Save The Animals from planetary destruction, but this costs time, and time is important for a speedrun. Or the speedruner can Kill The Animals through inaction for efficiency, &ldquo;Saving the Frames.&rdquo;</p>

<p>What is the split of these incentive choices?</p>

<p><img src="/img/agdq-2016/agdq-6.png" alt=""></p>

<p>Yes, the animals were killed (specifically, they were <strong>REKT</strong>, in the words of a last-minute donator). Both bonus games and vanity naming were popular, but nothing compared to the Save the Animals / Kill the Animals bid war.</p>

<p>Lastly, people can leave comments with donations, and these comments are usually read on-stream when possible, as you&rsquo;ve likely noticed if you&rsquo;re watched the videos above.</p>

<p>Here&rsquo;s a fun, nonscientific word cloud of those comments:</p>

<p><img src="/img/agdq-2016/agdq-7.png" alt=""></p>

<p>Lots of positivity, aside from the whole &ldquo;Kill the Animals&rdquo; thing. After all, the event is all about preventing cancer.</p>

<p>While this post isn&rsquo;t an academic analysis, it&rsquo;s neat to see to see what kinds of things drive donation to charity. This model of livestreaming and charitable applications is very successful, and important given the renewed attention toward livestreaming with the rise of Twitch to mainstream attention, alongside the rise of personal streaming with apps like Periscope. Donation incentives are a <em>very</em> successful technique for facilitating donations.</p>

<p>It will be interesting to see if Twitch and events like AGDQ can leverage charitable livestreaming, or if another startup/organization beats them first. Bidding-Wars-for-Deciding-the-Fate-of-Fictional-Animals-as-a-Service has a nice ring to it.</p>

<hr>

<p><em>You can access a Jupyter notebook with the data processing and chart processing code <a href="https://github.com/minimaxir/agdq-2016">in this GitHub repository</a>. If you use the processed donation data or visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks! :)</em></p>

<p><em>Note for donation incentive statistics: the user can theoretically split their donation among multiple incentives; unfortunately I assumed at time of scrape that all the money could only go toward one bid. All donations I investigated from the source data were toward a single incentive except two donations from The Yetee which I fixed manually. If there are any data discrepancies, that is the likely cause.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Movie Review Aggregator Ratings Have No Relationship with Box Office Success]]></title>
    <link href="http://minimaxir.com/2016/01/movie-revenue-ratings/"/>
    <updated>2016-01-07T08:30:00-08:00</updated>
    <id>http://minimaxir.com/2016/01/movie-revenue-ratings</id>
    <content type="html"><![CDATA[<p><a href="http://www.rottentomatoes.com">Rotten Tomatoes</a> has become synonymous with movie quality in recent years. The Rotten Tomatoes Tomatometer aggregates all reviews written by movie critics for a given movie on the internet, determines whether each reviewer rates the movie as &ldquo;Fresh&rdquo; or &ldquo;Rotten&rdquo; and calculates an average. If the proportion of Fresh reviews for a given movie is greater than or equal to 60%, the movie itself is considered &ldquo;Fresh&rdquo; and receives a special icon.</p>

<p><img src="/img/movie-revenue-ratings/examples.png" alt=""></p>

<p>Top Movies like Christopher Nolan&rsquo;s <a href="http://www.rottentomatoes.com/m/the_dark_knight/">The Dark Knight</a> received a 94% Rotten Tomatoes rating, and generated $533.3 million in domestic box office revenue. But other movies, like Michael Bay&rsquo;s <a href="http://www.rottentomatoes.com/m/transformers_revenge_of_the_fallen/">Transformers: Revenge of the Fallen</a>, received a 19% Tomatometer rating, but still generated $402.1 million in domestic box office revenue.</p>

<p>How strong is the relationship between Tomatometer scores and box office success, anyways? Or are other, better metrics? Time to make some pretty charts.</p>

<p>I obtained a large amount of movie data from the <a href="http://www.omdbapi.com">OMDb API</a>, which provides easy access to movie metadata from IMDb and Rotten Tomatoes. This data contains Rotten Tomatoes Tomatometer scores, Rotten Tomatoes Audience Scores, IMDb User Rankings, and Metacritic Scores. If you want to know how I processed the data in R and plotted the charts using ggplot2, I have <a href="https://www.youtube.com/watch?v=F5Hjlkxw_2A">prepared a screencast</a> for your viewing pleasure.</p>

<div class="responsive-video"><iframe src="http://www.youtube.com/embed/F5Hjlkxw_2A " style="width: 100%; height: 100%; border: none; padding-bottom: 20px" allowfullscreen></iframe></div>

<p>For this analysis, we will be looking at the <a href="http://www.r-statistics.com/2013/05/log-transformations-for-skewed-and-wide-distributions-from-practical-data-science-with-r/">log-transformation</a> of domestic box office revenue, since the values are skewed by mega-blockbusters like the ones mentioned previously. Revenues are not inflation-adjusted since the rating data is only present for recent years and due to the log-transformation already present, inflation correction would not impact this particular analysis much.</p>

<h2>Rotten Tomatoes Tomatometer</h2>

<p>After processing, I have a data subset of 4,863 movies with both Tomatometer and Box Office Gross values. Let&rsquo;s plot all those movies on a scatterplot of log(BoxOffice) vs. Meter with each point having a slight transparency; that way, clusters of points will be come apparent where the areas are darker on the chart.</p>

<p>We expect a positive linear relationship: movies with high Tomatometer scores to have high box office revenue, and inversely movies with low score to have low box office revenue.</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-1.png" alt=""></p>

<p>Wait, why does the trendline have a <em>negative</em> slope?</p>

<p>The <a href="https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient">Pearson correlation</a> between the Tomatometer scores and log(BoxOffice) is <strong>-0.18</strong>, implying a weak <em>negative</em> linear relationship between the two variables. Not what I expected.</p>

<p>There do appear to be clusters in the data. There is a group of points between $10M and $100M revenue and 0% to 20% Tomatometer rating.  Another group is present between $1,000 and $1M revenue and 80% to 100% RT rating. Both of these areas are outside of a linear relationship: perhaps these clusters are skewing trends too?</p>

<p>Let&rsquo;s try another visualization of the data using <a href="https://en.wikipedia.org/wiki/Contour_line">contour maps</a>, which allow the data to become 3D, so-to-speak. Using a 2D <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation">kernel density estimator</a>, we can identify and color areas on the plot according to the number of points present in that area; the greater the color saturation, the more points present in the given area.</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-2.png" alt=""></p>

<p>The two clusters mentioned previously are now much more apparent. It appears there are two distinct sets of movies: blockbusters which critics hate, and limited-appeal films which critics loves. Incidentally, there is no discernible difference between movies which are Fresh (&gt;60%) and Rotten.</p>

<h2>Metacritic</h2>

<p>The <a href="http://www.metacritic.com">Metacritic</a> score is also <a href="http://www.metacritic.com/about-metascores">derived from review data</a> by critics; however, instead of calculating a binary review sentiment and calculating a proportion from that sentiment, Metacritic gives a quantification from 0 to 100 to each critic review and averages them together.</p>

<p>Does that change the results for 4,479 movies?</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-7.png" alt=""></p>

<p>Correlation between Metacritic score and log(BoxOffice) is <strong>-0.13</strong>, which puts the analysis in a similar state as the Rotten Tomatoes data. However, the blockbuster cluster has shifted right, and the lesser-appeal cluster has shifted left.</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-8.png" alt=""></p>

<p>Clusters are much closer together.</p>

<p>Perhaps a review metric by non-critics will tell a different story.</p>

<h2>Rotten Tomatoes Audience Score</h2>

<p>The Audience Score is calculated in a similar way to the Rotten Tomatoes Tomatometer score: user to the site rate a movie from 0 to 5 stars in half-star increments (i.e. effectively a scale from 0-10) and the proportion of reviews with 3.5 star ratings or higher becomes the Audience Score.</p>

<p>This also presents a cognitive bias in ratings: the <a href="http://tvtropes.org/pmwiki/pmwiki.php/Main/FourPointScale">Four Point Scale</a>, where having a discrete form of ranking may cause people to tend to rate toward the top of the scale and make the entire metric skewed or misleading.</p>

<p>How does the Audience Score compare for 5,163 movies? After all, the audience is the group of people who determine how much money a movie makes at the Box Office.</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-3.png" alt=""></p>

<p>Correlation between the Audience score and log(BoxOffice) is <strong>0.05</strong>, which is a positive linear correlation, but representative of barely any practical correlation.</p>

<p>Speaking of the Four Point Scale, notice how, like with Metacritic score, there are barely any movies between 0% and 20% Audience Score. Is there really a skew? Let&rsquo;s look at the contours:</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-4.png" alt=""></p>

<p>The locations of the clusters are much different than that of Tomatometer clusters. Both clusters are closer together, with the blockbuster cluster between 50% and 60% audience score and the lesser-appeal cluster between 70% and 80%. Hence, the low correlation.</p>

<h2>IMDb</h2>

<p><a href="http://www.imdb.com">IMDb</a> works <a href="http://www.imdb.com/help/show_leaf?votestopfaq">almost the same way</a> as the Metacritic for non-critics: ratings from IMDb users between 1-10 (note that 0 is missing!) are averaged to get a final score.</p>

<p>How do 5,167 movies fare?</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-5.png" alt=""></p>

<p><strong>What?!</strong></p>

<p>The point groupings are at the <em>same</em> positions of ratings, and the correlation between IMDb ratings and log(BoxOffice) is <strong>0.00</strong>. Yes, there&rsquo;s <em>zero</em> correlation!</p>

<p>Checking the contour map confirms it:</p>

<p><img src="/img/movie-revenue-ratings/box-office-rating-6.png" alt=""></p>

<p>That is <em>literally</em> a Four Point Scale between 5 and 8!</p>

<p>The Rotten Tomatoes metric is the only metric that actually <em>uses</em> the entire rating scale. None of the other potential metrics provide more insight into a potential reason for high box-office revenue. Perhaps the movie rating system itself is broken.</p>

<p>That&rsquo;s not to say that movies need high box-office revenues to be considered successful. However, working with movie profitability, and by extension movie budget, is opening another can-of-worms with respect to data integrity. (that said, on Reddit, /u/chartmkr recently <a href="https://www.reddit.com/r/dataisbeautiful/comments/3zpp3w/movie_budgets_and_box_office_success_19552015_oc/">posted a visualization</a> of Gross vs. Budget which is interesting).</p>

<p>It&rsquo;ll still be fun to point to a Rotten Tomatoes Tomatometer rating as a kneejerk reaction to whether a movie rocks/sucks. Although, the reasons for movie financial success at the box office definitely warrant further investigation.</p>

<p><strong>UPDATE 1/11/15</strong>: On a <a href="https://news.ycombinator.com/item?id=10872076">discussion on Hacker News</a>, it was suggested that the blockbuster movies and the indie movies cancel each other out, i.e. blockbusters have a positive correlation and indies have a negative correlation.</p>

<p>For the blockbuster cluster alone, the log-correlation is <strong>0.23</strong> (not weak but not great positive correlation). For the indie cluster alone, the log-correlation is <strong>-0.12</strong> (same as original analysis).</p>

<p>For future analysis, it may be worthwhile to split these two clusters. I stand by the original analysis for this post: very frequently I&rsquo;ve heard the question &ldquo;is this a good movie?&rdquo; and the response is &ldquo;what does the RT score say?&rdquo; Both Box Office revenues and RT scores are important measures of quality (depending on perspective), and users who want to see or purchase a movie may not necessarily care if it&rsquo;s indie or a blockbuster.</p>

<p>User cwyers <a href="https://news.ycombinator.com/item?id=10878019">suggested</a> that Simpson&rsquo;s Paradox may be in play since the number of theaters showing a movie is positively correlated to box office revenue, adding a potentially-confounding affect. I will see if I can obtain that data for future analysis.</p>

<hr>

<p><em>You can access the open-sourced Jupyter notebook and high-resolution charts from this article in <a href="https://github.com/minimaxir/movie-revenue-ratings">this GitHub repository</a>. If you use the code or data visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks!</em></p>

<p><em>Unfortunately, I cannot redistribute the data itself due to licensing concerns.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Let's Code an Analysis and Visualizations of Yelp Data using R and ggplot2]]></title>
    <link href="http://minimaxir.com/2015/12/lets-code-1/"/>
    <updated>2015-12-28T09:00:00-08:00</updated>
    <id>http://minimaxir.com/2015/12/lets-code-1</id>
    <content type="html"><![CDATA[<p>One of the reasons I have open-sourced the code for my complicated data visualizations is transparency for the creation process. 2015 was a <a href="http://qz.com/580859/the-most-misleading-charts-of-2015-fixed/">year of misleading and incorrect data visualizations</a>, and I don&rsquo;t want to help contribute to the misconception that data can be used for trickery. &ldquo;Big data&rdquo; in particular is a area where the steps to reproduce results are rarely released publicly in a step-by-step manner, often in an attempt to make the resulting analysis unimpeachable.</p>

<p>It&rsquo;s time to take things to the next level of transparency by recording <a href="https://en.wikipedia.org/wiki/Screencast">screencasts</a> of my data analysis and visualizations.</p>

<p>Last week, ggplot2 author Hadley Wickham released <a href="http://blog.rstudio.org/2015/12/21/ggplot2-2-0-0/">a surprise update</a> for my favorite R package, bumping the version to 2.0.0. Why not celebrate by playing around with ggplot2 and making some pretty charts?</p>

<h2>Let&rsquo;s Code!</h2>

<p>I have recorded a screencast of myself coding in R to play around with data from <a href="http://www.yelp.com/dataset_challenge">Yelp Dataset Challenge</a> and <a href="https://www.youtube.com/watch?v=Emt9bn0D5ZI">uploaded it to YouTube</a>. Additionally, the video can be played at an unusually high quality for screencasting: 1440p on supported browsers, at 60 frames per second.</p>

<div class="responsive-video"><iframe src="http://www.youtube.com/embed/Emt9bn0D5ZI " style="width: 100%; height: 100%; border: none; padding-bottom: 20px" allowfullscreen></iframe></div>

<p>This particular screencast is also my first significant attempt at working with audio/video editing and voice-over. Feel free to provide suggestions for future videos.</p>

<p>Since the screencast is 40 minutes long (inadvertently!), I&rsquo;ve written an abridged summary of the screencast, along with some clarification of points made.</p>

<h2>Yelp Data v2</h2>

<p>A year ago I made a <a href="http://minimaxir.com/2014/09/one-star-five-stars/">blog post analyzing the same Yelp data</a>. Now that the data set contains 1.6 million reviews (as opposed to just 1.1 million back then), it might be interesting to look at it again to see if anything has changed. The data is formatted as by-line JSON: I wrote a pair of Python scripts to convert it to CSV for easy import into R.</p>

<p>The screencast centralizes on three R packages: readr, dplyr, and ggplot2. (all authored by Hadley Wickham)</p>

<p>Loading the dataset into R is easy and fast with <code>read_csv</code>.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_reviews <span class="o">&lt;-</span> read_csv<span class="p">(</span><span class="s">&quot;yelp_reviews.csv&quot;</span><span class="p">)</span></code></pre></figure>

<p>Since dplyr was loaded beforehand, read_csv loads the data into a tbl_df instead of a normal data.frame. When you call a normal data.frame by itself, <em>all data is printed to console</em>, which is a problem when you have 1.6M rows (yes, that happened during a test recording). Calling a tbl_df results in a very descriptive overview of the data:</p>

<p><img src="/img/lets-code-1/overview.png" alt=""></p>

<p>Most columns are self-explanatory. <code>review_length</code> is approximate number of words in the review, <code>pos_words</code> is the number of positive words in the review, <code>neg_words</code> is what you expect, <code>net_sentiment</code> is pos_words - neg_words.</p>

<p>A quick way to analyze the distribution of numerical data is to perform a summary on the data frame, which returns a by-column <a href="https://en.wikipedia.org/wiki/Five-number_summary">five-number summary</a> + mean:</p>

<p><img src="/img/lets-code-1/summary.png" alt=""></p>

<p>Ratings are biased toward 4 and 5 star reviews. There is a lot of skew for review length.</p>

<p>dplyr makes it easy to add columns in-line with the <code>mutate</code> command. Let&rsquo;s normalize the pos_words column:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_reviews <span class="o">&lt;-</span> df_reviews <span class="o">%&gt;%</span> mutate<span class="p">(</span>pos_norm <span class="o">=</span> pos_words <span class="o">/</span> review_length<span class="p">)</span></code></pre></figure>

<p>And we could do similar steps for the neg_words column too. Or use mutate to transform the data of an existing column.</p>

<p>Onto ggplot2. If you want a quick histogram of univariate data, qplot does just that. Let&rsquo;s visualize the distribution of stars.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">qplot<span class="p">(</span>data<span class="o">=</span>df_reviews<span class="p">,</span> stars<span class="p">)</span></code></pre></figure>

<p><img src="/img/lets-code-1/lc1-qplot-1.png" alt=""></p>

<p>Definitely a skew toward 4 and 5 star reviews.</p>

<p>We can do that for other variables too, like review length.</p>

<p><img src="/img/lets-code-1/lc1-qplot-2.png" alt=""></p>

<p>What about bivariate data? If you give two variables to qplot, it will create a scatter plot. Perhaps there is a relationship between the number of stars and the number of positive words?</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">qplot<span class="p">(</span>data<span class="o">=</span>df_reviews<span class="p">,</span> stars<span class="p">,</span> pos_words<span class="p">)</span></code></pre></figure>

<p>&hellip;and then we run into a problem. In this case, ggplot2 has to plot 1.6M points to screen, which can take awhile, especially if you are simultaneously using your GPU for video recording. Eventually, we get this:</p>

<p><img src="/img/lets-code-1/lc1-qplot-3.png" alt=""></p>

<p>At first glance, there appears to be a positive correlation between star rating and number of positive words, but that&rsquo;s misleading: since we don&rsquo;t have alpha transparency on the points, the density is ambiguous. (fixing it requires working outside of a qplot).</p>

<h2>Serious Business Data</h2>

<p>We load the Yelp Businesses data into R through the same way as the reviews data. Here&rsquo;s an overview of the data:</p>

<p><img src="/img/lets-code-1/businesses.png" alt=""></p>

<p>Both data frames have a <code>business_id</code> column. We can merge them with a <code>left_join</code>, a la SQL. If both data frames have a column with the same name, it will merge on that column by default.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_reviews <span class="o">&lt;-</span> df_reviews <span class="o">%&gt;%</span> left_join<span class="p">(</span>df_businesses<span class="p">)</span></code></pre></figure>

<p>Then the R console helpfully points out that both dataframes also have a &ldquo;stars&rdquo; column. Uh-oh.</p>

<p>We reset the df_reviews data frame from scratch and merge again, explicitly stating the &ldquo;by&rdquo; column for merging. Now we know <em>where</em> reviews were made, and that might provide helpful information.</p>

<h2>Aggregation Station</h2>

<p>It might be interesting to know the average star rating by city. dplyr allows for <code>group_by</code> and <code>summarize</code> operations in a similar manner as SQL.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_cities <span class="o">&lt;-</span> df_reviews <span class="o">%&gt;%</span> group_by<span class="p">(</span>city<span class="p">)</span> <span class="o">%&gt;%</span> summarize<span class="p">(</span>avg_stars <span class="o">=</span> <span class="kp">mean</span><span class="p">(</span>stars.x<span class="p">))</span></code></pre></figure>

<p><img src="/img/lets-code-1/cities.png" alt=""></p>

<p>&hellip;that&rsquo;s not good. The original Yelp Dataset Challenge page mentioned that the dataset is only from specific cities, not &ldquo;1023 E Frye Rd.&rdquo;</p>

<p><img src="/img/lets-code-1/Dataset_Map.png" alt=""></p>

<p>Hmrph.</p>

<p>From the map, it appears there is no overlap between any of the cities with geographic states, so let&rsquo;s use <code>state</code> instead. Additionally, we can add a count of reviews from that state, and sort by that count descending.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_states <span class="o">&lt;-</span> df_reviews <span class="o">%&gt;%</span> group_by<span class="p">(</span>state<span class="p">)</span> <span class="o">%&gt;%</span> summarize<span class="p">(</span>avg_stars <span class="o">=</span> <span class="kp">mean</span><span class="p">(</span>stars.x<span class="p">),</span> count<span class="o">=</span>n<span class="p">())</span> <span class="o">%&gt;%</span> arrange<span class="p">(</span>desc<span class="p">(</span>count<span class="p">))</span></code></pre></figure>

<p><img src="/img/lets-code-1/states.png" alt=""></p>

<p>Looks good enough, but that&rsquo;s tempting fate.</p>

<h2>ggplot All the Things</h2>

<p>We can plot state vs. avg_stars with ggplot2. Setting it up is easy:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">ggplot<span class="p">(</span>data<span class="o">=</span>df_states<span class="p">,</span> aes<span class="p">(</span>state<span class="p">,</span> avg_stars<span class="p">))</span></code></pre></figure>

<p><img src="/img/lets-code-1/lc1-ggplot-1.png" alt=""></p>

<p>The blank plot is actually new to 2.0.0: running the code without any layers would normally throw an error. The axis values appear valid. Let&rsquo;s add columns via <code>geom_bar</code>:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">ggplot<span class="p">(</span>data<span class="o">=</span>df_states<span class="p">,</span> aes<span class="p">(</span>state<span class="p">,</span> avg_stars<span class="p">))</span> <span class="o">+</span> geom_bar<span class="p">()</span></code></pre></figure>

<p>&hellip;and this results in an error. geom_bar by itself does histograms on raw values, as shown in the qplots. The correct fix is to add a <code>stat=&quot;identity&quot;</code> parameter to geom_bar, which tells it to scale the bars by the given value of the aesthetic.</p>

<p><img src="/img/lets-code-1/lc1-ggplot-2.png" alt=""></p>

<p>Better. But the x-axis is cluttered and the States would look better on the y-axis. Time for a <code>coord_flip</code>.</p>

<p><img src="/img/lets-code-1/lc1-ggplot-3.png" alt=""></p>

<p>Better. Now time to fix the order. You may notice that the order of the states is alphabetical going from the bottom of the axis to the top, and R will always set this order for any character vector. We want the sort the labels by their average star rating, descending. To do that we change the internal factor labels of state volume to the specified order.</p>

<p>In the recording, this took awhile due to several brain farts (which happen often when dealing with factor ordering). First, we need to remove a few states with few reviews using a filter The easiest way to do this is to sort the original data frame by avg_stars descending, then set the factor order by using the new state order <em>in reverse</em>. (Ok, ok, it might be easier to just sort ascending and not reverse, but it makes the overview harder to visualize)</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_states <span class="o">&lt;-</span> df_states <span class="o">%&gt;%</span> arrange<span class="p">(</span>desc<span class="p">(</span>avg_stars<span class="p">))</span> <span class="o">%&gt;%</span> filter<span class="p">(</span>count <span class="o">&gt;</span> <span class="m">2000</span><span class="p">)</span> <span class="o">%&gt;%</span> mutate<span class="p">(</span>state <span class="o">=</span> <span class="kp">factor</span><span class="p">(</span>state<span class="p">,</span> levels<span class="o">=</span><span class="kp">rev</span><span class="p">(</span>state<span class="p">)))</span></code></pre></figure>

<p><img src="/img/lets-code-1/states-2.png" alt=""></p>

<p>Rerunning the plot code afterward yields:</p>

<p><img src="/img/lets-code-1/lc1-ggplot-4.png" alt=""></p>

<p>Good! Why not add labels for each point? This can be done with geom_text, along with adding <code>hjust=1</code> to offset the label, changing the size, and setting the text to white. We can round the avg_star values to 2 decimal places as well.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">ggplot<span class="p">(</span>data<span class="o">=</span>df_states<span class="p">,</span> aes<span class="p">(</span>state<span class="p">,</span> avg_stars<span class="p">))</span> <span class="o">+</span> geom_bar<span class="p">(</span>stat<span class="o">=</span><span class="s">&quot;identity&quot;</span><span class="p">)</span> <span class="o">+</span> coord_flip<span class="p">()</span> <span class="o">+</span> geom_text<span class="p">(</span>aes<span class="p">(</span>label<span class="o">=</span><span class="kp">round</span><span class="p">(</span>avg_stars<span class="p">,</span> <span class="m">2</span><span class="p">)),</span> hjust<span class="o">=</span><span class="m">1</span><span class="p">,</span> color<span class="o">=</span><span class="s">&quot;white&quot;</span><span class="p">)</span></code></pre></figure>

<p><img src="/img/lets-code-1/lc1-ggplot-5.png" alt=""></p>

<p>The &ldquo;3.7&rdquo; label requires using the <code>sprintf</code> function instead of <code>round</code> to print &ldquo;3.70&rdquo;, which is not fun. Otherwise, these labels are nice so far. Why not add a theme and axis labels?</p>

<p>I go to my <a href="http://minimaxir.com/2015/02/ggplot-tutorial/">previous ggplot2 tutorial</a> and copy-paste the FiveThirtyEight-inspired theme from there because I am efficient. (The theme required loading the RColorBrewer package, though). The axis labels are added through the <code>labs</code> function. (note that since the axes are flipped, the labels must be flipped too!)</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">ggplot<span class="p">(</span>data<span class="o">=</span>df_states<span class="p">,</span> aes<span class="p">(</span>state<span class="p">,</span> avg_stars<span class="p">))</span> <span class="o">+</span> geom_bar<span class="p">(</span>stat<span class="o">=</span><span class="s">&quot;identity&quot;</span><span class="p">)</span> <span class="o">+</span> coord_flip<span class="p">()</span> <span class="o">+</span> geom_text<span class="p">(</span>aes<span class="p">(</span>label<span class="o">=</span><span class="kp">round</span><span class="p">(</span>avg_stars<span class="p">,</span> <span class="m">2</span><span class="p">)),</span> hjust<span class="o">=</span><span class="m">2</span><span class="p">,</span> size<span class="o">=</span><span class="m">2</span><span class="p">,</span> color<span class="o">=</span><span class="s">&quot;white&quot;</span><span class="p">)</span> <span class="o">+</span> fte_theme<span class="p">()</span> <span class="o">+</span> labs<span class="p">(</span>y<span class="o">=</span><span class="s">&quot;Average Star Rating by State&quot;</span><span class="p">,</span> x<span class="o">=</span><span class="s">&quot;State&quot;</span><span class="p">,</span> title<span class="o">=</span><span class="s">&quot;Average Yelp Review Star Ratings by State&quot;</span><span class="p">)</span></code></pre></figure>

<p><img src="/img/lets-code-1/lc1-ggplot-6.png" alt=""></p>

<p>Why not add 95% confidence intervals for each average? (Note that the normality assumptions for the confidence interval may not be entirely valid). We can calculate the standard error of the mean and rebuild the dataframe and reorder factors again.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_states <span class="o">&lt;-</span> df_reviews <span class="o">%&gt;%</span> group_by<span class="p">(</span>state<span class="p">)</span> <span class="o">%&gt;%</span> summarize<span class="p">(</span>avg_stars <span class="o">=</span> <span class="kp">mean</span><span class="p">(</span>stars.x<span class="p">),</span> count<span class="o">=</span>n<span class="p">(),</span> se_mean<span class="o">=</span>sd<span class="p">(</span>stars.x<span class="p">)</span><span class="o">/</span><span class="kp">sqrt</span><span class="p">(</span>count<span class="p">))</span> <span class="o">%&gt;%</span> arrange<span class="p">(</span>desc<span class="p">(</span>avg_stars<span class="p">))</span> <span class="o">%&gt;%</span> filter<span class="p">(</span>count <span class="o">&gt;</span> <span class="m">2000</span><span class="p">)</span> <span class="o">%&gt;%</span> mutate<span class="p">(</span>state <span class="o">=</span> <span class="kp">factor</span><span class="p">(</span>state<span class="p">,</span> levels<span class="o">=</span><span class="kp">rev</span><span class="p">(</span>state<span class="p">)))</span></code></pre></figure>

<p>Time to add a <code>geom_errorbar</code> (not a <code>geom_crossbar</code>!)</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">ggplot<span class="p">(</span>data<span class="o">=</span>df_states<span class="p">,</span> aes<span class="p">(</span>state<span class="p">,</span> avg_stars<span class="p">))</span> <span class="o">+</span> geom_bar<span class="p">(</span>stat<span class="o">=</span><span class="s">&quot;identity&quot;</span><span class="p">)</span> <span class="o">+</span> coord_flip<span class="p">()</span> <span class="o">+</span> geom_text<span class="p">(</span>aes<span class="p">(</span>label<span class="o">=</span><span class="kp">round</span><span class="p">(</span>avg_stars<span class="p">,</span> <span class="m">2</span><span class="p">)),</span> hjust<span class="o">=</span><span class="m">2</span><span class="p">,</span> size<span class="o">=</span><span class="m">2</span><span class="p">,</span> color<span class="o">=</span><span class="s">&quot;white&quot;</span><span class="p">)</span> <span class="o">+</span> fte_theme<span class="p">()</span> <span class="o">+</span> labs<span class="p">(</span>y<span class="o">=</span><span class="s">&quot;Average Star Rating by State&quot;</span><span class="p">,</span> x<span class="o">=</span><span class="s">&quot;State&quot;</span><span class="p">,</span> title<span class="o">=</span><span class="s">&quot;Average Yelp Review Star Ratings by State&quot;</span><span class="p">)</span> <span class="o">+</span> geom_errorbar<span class="p">(</span>aes<span class="p">(</span>ymin<span class="o">=</span>avg_stars <span class="o">-</span> <span class="m">1.96</span> <span class="o">*</span> se_mean<span class="p">,</span> ymax<span class="o">=</span>avg_stars <span class="o">+</span> <span class="m">1.96</span> <span class="o">*</span> se_mean<span class="p">))</span></code></pre></figure>

<p><img src="/img/lets-code-1/lc1-ggplot-7.png" alt=""></p>

<p>Averages are very stable for all cities due to the large sample size.</p>

<p>At this point I realized the recording is too long and I end it there. For a normal blog post, I&rsquo;d add more theming, adjust colors so they don&rsquo;t clash, and add annotations, such as a line representing the true review average from the population. And ideally, performing statistical tests to determine if any averages are different from the population average.</p>

<p>Hopefully this gives some insight into the mechanical process of creating simple data visualizations with R and ggplot2 (the &ldquo;abridged summary&rdquo; ended up being as long as a typical blog post!). As my screencast shows, programming is a recurring process of saying &ldquo;this is easy to do!&rdquo; then failing miserably for stupid reasons. Even after the 40 minute screencast, there&rsquo;s still much, much more polish needed for the data visualization. My blog posts take a very long time to produce for those reasons; the clear, clean code from the finished product is not indicative of the unexpected errors that occur when writing it.</p>

<p>I did this recording &ldquo;blind&rdquo; to test whether or not it&rsquo;s feasible for me to <em>stream</em> the coding of data visualization on services like <a href="http://www.twitch.tv">Twitch</a>. It&rsquo;s definitely possible, but has more logistical challenges. (namely, that <a href="https://obsproject.com">OBS</a> is fussy outside of Windows and I still need to figure out how to configure it optimally). I admit the code in this screencast may not be the highest-quality code (in retrospect I should have put the code in an editor instead of directly in the console, and reuse dataframe/ggplot objects), but the transparent process for coding data visualizations is important. If there is enough interest, I may revisit Yelp data again, or even more advanced datasets.</p>

<hr>

<p><em>You can access the R code used for the data visualizations and the Python scripts used to process the raw Yelp dataset <a href="https://github.com/minimaxir/lets-code-1">in this GitHub repository</a>. However, the raw data itself cannot be redistributed.</em></p>

<p><em>For those wondering what I used for recording the screencast:</em></p>

<p>Computer: <em>Late 2013 13&quot; Retina MacBook Pro running OS X 10.11.2</em></p>

<p>Recording Software: <em>Screenflow 4.5</em></p>

<p>Microphone: <em>Shure MV5 Digital Condenser</em></p>

<p>Music: <em>Various artists from the &ldquo;No Attribution Required&rdquo; section of the YouTube Audio Library</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mapping Where Arrests Frequently Occur in San Francisco Using Crime Data]]></title>
    <link href="http://minimaxir.com/2015/12/sf-arrest-maps/"/>
    <updated>2015-12-07T08:00:00-08:00</updated>
    <id>http://minimaxir.com/2015/12/sf-arrest-maps</id>
    <content type="html"><![CDATA[<p>In my previous post, <a href="http://minimaxir.com/2015/12/sf-arrests/">Analyzing San Francisco Crime Data to Determine When Arrests Frequently Occur</a>, I found out that there are trends where SF Police arrests occur more frequently than others. By processing the <a href="https://data.sfgov.org/Public-Safety/SFPD-Incidents-from-1-January-2003/tmnf-yvry">SFPD Incidents dataset</a> from the <a href="https://data.sfgov.org">SF OpenData portal</a>, I found that arrests typically occur on Wednesdays at 4-5 PM, and that the type of crime is relevant to the frequency of the crime. (e.g. DUIs happen late Friday/Saturday night).</p>

<p>However, I could not understand <em>why</em> Wednesday/4-5PM is a peak time for arrests. In addition to analyzing <em>when</em> arrests occur, I also looked at <em>where</em> arrests occur. For example, perhaps more crime happens as people are leaving work; in that case, we would expect to see crimes downtown.</p>

<h2>Making a Map of SF Arrests</h2>

<p>Continuing from the previous analysis, I have a data frame of all police arrests that have occurred in San Francisco from 2003 - 2015 (587,499 arrests total).</p>

<p><img src="/img/sf-arrest-map/arrests.png" alt=""></p>

<p>What is the most efficient way to make a map for the data? There are too many data points for rendering each point in a tool like <a href="http://www.tableau.com">Tableau</a> or <a href="https://www.google.com/maps">Google Maps</a>. I can use <code>ggplot2</code> again as I did <a href="http://minimaxir.com/2015/11/nyc-ggplot2-howto/">to make a map of New York City</a> manually, but as noted in that article, the abstract nature of the map may hide information.</p>

<p>Enter <code>ggmap</code>. ggmap, an <a href="https://github.com/dkahle/ggmap">R package by David Kahle</a>, is a tool that allows the user to retrieve a map image from a number of map data providers, and integrates seamlessly with ggplot2 for simple visualization creation. Kahle and Hadley Wickham (the creator of ggplot2) <a href="https://journal.r-project.org/archive/2013-1/kahle-wickham.pdf">coauthored a paper</a> describing practical applications of ggmap.</p>

<p>I will include most of the map generation code in-line. <em>For more detailed code and output, a <a href="https://github.com/minimaxir/sf-arrests-when-where/blob/master/crime_data_sf.ipynb">Jupyter notebook</a> containing the code and visualizations used in this article is available open-source on GitHub.</em></p>

<p>By default, you can ask ggmap just for a location using <code>get_map()</code>, and it will give you an approximate map around that location. You can configure the zoom level on that point as well. Optionally, if you need precise bounds for the map, you can set the bounding box manually, and the <a href="http://boundingbox.klokantech.com">Bounding Box Tool</a> works extremely well for this purpose, with the CSV coordinate export already being in the correct format.</p>

<p>ggmap allows maps from sources such as <a href="https://www.google.com/maps">Google Maps</a> and <a href="http://www.openstreetmap.org/#map=5/51.500/-0.100">OpenStreetMap</a>, and the maps can be themed, such as a color map of a black-and-white map. A black-and-white minimalistic map would be best for readability. A <a href="https://www.reddit.com/r/dataisbeautiful/comments/3ule41/mapping_restaurants_in_san_francisco_by_health/">Reddit submission by /u/all_genes_considered</a> used <a href="http://maps.stamen.com/#terrain/12/37.7706/-122.3782">Stamen maps</a> as a source with the toner-lite theme, and that worked well.</p>

<p>Since we&rsquo;ve identified the map parameters, now we can request an appropriate map:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">bbox <span class="o">=</span> <span class="kt">c</span><span class="p">(</span><span class="m">-122.516441</span><span class="p">,</span><span class="m">37.702072</span><span class="p">,</span><span class="m">-122.37276</span><span class="p">,</span><span class="m">37.811818</span><span class="p">)</span>

map <span class="o">&lt;-</span> get_map<span class="p">(</span>location <span class="o">=</span> bbox<span class="p">,</span> <span class="kn">source</span> <span class="o">=</span> <span class="s">&quot;stamen&quot;</span><span class="p">,</span> maptype <span class="o">=</span> <span class="s">&quot;toner-lite&quot;</span><span class="p">)</span></code></pre></figure>

<p>Plotting the map by itself results in something like this:</p>

<p><img src="/img/sf-arrest-map/sf-arrest-where-0.png" alt=""></p>

<p>On the right track (aside from two Guerrero Streets), but obviously it&rsquo;ll need some aesthetic adjustments.</p>

<p>Let&rsquo;s plot all 587,499 arrests on top of the map for fun and see what happens.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggmap<span class="p">(</span>map<span class="p">)</span> <span class="o">+</span>
            geom_point<span class="p">(</span>data <span class="o">=</span> df_arrest<span class="p">,</span> aes<span class="p">(</span>x<span class="o">=</span>X<span class="p">,</span> y<span class="o">=</span>Y<span class="p">),</span> color <span class="o">=</span> <span class="s">&quot;#27AE60&quot;</span><span class="p">,</span> size <span class="o">=</span> <span class="m">0.5</span><span class="p">,</span> alpha <span class="o">=</span> <span class="m">0.01</span><span class="p">)</span> <span class="o">+</span>
            fte_theme<span class="p">()</span> <span class="o">+</span>
            theme<span class="p">(</span><span class="kc">...</span><span class="p">)</span> <span class="o">+</span>
            labs<span class="p">(</span>title <span class="o">=</span> <span class="s">&quot;Locations of Police Arrests Made in San Francisco from 2003  2015&quot;</span><span class="p">)</span></code></pre></figure>

<ul>
<li><code>ggmap()</code> sets up the base map.</li>
<li><code>geom_point()</code> plots points. The data for plotting is specified at this point. &ldquo;color&rdquo; and &ldquo;size&rdquo; parameters do just that. An alpha of 0.01 causes each point to be 99% transparent; therefore, addresses with a lot of points will be more opaque.</li>
<li><code>fte_theme()</code> is my theme based on the FiveThirtyEight style.</li>
<li><code>theme()</code> is needed for a few additional theme tweaks to remove the axes/margins</li>
<li><code>labs()</code> is for labeling the plot (<em>always</em> label!)</li>
</ul>

<p>Rendering the plot results in:</p>

<p><a href="http://i.imgur.com/Xu8wXzc.png" target=_blank><img src="/img/sf-arrest-map/sf-arrest-where-1.png" alt=""></a></p>

<p><em>NOTE: All maps in this article are embedded at a lower size to ensure that the article doesn&rsquo;t take days to load. To load a high-resolution version of any map, click on it and it will open in a new tab.</em></p>

<p><em>Additionally, since ggmap forces plots to a fixed ratio, this results in the &ldquo;random white space&rdquo; problem mentioned in the NYC article, for which I still have not found a solution, but have minimized the impact.</em></p>

<p>It&rsquo;s clear to see where arrests in the city occur. A large concentration in the Tenderloin and the Mission, along with clusters in Bayview and Fisherman&rsquo;s Wharf. However, point-stacking is not helpful when comparing high-density areas, so this visualization can be optimized.</p>

<p>How about faceting by type of crime again? We can render a map of San Francisco for each type of crime, and then we can see if the clusters for a given type of crime are different from others.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggmap<span class="p">(</span>map<span class="p">)</span> <span class="o">+</span>
            geom_point<span class="p">(</span>data <span class="o">=</span> df_arrest <span class="o">%&gt;%</span> filter<span class="p">(</span>Category <span class="o">%in%</span> df_top_crimes<span class="o">$</span>Category<span class="p">[</span><span class="m">2</span><span class="o">:</span><span class="m">19</span><span class="p">]),</span> aes<span class="p">(</span>x<span class="o">=</span>X<span class="p">,</span> y<span class="o">=</span>Y<span class="p">,</span> color<span class="o">=</span>Category<span class="p">),</span> size<span class="o">=</span><span class="m">0.75</span><span class="p">,</span> alpha<span class="o">=</span><span class="m">0.05</span><span class="p">)</span> <span class="o">+</span>
            fte_theme<span class="p">()</span> <span class="o">+</span>
            theme<span class="p">(</span><span class="kc">...</span><span class="p">)</span> <span class="o">+</span>
            labs<span class="p">(</span>title <span class="o">=</span> <span class="s">&quot;Locations of Police Arrests Made in San Francisco from 2003  2015, by Type of Crime&quot;</span><span class="p">)</span> <span class="o">+</span>
            facet_wrap<span class="p">(</span><span class="o">~</span> Category<span class="p">,</span> nrow <span class="o">=</span> <span class="m">3</span><span class="p">)</span></code></pre></figure>

<p>Again, only one line of new code for the facet, although the source data needs to be filtered as it was in the previous post.</p>

<p>Running the code yields:</p>

<p><a href="http://i.imgur.com/6NgzV3k.jpg" target=_blank><img src="/img/sf-arrest-map/sf-arrest-where-2.png" alt=""></a></p>

<p>This is certainly more interesting (and pretty). Some crimes, such as Assaults, Drugs/Narcotics, and Warrants, occur all over the city. Other crimes, such as Disorderly Conduct and Robbery, primarily have clusters in the Tenderloin and in the Mission close to the 16th Street BART stop. (Prostitution notably has a cluster in the Mission and a cluster <em>above</em> the Tenderloin.)</p>

<p>Again, we can&rsquo;t compare high-density points, so now we should probably normalize the data by facet. One way to do this is to weight each point by the reciprocal of the number of points in the facet (e.g. if there are 5,000 Fraud arrests, assign a weight of 1/5000 to each Fraud arrest), and aggregate the sums of the weights in a geographical area.</p>

<p>We can reuse the normalization code from the previous post, and the hex overlay code from my NYC taxi plot post as well:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">sum_thresh <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">,</span> threshold <span class="o">=</span> <span class="m">10</span><span class="o">^</span><span class="m">-3</span><span class="p">)</span> <span class="p">{</span>
    <span class="kr">if</span> <span class="p">(</span><span class="kp">sum</span><span class="p">(</span>x<span class="p">)</span> <span class="o">&lt;</span> threshold<span class="p">)</span> <span class="p">{</span><span class="kr">return</span> <span class="p">(</span><span class="kc">NA</span><span class="p">)}</span>
    <span class="kr">else</span> <span class="p">{</span><span class="kr">return</span> <span class="p">(</span><span class="kp">sum</span><span class="p">(</span>x<span class="p">))}</span>
<span class="p">}</span>

plot <span class="o">&lt;-</span> ggmap<span class="p">(</span>map<span class="p">)</span> <span class="o">+</span>
            stat_summary_hex<span class="p">(</span>data <span class="o">=</span> df_arrest <span class="o">%&gt;%</span> filter<span class="p">(</span>Category <span class="o">%in%</span> df_top_crimes<span class="o">$</span>Category<span class="p">[</span><span class="m">2</span><span class="o">:</span><span class="m">19</span><span class="p">])</span> <span class="o">%&gt;%</span> group_by<span class="p">(</span>Category<span class="p">)</span> <span class="o">%&gt;%</span> mutate<span class="p">(</span>w<span class="o">=</span><span class="m">1</span><span class="o">/</span>n<span class="p">()),</span> aes<span class="p">(</span>x<span class="o">=</span>X<span class="p">,</span> y<span class="o">=</span>Y<span class="p">,</span> z<span class="o">=</span>w<span class="p">),</span> fun<span class="o">=</span>sum_thresh<span class="p">,</span> alpha <span class="o">=</span> <span class="m">0.8</span><span class="p">,</span> color<span class="o">=</span><span class="s">&quot;#CCCCCC&quot;</span><span class="p">)</span> <span class="o">+</span>
            fte_theme<span class="p">()</span> <span class="o">+</span>
            theme<span class="p">(</span><span class="kc">...</span><span class="p">)</span> <span class="o">+</span>
            scale_fill_gradient<span class="p">(</span>low <span class="o">=</span> <span class="s">&quot;#DDDDDD&quot;</span><span class="p">,</span> high <span class="o">=</span> <span class="s">&quot;#2980B9&quot;</span><span class="p">)</span> <span class="o">+</span>
            labs<span class="p">(</span>title <span class="o">=</span> <span class="s">&quot;Locations of Police Arrests Made in San Francisco from 2003  2015, Normalized by Type of Crime&quot;</span><span class="p">)</span> <span class="o">+</span>
            facet_wrap<span class="p">(</span><span class="o">~</span> Category<span class="p">,</span> nrow <span class="o">=</span> <span class="m">3</span><span class="p">)</span></code></pre></figure>

<ul>
<li><code>sum_thresh()</code> is a helper function that aggregates the sums of weights, but will not plot the corresponding hex if there is not enough data at that location.</li>
<li><code>scale_fill_gradient()</code> sets the gradient for the chart. If there are few arrests, the hex will be gray; if there are many arrests, it will be deep blue.</li>
</ul>

<p>Putting it all together:</p>

<p><a href="http://i.imgur.com/LXKPseq.jpg" target=_blank><img src="/img/sf-arrest-map/sf-arrest-where-3.png" alt=""></a></p>

<p>This confirms the interpretations mentioned above.</p>

<p>Since the code base is already created, it is very simple to facet on any variable. So why not create a faceted map for <em>every conceivable variable</em>?</p>

<p>How about checking arrest locations by Police Districts?</p>

<p><a href="http://i.imgur.com/i82wsIZ.png" target=_blank><img src="/img/sf-arrest-map/sf-arrest-where-4.png" alt=""></a></p>

<p>The map shows that the hex plotting works correctly, at the least. Notably, the Central, Northern, and Southern Police Districts end up making a large proportion of their arrests nearby the Tenderloin/Market Street instead of anywhere else in their area of perview.</p>

<p>Is the location of arrests seasonal? Does it vary by the month the arrest occured?</p>

<p><a href="http://i.imgur.com/u2eQMZf.png" target=_blank><img src="/img/sf-arrest-map/sf-arrest-where-5.png" alt=""></a></p>

<p>Nope. Still Tenderloin and Mission.</p>

<p>Maybe the locations of arrests have changed over time, as legal polices changed. Let&rsquo;s facet by year.</p>

<p><a href="http://i.imgur.com/x4SRnkU.png" target=_blank><img src="/img/sf-arrest-map/sf-arrest-where-6.png" alt=""></a></p>

<p>Here things are <em>slightly</em> different across each facet; Tenderloin had a much higher concentration of arrests peaking in 2009-2010, and the concentration of yearly arrests in the Tenderloin has decreased relative to everywhere else in the city.</p>

<p>Does the location of arrests vary by the time of day? As noted earlier, there could be more arrests downtown during working hours.</p>

<p><a href="http://i.imgur.com/cDKo8Lt.png" target=_blank><img src="/img/sf-arrest-map/sf-arrest-where-7.png" alt=""></a></p>

<p>Higher relative concentration in Tenderlion/Mission during work hours, lesser during the night.</p>

<p>Last try. Perhaps the day of week leads to different locations, especially as people tend to go out to bars all across the city.</p>

<p><a href="http://i.imgur.com/tNBRilL.png" target=_blank><img src="/img/sf-arrest-map/sf-arrest-where-8.png" alt=""></a></p>

<p><em>Zero</em> difference. Eh.</p>

<p>We did learn that there is certainly a lot of arrests in the Tenderloin and 16th Street/Mission BART stop, though. However, that doesn&rsquo;t necessarily mean there is more <em>crime</em> in those areas (correlation does not imply causation), but it is something worth noting when traveling around the San Francisco.</p>

<h2>Bonus: Do Social Security payments lead to an increase in arrests?</h2>

<p>In response to my previous article, <a href="https://www.reddit.com/r/sanfrancisco/comments/3vfgg2/analyzing_san_francisco_crime_data_to_determine/cxn29wd">Redditor /u/NowProveIt hypothesizes</a> that the spike in Wednesday arrests could be attributed to <a href="https://www.ssa.gov/kc/rp_paybenefits.htm">Social Security disability</a> (RDSI) payments. The <a href="https://www.socialsecurity.gov/pubs/EN-05-10031-2015.pdf">Social Security Benefit Payments schedule</a> is typically every second, third, and fourth Wednesday of a month.</p>

<p>Normally, you would expect that the arrest behavior for any Wednesday in a given month to be independent from each other. Therefore, if the arrest behavior for the <em>first</em> Wednesday is different than that for the secord/third/fourth Wednesday (presumably, the First Wednesday has fewer arrests overall), then we might have a lead.</p>

<p>Through more <code>dplyr</code> shenanigans, I am able to filter the dataset of arrests to Wednesday arrests only, and classify each Wednesday as the first, second, third, or fourth of the month. (there are occasionally fifth Wednesdays but no one cares about those).</p>

<p><img src="/img/sf-arrest-map/ordinal.png" alt=""></p>

<p>We can plot a single line chart for each ordinal of the number of arrests over the day. We are looking to see if the First Wednesday has different behavior than the other Wednesdays.</p>

<p><img src="/img/sf-arrest-map/ssi-crime-1.png" alt=""></p>

<p>&hellip;and it doesn&rsquo;t.</p>

<p>Looking at locations data doesn&rsquo;t help either.</p>

<p><a href="http://i.imgur.com/wTDKjOm.png" target=_blank><img src="/img/sf-arrest-map/ssi-crime-2.png" alt=""></a></p>

<p>Oh well, it was worth a shot.</p>

<p>As always, all the code and raw images are available <a href="https://github.com/minimaxir/sf-arrests-when-where">in the GitHub repository</a>. Not many more questions were answered by looking at the location data of San Francisco crimes. But that&rsquo;s OK. There&rsquo;s certainly other cool things to do with this data. Kaggle, for instance, is creating <a href="https://www.kaggle.com/c/sf-crime/scripts">a repository of scripts</a> which play around with the Crime Incident dataset.</p>

<p>But for now, at least I made a few pretty charts out of it.</p>

<hr>

<p><em>If you use the code or data visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks!</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Analyzing San Francisco Crime Data to Determine When Arrests Frequently Occur]]></title>
    <link href="http://minimaxir.com/2015/12/sf-arrests/"/>
    <updated>2015-12-04T08:00:00-08:00</updated>
    <id>http://minimaxir.com/2015/12/sf-arrests</id>
    <content type="html"><![CDATA[<p>The <a href="https://data.sfgov.org">SF OpenData portal</a> is a good source for detailed statistics about San Francisco. One of the most popular datasets on the portal is the <a href="https://data.sfgov.org/Public-Safety/SFPD-Incidents-from-1-January-2003/tmnf-yvry">SFPD Incidents dataset</a>, which contains a tabular list of 1,842,050 reports (at time of writing) from 2003 to present.</p>

<p><img src="/img/sf-arrests/incident-data.png" alt=""></p>

<p>The data can be exported into a 377.9 MB CSV; not large enough to be considered &ldquo;big data,&rdquo; but still too heavy for programs like Excel to process efficiently. Let&rsquo;s take a look at the data using <a href="https://www.r-project.org">R</a> and see if there&rsquo;s anything interesting.</p>

<h2>Processing the Data</h2>

<p>For this article, I&rsquo;m going to do something different and illustrate the data processing step-by-step, both as a teaching tool, and to show that I am not using vague methodology to generate a narratively-convenient conclusion. <em>For more detailed code and output, a <a href="https://github.com/minimaxir/sf-arrests-when-where/blob/master/crime_data_sf.ipynb">Jupyter notebook</a> containing the code and visualizations used in this article is available open-source on GitHub.</em></p>

<p>Loading a 1.9 million row file into R can take awhile, even on modern computers with a SSD. Enter <code>readr</code>, <a href="https://github.com/hadley/readr">another R package</a> by <code>ggplot2</code> author Hadley Wickham, which grants access to a <code>read_csv()</code> function that has nearly 10x the speed of the base <code>read.csv()</code> R function, with more sensible defaults too.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">path <span class="o">&lt;-</span> <span class="s">&quot;~/Downloads/SFPD_Incidents_-_from_1_January_2003.csv&quot;</span>

df <span class="o">&lt;-</span> read_csv<span class="p">(</span>path<span class="p">)</span></code></pre></figure>

<p>In memory, the data set is 180.9 MB, and removing a few useless columns (e.g. IncidentNum) further reduces the size to 126.9 MB. Since there are many redundancies in the row data (e.g. only 10 distinct PdDistrict values), R can perform memory optimizations.</p>

<p>You may have noticed in the first article image that the text data in some of the columns is in ALL CAPS, which would look ugly if the text was used in a data visualization. We can create a helper function to convert a column of text values into proper case through the use of <a href="http://stackoverflow.com/questions/15776732/how-to-convert-a-vector-of-strings-to-title-case">regular expression shenanigans</a>.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">proper_case <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">)</span> <span class="p">{</span>
    <span class="kr">return</span> <span class="p">(</span><span class="kp">gsub</span><span class="p">(</span><span class="s">&quot;\\b([A-Z])([A-Z]+)&quot;</span><span class="p">,</span> <span class="s">&quot;\\U\\1\\L\\2&quot;</span> <span class="p">,</span> x<span class="p">,</span> perl<span class="o">=</span><span class="kc">TRUE</span><span class="p">))</span>
<span class="p">}</span></code></pre></figure>

<p>Now we can do more through processing using <code>dplyr</code>, <a href="https://github.com/hadley/dplyr"><em>another</em> Hadley Wickham R package</a>. dplyr is a utility that makes R easier to use: it provides a new syntax that allows data manipulation with intuitive function names, the functions can be chained using the <code>%&gt;%</code> operator for efficiency, and all data processing is <em>significantly</em> faster due to a C++ code base. (Fun fact: before the release of dplyr, I intended to quit using R for data analysis in favor of Python. Base R syntax is <em>that</em> difficult to use.)</p>

<p>In dplyr, <code>mutate</code> allows the creation and transformation of columns. We will transform the text columns by running the columns through the <code>proper_case</code> function earlier:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df <span class="o">&lt;-</span> df <span class="o">%&gt;%</span> mutate<span class="p">(</span>Category <span class="o">=</span> proper_case<span class="p">(</span>Category<span class="p">),</span>
                 Descript <span class="o">=</span> proper_case<span class="p">(</span>Descript<span class="p">),</span>
                 PdDistrict <span class="o">=</span> proper_case<span class="p">(</span>PdDistrict<span class="p">),</span>
                 Resolution <span class="o">=</span> proper_case<span class="p">(</span>Resolution<span class="p">))</span></code></pre></figure>

<p>After all that, the data looks like:</p>

<p><img src="/img/sf-arrests/processed-table.png" alt=""></p>

<p>Much better!</p>

<p>However, many of the records have a &ldquo;None&rdquo; value for Resolution. This implies that the police appeared at the incident but did no action, which isn&rsquo;t that helpful for analysis. How about we look at incidents which resulted in an arrest?</p>

<p>dplyr&rsquo;s  <code>filter</code> command does that, and we can use <code>grepl()</code> to do a text search for each Resolution value for the presence of &ldquo;Arrest&rdquo;.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_arrest <span class="o">&lt;-</span> df <span class="o">%&gt;%</span> filter<span class="p">(</span><span class="kp">grepl</span><span class="p">(</span><span class="s">&quot;Arrest&quot;</span><span class="p">,</span> Resolution<span class="p">))</span></code></pre></figure>

<p>That&rsquo;s it! There are 587,499 arrests total in the dataset.</p>

<h2>Arrests Over Time</h2>

<p>One of the most simple data visualizations is a line chart, and it&rsquo;s a good starting point to use for analyzing arrests. Has the number of daily arrests been changing over time? dplyr and ggplot2 make this very easy to visualize in R.</p>

<p>First, the Date column must be formatted as a Date internally in R instead of text. Then we <code>group_by</code> the Date, and then use <code>summarize</code> to perform an aggregate on each group; in this case, count how many entries for the group. (<code>n()</code> is a convenient shortcut). We can also ensure that the dates are in ascending order.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_arrest_daily <span class="o">&lt;-</span> df_arrest <span class="o">%&gt;%</span>
                    mutate<span class="p">(</span>Date <span class="o">=</span> <span class="kp">as.Date</span><span class="p">(</span>Date<span class="p">,</span> <span class="s">&quot;%m/%d/%Y&quot;</span><span class="p">))</span> <span class="o">%&gt;%</span>
                    group_by<span class="p">(</span>Date<span class="p">)</span> <span class="o">%&gt;%</span>
                    summarize<span class="p">(</span>count <span class="o">=</span> n<span class="p">())</span> <span class="o">%&gt;%</span>
                    arrange<span class="p">(</span>Date<span class="p">)</span></code></pre></figure>

<p><img src="/img/sf-arrests/date-table.png" alt=""></p>

<p>Nifty! However, keep in mind that there are thousands of days in this dataset.</p>

<p>Now we can make a pretty line chart in ggplot2. Here&rsquo;s the code, and I will explain what everything does afterward:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df_arrest_daily<span class="p">,</span> aes<span class="p">(</span>x <span class="o">=</span> Date<span class="p">,</span> y <span class="o">=</span> count<span class="p">))</span> <span class="o">+</span>
    geom_line<span class="p">(</span>color <span class="o">=</span> <span class="s">&quot;#F2CA27&quot;</span><span class="p">,</span> size <span class="o">=</span> <span class="m">0.1</span><span class="p">)</span> <span class="o">+</span>
    geom_smooth<span class="p">(</span>color <span class="o">=</span> <span class="s">&quot;#1A1A1A&quot;</span><span class="p">)</span> <span class="o">+</span>
    fte_theme<span class="p">()</span> <span class="o">+</span>
    scale_x_date<span class="p">(</span>breaks <span class="o">=</span> date_breaks<span class="p">(</span><span class="s">&quot;2 years&quot;</span><span class="p">),</span> labels <span class="o">=</span> date_format<span class="p">(</span><span class="s">&quot;%Y&quot;</span><span class="p">))</span> <span class="o">+</span>
    labs<span class="p">(</span>x <span class="o">=</span> <span class="s">&quot;Date of Arrest&quot;</span><span class="p">,</span> y <span class="o">=</span> <span class="s">&quot;# of Police Arrests&quot;</span><span class="p">,</span> title <span class="o">=</span> <span class="s">&quot;Daily Police Arrests in San Francisco from 2003  2015&quot;</span><span class="p">)</span></code></pre></figure>

<ul>
<li><code>ggplot()</code> sets up the base chart and axes.</li>
<li><code>geom_line()</code> creates the line for the line chart. &ldquo;color&rdquo; and &ldquo;size&rdquo; parameters do just that.</li>
<li><code>geom_smooth()</code> adds a smoothing spline on top of the chart to serve as a trendline, which is helpful since there are a lot of points.</li>
<li><code>fte_theme()</code> is my theme based on the FiveThirtyEight style.</li>
<li><code>scale_x_date()</code> explicitly sets the x-axis to scale with date values. However, there are a few extremely useful formatting parameters with this function: &ldquo;breaks&rdquo; lets you set the chart breaks in plain English, and &ldquo;labels&rdquo; lets you format the dates at this breaks; in this case, there are breaks every 2 years, and only the year will be displayed for minimalism.</li>
<li><code>labs()</code> is a quick shortcut for labeling your axes and plot (<em>always</em> label!)</li>
</ul>

<p>Running the code and saving the output results in this image:</p>

<p><img src="/img/sf-arrests/sf-arrest-when-1.png" alt=""></p>

<p>The line chart has high variation due to the number of points (in retrospect, a 30-day moving average of arrests would work better visually). As the trendline indicates, the trend is actually <em>multimodal</em>, with daily arrest peaks in 2009 and 2014. Definitely interesting. The number of arrests appears to be on a downward trend since then.</p>

<p>The next step is to look into possible answers for the day-by-day variation.</p>

<h2>When Do Arrests Happen?</h2>

<p>One of my go-to data visualizations is a heat map of times of week; in this case, we can find which day-of-week and time-of-day when the most Arrests occur in San Francisco, and compare that with other time slots at a glance.</p>

<p>This requires the Hour and Day-of-Week to be present in separate columns: we have a DOY column already, but we need to parse the Hour component out of the HH:MM values in the Time column.</p>

<p>This requires another helper function which uses <code>strsplit()</code> to split a single time value to Hour and Minute components, take the first value (Hour), and convert that value to a numeric value (instead of text) For example, &ldquo;09:40&rdquo; input returns 9.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">get_hour <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">)</span> <span class="p">{</span>
    <span class="kr">return</span> <span class="p">(</span><span class="kp">as.numeric</span><span class="p">(</span><span class="kp">strsplit</span><span class="p">(</span>x<span class="p">,</span><span class="s">&quot;:&quot;</span><span class="p">)[[</span><span class="m">1</span><span class="p">]][</span><span class="m">1</span><span class="p">]))</span>
<span class="p">}</span></code></pre></figure>

<p>Unfortunately, this will not work for an entire column. Using <code>sapply()</code> applies a specified function to each element in a column, which accomplishes the same goal.</p>

<p>The goal is to count how many Arrests occur for a given day-of-week and hour combination. In dplyr, we <code>group_by</code> both &ldquo;DayOfWeek&rdquo; and &ldquo;Hour&rdquo;, and then use <code>summarize</code> again.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_arrest_time <span class="o">&lt;-</span> df_arrest <span class="o">%&gt;%</span>
                    mutate<span class="p">(</span>Hour <span class="o">=</span> <span class="kp">sapply</span><span class="p">(</span>Time<span class="p">,</span> get_hour<span class="p">))</span> <span class="o">%&gt;%</span>
                    group_by<span class="p">(</span>DayOfWeek<span class="p">,</span> Hour<span class="p">)</span> <span class="o">%&gt;%</span>
                    summarize<span class="p">(</span>count <span class="o">=</span> n<span class="p">())</span></code></pre></figure>

<p><img src="/img/sf-arrests/dow-table.png" alt=""></p>

<p>A few more tweaks are done (off camera) to convert the Hours to representations like &ldquo;12 PM&rdquo; and get everything in the correct order.</p>

<p>Now, it&rsquo;s time to make the heatmap using <code>ggplot2</code>. Here&rsquo;s the code, and I will explain what the new functions do:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df_arrest_time<span class="p">,</span> aes<span class="p">(</span>x <span class="o">=</span> Hour<span class="p">,</span> y <span class="o">=</span> DayOfWeek<span class="p">,</span> fill <span class="o">=</span> count<span class="p">))</span> <span class="o">+</span>
    geom_tile<span class="p">()</span> <span class="o">+</span>
    fte_theme<span class="p">()</span> <span class="o">+</span>
    theme<span class="p">(</span><span class="kc">...</span><span class="p">)</span> <span class="o">+</span>
    labs<span class="p">(</span>x <span class="o">=</span> <span class="s">&quot;Hour of Arrest (Local Time)&quot;</span><span class="p">,</span> y <span class="o">=</span> <span class="s">&quot;Day of Week of Arrest&quot;</span><span class="p">,</span> title <span class="o">=</span> <span class="s">&quot;# of Police Arrests in San Francisco from 2003  2015, by Time of Arrest&quot;</span><span class="p">)</span> <span class="o">+</span>
    scale_fill_gradient<span class="p">(</span>low <span class="o">=</span> <span class="s">&quot;white&quot;</span><span class="p">,</span> high <span class="o">=</span> <span class="s">&quot;#27AE60&quot;</span><span class="p">,</span> labels <span class="o">=</span> comma<span class="p">)</span></code></pre></figure>

<ul>
<li><code>geom_title()</code> creates tiles. (instead of lines)</li>
<li><code>theme()</code> is needed for a few additional theme tweaks to get the gradient bar to render (tweaks not shown)</li>
<li><code>scale_fill_gradient()</code> tells the tiles to fill on a gradient, from white as the lowest value to a green as the highest value. The &ldquo;labels = comma&rdquo; parameter is a hidden helpful tip to allow any values in the legend to show with commas.</li>
</ul>

<p>Putting it all together:</p>

<p><img src="/img/sf-arrests/sf-arrest-when-2.png" alt=""></p>

<p>The heatmap is an intuitive result. Arrests don&rsquo;t happen in the early morning, and arrests tend to be elevated Friday and Saturday night, when everyone is out on the town.</p>

<p>However, the peak arrest time is apparently on Wednesdays at 4-5 PM. Wednesdays and the 4-5 PM timeslot in general have elevated arrest frequency, too. Why is that the case?</p>

<p>This requires further analysis.</p>

<h2>Facets of Arrest</h2>

<p>Perhaps the odd results can be explained by another lurking variable. Logically, certain types of crime, such as DUIs, should happen primarily at night. ggplot2 has tool known as faceting that makes such analysis easy by rendering a chart for each instance of another value in another variable. In this case, with only <em>one</em> line of ggplot2 code, we can plot a heatmap for <em>each</em> of the top types of arrests, and see if there is any significant variation in the heatmap.</p>

<p>After quickly using dplyr to aggregate and sort the top categories of arrest, by number of occurrences:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_top_crimes <span class="o">&lt;-</span> df_arrest <span class="o">%&gt;%</span>
                    group_by<span class="p">(</span>Category<span class="p">)</span> <span class="o">%&gt;%</span>
                    summarize<span class="p">(</span>count <span class="o">=</span> n<span class="p">())</span> <span class="o">%&gt;%</span>
                    arrange<span class="p">(</span>desc<span class="p">(</span>count<span class="p">))</span></code></pre></figure>

<p><img src="/img/sf-arrests/top-crimes.png" alt=""></p>

<p>&ldquo;Other Offenses&rdquo; is a catch-all, so we will ignore that. Filter on the top 18 types of crime excluding Other Offenses and aggregate as usual.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_arrest_time_crime <span class="o">&lt;-</span> df_arrest <span class="o">%&gt;%</span>
                    filter<span class="p">(</span>Category <span class="o">%in%</span> df_top_crimes<span class="o">$</span>Category<span class="p">[</span><span class="m">2</span><span class="o">:</span><span class="m">19</span><span class="p">])</span> <span class="o">%&gt;%</span>
                    mutate<span class="p">(</span>Hour <span class="o">=</span> <span class="kp">sapply</span><span class="p">(</span>Time<span class="p">,</span> get_hour<span class="p">))</span> <span class="o">%&gt;%</span>
                    group_by<span class="p">(</span>Category<span class="p">,</span> DayOfWeek<span class="p">,</span> Hour<span class="p">)</span> <span class="o">%&gt;%</span>
                    summarize<span class="p">(</span>count <span class="o">=</span> n<span class="p">())</span></code></pre></figure>

<p>Time for the heat map! The <code>ggplot</code> code is nearly identical to the previous heatmap code, except we add <code>facet_wrap()</code>.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df_arrest_time_crime<span class="p">,</span> aes<span class="p">(</span>x <span class="o">=</span> Hour<span class="p">,</span> y <span class="o">=</span> DayOfWeek<span class="p">,</span> fill <span class="o">=</span> count<span class="p">))</span> <span class="o">+</span>
    geom_tile<span class="p">()</span> <span class="o">+</span>
    fte_theme<span class="p">()</span> <span class="o">+</span>
    theme<span class="p">(</span><span class="kc">...</span><span class="p">)</span> <span class="o">+</span>
    labs<span class="p">(</span>x <span class="o">=</span> <span class="s">&quot;Hour of Arrest (Local Time)&quot;</span><span class="p">,</span> y <span class="o">=</span> <span class="s">&quot;Day of Week of Arrest&quot;</span><span class="p">,</span> title <span class="o">=</span> <span class="s">&quot;# of Police Arrests in San Francisco from 2003  2015, by Category and Time of Arrest&quot;</span><span class="p">)</span> <span class="o">+</span>
    scale_fill_gradient<span class="p">(</span>low <span class="o">=</span> <span class="s">&quot;white&quot;</span><span class="p">,</span> high <span class="o">=</span> <span class="s">&quot;#2980B9&quot;</span><span class="p">)</span> <span class="o">+</span>
    facet_wrap<span class="p">(</span><span class="o">~</span> Category<span class="p">,</span> nrow <span class="o">=</span> <span class="m">6</span><span class="p">)</span></code></pre></figure>

<p><img src="/img/sf-arrests/sf-arrest-when-3.png" alt=""></p>

<p>Easy visualization to make, but it&rsquo;s not fully correct. There can only be one scale for the whole visualization, which is why the categories with lots of arrests appear colored and others do not (however, it shows that Drugs/Narcotics arrests are a large contributor to the Wednesday emphasis of the data). We need to normalize the counts by facet. dplyr has a nice trick for normalization: group by the normalization variable (Category), then mutate to add a column based on the aggregate for each unique value.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">df_arrest_time_crime <span class="o">&lt;-</span> df_arrest_time_crime <span class="o">%&gt;%</span>
                            group_by<span class="p">(</span>Category<span class="p">)</span> <span class="o">%&gt;%</span>
                            mutate<span class="p">(</span>norm <span class="o">=</span> count<span class="o">/</span><span class="kp">sum</span><span class="p">(</span>count<span class="p">))</span></code></pre></figure>

<p><img src="/img/sf-arrests/crime-norm.png" alt=""></p>

<p>Setting the &ldquo;fill&rdquo; to &ldquo;norm&rdquo; and rerunning the heatmap code yields:</p>

<p><img src="/img/sf-arrests/sf-arrest-when-4.png" alt=""></p>

<p>Now things get interesting.</p>

<p>Prostitution has the most notably unique behavior, which high concentrations of arrests at night on weekdays. Drunkenness and DUIs have high concentrations at night on weekends. And Disorderly Conduct has a high concentration of arrests at 5 AM on weekdays? That&rsquo;s not intuitive.</p>

<p>Notably, some offenses have relatively random times of arrests, such as Stolen Property and Vehicle Theft.</p>

<p>However, this doesn&rsquo;t help explain why arrests tend to happen Wednesdays/4-5PM. Maybe faceting by another variable will provide more information.</p>

<p>Perhaps Police district? Maybe some PDs in San Francisco are more zealous than others. Since we created a code workflow earlier, we can apply it to any other variable very easily; in this case, it&rsquo;s mostly just replacing instances of &ldquo;Category&rdquo; with &ldquo;PdDistrict.&rdquo;</p>

<p>Doing thus yields this heatmap.</p>

<p><img src="/img/sf-arrests/sf-arrest-when-5.png" alt=""></p>

<p>Which isn&rsquo;t helpful. The charts are mostly identical to each other, and to the original heatmap. (Central Station (<a href="http://www.sf-police.org/Modules/ShowDocument.aspx?documentID=27554">coverage map</a>), however, has activity correlated to Drunkenness arrests.)</p>

<p>Perhaps the frequency of arrests is correlated to the time of year? How about faceting by month?</p>

<p><img src="/img/sf-arrests/sf-arrest-when-6.png" alt=""></p>

<p>Nope. Zero difference.</p>

<p>Last try. As shown in the line chart, the # of Arrests has oscillated over the years. Perhaps there&rsquo;s a specific year that&rsquo;s skewing the results. Let&rsquo;s facet by Year.</p>

<p><img src="/img/sf-arrests/sf-arrest-when-7.png" alt=""></p>

<p>Nope<sup>2.</sup> 2010-2012 have elevated Wednesday activity, but not by much.</p>

<p>This is frustrating. As of this posting, I don&rsquo;t have an obvious answer for the elevated arrests Wednesdays at 4-5PM. That being said, there definitely is still more to learn from looking at SF Crime data, although that&rsquo;s enough analysis for the time being.</p>

<p><a href="http://minimaxir.com/2015/12/sf-arrest-maps/">My next article</a> discusses how to plot arrests on a map using the <code>ggmap</code> R library, which hopefully will provide more answers. The <a href="https://github.com/minimaxir/sf-arrests-when-where">GitHub repository</a> contains a Jupyter notebook with code and visualizations for both for this article, and for the upcoming ggmap visualizations (if you want a sneak peek) which will show <em>where</em> arrests in San Francisco frequently occur.</p>

<hr>

<p><em>If you use the code or data visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks!</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Visualize New York City Using Taxi Location Data and ggplot2]]></title>
    <link href="http://minimaxir.com/2015/11/nyc-ggplot2-howto/"/>
    <updated>2015-11-16T08:00:00-08:00</updated>
    <id>http://minimaxir.com/2015/11/nyc-ggplot2-howto</id>
    <content type="html"><![CDATA[<p>A few months ago, I had <a href="http://minimaxir.com/2015/08/nyc-map/">posted a visualization</a> of NYC Yellow Taxis using <a href="http://ggplot2.org">ggplot2</a>, an extremely-popular R package by Hadley Wickham for data visualization. At the time, the code used for the chart was very messy since I was eager to create something cool after seeing the <a href="https://news.ycombinator.com/item?id=10003118">referenced Hacker News thread</a>. Due to popular demand, I&rsquo;ve cleaned up the code and have <a href="https://github.com/minimaxir/nyc-taxi-notebook">released it open source</a>, with a few improvements.</p>

<p>Here are some tips and tutorials on how to make such visualizations.</p>

<h2>Getting the Data</h2>

<p><em>As usual, a <a href="https://github.com/minimaxir/nyc-taxi-notebook/blob/master/nyc_taxi_map.ipynb">Jupyter notebook</a> containing the code and visualizations used in this article is available open-source on GitHub.</em></p>

<p>A quick summary of the previous post: I obtained the data from <a href="https://cloud.google.com/bigquery/">BigQuery</a>, which <a href="http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml">was uploaded</a> from the official NYC Taxi &amp; Limousine Commission datasets, plotted each taxi point as a tiny white dot on a fully-black map, and colorized the dots depending on the number of taxis at that location.</p>

<p>In September, the <a href="https://bigquery.cloud.google.com/table/nyc-tlc:yellow.trips">BigQuery dataset</a> was updated to include all data from January 2009 to June 2015: over 1.1 <em>billion</em> Yellow Taxi rides recorded. Here&rsquo;s an updated query, which additionally calculates the total non-tip revenue for a given location, since that might be useful later, and implements a <a href="https://www.reddit.com/r/bigquery/comments/3fo9ao/nyc_taxi_trips_now_officially_shared_by_the_nyc/ctqfr8h">sanity check filter</a> noted by Felipe Hoffa.</p>

<figure class="highlight"><pre><code class="language-sql" data-lang="sql"><span class="k">SELECT</span> <span class="n">ROUND</span><span class="p">(</span><span class="n">pickup_latitude</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="k">AS</span> <span class="n">lat</span><span class="p">,</span>
<span class="n">ROUND</span><span class="p">(</span><span class="n">pickup_longitude</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="k">AS</span> <span class="n">long</span><span class="p">,</span>
<span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">AS</span> <span class="n">num_pickups</span><span class="p">,</span>
<span class="k">SUM</span><span class="p">(</span><span class="n">fare_amount</span><span class="p">)</span> <span class="k">AS</span> <span class="n">total_revenue</span>
<span class="k">FROM</span> <span class="p">[</span><span class="n">nyc</span><span class="o">-</span><span class="n">tlc</span><span class="p">:</span><span class="n">yellow</span><span class="p">.</span><span class="n">trips</span><span class="p">]</span>
<span class="k">WHERE</span> <span class="n">fare_amount</span><span class="o">/</span><span class="n">trip_distance</span> <span class="k">BETWEEN</span> <span class="mi">2</span> <span class="k">AND</span> <span class="mi">10</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">lat</span><span class="p">,</span> <span class="n">long</span></code></pre></figure>

<p>The resulting dataset is 4 million rows and 116MB in size! This is well over the limit for downloading from the web BigQuery client, so you must use a local client (in the attached notebook, R), and it will still take about 10-15 minutes to download (as a result, I recommend caching the results locally). Relatedly, rendering 4 million points on a single plot on screen may be computationally intensive: I strongly recommend rendering the visualization to disk by instantiating a <code>png</code> device or by using <code>ggsave</code>.</p>

<p>Here&rsquo;s a few results from that query.</p>

<p><img src="/img/nyc-ggplot2-howto/test-data.png" alt=""></p>

<p>As you can see, the second latitude/longitude combo is blatantly <em>wrong</em>. This isn&rsquo;t the first fidelity issue with the dataset, but we will address those in due time.</p>

<h2>Plotting the Taxis</h2>

<p>Let&rsquo;s do a basic ggplot2 plot to test things out. All we need to do is plot a small point for every lat/long combination, and then save the resulting plot.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df<span class="p">,</span> aes<span class="p">(</span>x<span class="o">=</span>long<span class="p">,</span> y<span class="o">=</span>lat<span class="p">))</span> <span class="o">+</span>
            geom_point<span class="p">(</span>size<span class="o">=</span><span class="m">0.06</span><span class="p">)</span> <span class="o">+</span>

png<span class="p">(</span><span class="s">&quot;nyc-taxi-1.png&quot;</span><span class="p">,</span> w<span class="o">=</span><span class="m">600</span><span class="p">,</span> h<span class="o">=</span><span class="m">600</span><span class="p">)</span>
plot
dev.off<span class="p">()</span></code></pre></figure>

<p><img src="/img/nyc-ggplot2-howto/nyc-taxi-1.png" alt=""></p>

<p>&hellip;stupid data fidelity issues.</p>

<p>This issue is fixed by constraining the plot to a <a href="https://en.wikipedia.org/wiki/Minimum_bounding_box">bounding box</a> of latitude and longitude coordinates corresponding to NYC. Flickr has <a href="https://www.flickr.com/places/info/2459115">a good starting point</a> for a NYC bounding box; I took that and edited the limits more precisely using the <a href="http://boundingbox.klokantech.com">Bounding Box Tool</a>.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">min_lat <span class="o">&lt;-</span> <span class="m">40.5774</span>
max_lat <span class="o">&lt;-</span> <span class="m">40.9176</span>
min_long <span class="o">&lt;-</span> <span class="m">-74.15</span>
max_long <span class="o">&lt;-</span> <span class="m">-73.7004</span></code></pre></figure>

<p>You could also enforce the bounding box during the BigQuery. Now let&rsquo;s implement the bounding box in the plot:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df<span class="p">,</span> aes<span class="p">(</span>x<span class="o">=</span>long<span class="p">,</span> y<span class="o">=</span>lat<span class="p">))</span> <span class="o">+</span>
            geom_point<span class="p">(</span>size<span class="o">=</span><span class="m">0.06</span><span class="p">)</span> <span class="o">+</span>
            scale_x_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span>min_long<span class="p">,</span> max_long<span class="p">))</span> <span class="o">+</span>
            scale_y_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span>min_lat<span class="p">,</span> max_lat<span class="p">))</span>

png<span class="p">(</span><span class="s">&quot;nyc-taxi-2.png&quot;</span><span class="p">,</span> w<span class="o">=</span><span class="m">600</span><span class="p">,</span> h<span class="o">=</span><span class="m">600</span><span class="p">)</span>
plot
dev.off<span class="p">()</span></code></pre></figure>

<p><img src="/img/nyc-ggplot2-howto/nyc-taxi-2.png" alt=""></p>

<p>Much, much better! Now that the visualization generally looks like what we want it to be, we can start theming.</p>

<p>Let&rsquo;s start small and do just a few tweaks:</p>

<ul>
<li>Filter the data slightly to reduce some erroneous points.</li>
<li>The theme must be primarily a black background, with most of the ggplot2 theme attributes stripped out and the margins nullified. (implemented as <code>theme_map_dark()</code>; code is in the notebook)</li>
<li>Set the resolution of the rendering device to 300 DPI; this reduces some of the aliasing in the resulting image.</li>
</ul>

<figure class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df <span class="o">%&gt;%</span> filter<span class="p">(</span>num_pickups <span class="o">&gt;</span> <span class="m">10</span><span class="p">),</span> aes<span class="p">(</span>x<span class="o">=</span>long<span class="p">,</span> y<span class="o">=</span>lat<span class="p">))</span> <span class="o">+</span>
            geom_point<span class="p">(</span>color<span class="o">=</span><span class="s">&quot;white&quot;</span><span class="p">,</span> size<span class="o">=</span><span class="m">0.06</span><span class="p">)</span> <span class="o">+</span>
            scale_x_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span>min_long<span class="p">,</span> max_long<span class="p">))</span> <span class="o">+</span>
            scale_y_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span>min_lat<span class="p">,</span> max_lat<span class="p">))</span> <span class="o">+</span>
            theme_map_dark<span class="p">()</span>

png<span class="p">(</span><span class="s">&quot;nyc-taxi-3.png&quot;</span><span class="p">,</span> w<span class="o">=</span><span class="m">600</span><span class="p">,</span> h<span class="o">=</span><span class="m">600</span><span class="p">,</span> res<span class="o">=</span><span class="m">300</span><span class="p">)</span>
plot
dev.off<span class="p">()</span></code></pre></figure>

<p><img src="/img/nyc-ggplot2-howto/nyc-taxi-3.png" alt=""></p>

<p>Right on track! Now time to make things more professional.</p>

<p>This requires the implementation of a few more aesthetics:</p>

<ul>
<li>Add a gradient color based on intensity of the number of pickups: since the number of pickups will logically be near streets, the coloring will be more intense near streets. Exact color doesn&rsquo;t matter; I used the purple Wisteria from <a href="https://flatuicolors.com">Flat UI Colors</a> to represent maximum intensity. Additionally, the scale should be logarithmic to make the colors stand out. (Another approach is to scale the transparency of the points instead, which is the approach <a href="http://www.brianrlance.com/blog/2015/8/7/nyc-visualized-via-taxi-pickup-locations">Brian Lance had done</a> and that works well too)</li>
<li>Annotate the theme with a proper title (and remove the scale legend; since the exact values on specific points will not be helpful)</li>
<li>Force the plot to obey the dimension ratio with <code>coord_equal()</code>, otherwise the map will stretch and distort to fill the entirety of the plotting area. (you can see a vertical stretch effect with the previous image)</li>
</ul>

<p>Putting it all together:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df <span class="o">%&gt;%</span> filter<span class="p">(</span>num_pickups <span class="o">&gt;</span> <span class="m">10</span><span class="p">),</span> aes<span class="p">(</span>x<span class="o">=</span>long<span class="p">,</span> y<span class="o">=</span>lat<span class="p">,</span> color<span class="o">=</span>num_pickups<span class="p">))</span> <span class="o">+</span>
            geom_point<span class="p">(</span>size<span class="o">=</span><span class="m">0.06</span><span class="p">)</span> <span class="o">+</span>
            scale_x_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span>min_long<span class="p">,</span> max_long<span class="p">))</span> <span class="o">+</span>
            scale_y_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span>min_lat<span class="p">,</span> max_lat<span class="p">))</span> <span class="o">+</span>
            theme_map_dark<span class="p">()</span> <span class="o">+</span>
            scale_color_gradient<span class="p">(</span>low<span class="o">=</span><span class="s">&quot;#CCCCCC&quot;</span><span class="p">,</span> high<span class="o">=</span><span class="s">&quot;#8E44AD&quot;</span><span class="p">,</span> trans<span class="o">=</span><span class="s">&quot;log&quot;</span><span class="p">)</span> <span class="o">+</span>
            labs<span class="p">(</span>title <span class="o">=</span> <span class="s">&quot;Map of NYC, Plotted Using Locations Of All Yellow Taxi Pickups&quot;</span><span class="p">)</span> <span class="o">+</span>
            theme<span class="p">(</span>legend.position<span class="o">=</span><span class="s">&quot;none&quot;</span><span class="p">)</span> <span class="o">+</span>
            coord_equal<span class="p">()</span>

png<span class="p">(</span><span class="s">&quot;nyc-taxi-4.png&quot;</span><span class="p">,</span> w<span class="o">=</span><span class="m">600</span><span class="p">,</span> h<span class="o">=</span><span class="m">600</span><span class="p">,</span> res<span class="o">=</span><span class="m">300</span><span class="p">)</span>
plot
dev.off<span class="p">()</span></code></pre></figure>

<p>results in this image, which is what we want! However there is a slight problem, and I will wrap the image in a red border to demonstrate.</p>

<p><span><style>
.border img {
  border: 3px solid red;
}
</style></span></p>

<p><span class="border"><img src="/img/nyc-ggplot2-howto/nyc-taxi-4.png" alt=""></span></p>

<p>Due to <code>coord_equal()</code> enforcing the chart dimensions, the rendering device has a gap of white space at the top due to interaction with the <code>grid</code> graphics package that ggplot2 is based upon; normally not a problem for default charts, but a waste of space for visualizations with non-white backgrounds.</p>

<p>I attempted to fix this issue by forcing <code>grid</code> to render a black rectangle then plot on top of it. Unfornately, that was not successful. The quickest workaround is to set the image dimensions through trial-and-error such that the issue is minimized.</p>

<p>All things considered, that&rsquo;s minor but should still be noted. The streets of Manhattan are visible! And there&rsquo;s still more that can be done.</p>

<h2>Hexing the Revenue</h2>

<p>Hex map overlays are a popular technique for aggregating two-dimensional data on a 3rd dimension. ggplot2 has a relatively new <a href="http://docs.ggplot2.org/current/stat_summary_hex.html">stat_summary_hex</a> function which does just that.</p>

<p>Why not aggregate total revenue for NYC Yellow Taxi Pickups to determine where taxis generate the most money? Since we conveniently have the code to generate a map of NYC already, we can plot the hex bins on top of that map, after a few more tweaks:</p>

<ul>
<li>Set the 3rd dimension, <code>z</code>, to <code>total_revenue</code></li>
<li>Set the aggregation to the <code>sum</code> function, so it sums up all the revenues within a bin.</li>
<li>Scale the total hex revenues with a gradient.</li>
<li>Tweak all the aesthetics: color of the base points, the color of the hexes, the transparency of the hexes, and the name of the chart.</li>
<li>Set the chart dimensions to avoid the <code>coord_equal()</code> issue mention above.</li>
</ul>

<figure class="highlight"><pre><code class="language-r" data-lang="r">plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df <span class="o">%&gt;%</span> filter<span class="p">(</span>num_pickups <span class="o">&gt;</span> <span class="m">20</span><span class="p">),</span> aes<span class="p">(</span>x<span class="o">=</span>long<span class="p">,</span> y<span class="o">=</span>lat<span class="p">,</span> z<span class="o">=</span>total_revenue<span class="p">))</span> <span class="o">+</span>
            geom_point<span class="p">(</span>size<span class="o">=</span><span class="m">0.06</span><span class="p">,</span> color<span class="o">=</span><span class="s">&quot;#999999&quot;</span><span class="p">)</span> <span class="o">+</span>
            stat_summary_hex<span class="p">(</span>fun <span class="o">=</span> <span class="kp">sum</span><span class="p">,</span> bins<span class="o">=</span><span class="m">100</span><span class="p">,</span> alpha<span class="o">=</span><span class="m">0.7</span><span class="p">)</span> <span class="o">+</span>
            scale_x_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span>min_long<span class="p">,</span> max_long<span class="p">))</span> <span class="o">+</span>
            scale_y_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span>min_lat<span class="p">,</span> max_lat<span class="p">))</span> <span class="o">+</span>
            theme_map_dark<span class="p">()</span> <span class="o">+</span>
            scale_fill_gradient<span class="p">(</span>low<span class="o">=</span><span class="s">&quot;#CCCCCC&quot;</span><span class="p">,</span> high<span class="o">=</span><span class="s">&quot;#27AE60&quot;</span><span class="p">,</span> labels<span class="o">=</span>dollar<span class="p">)</span> <span class="o">+</span>
            labs<span class="p">(</span>title <span class="o">=</span> <span class="s">&quot;Total Revenue for NYC Yellow Taxis by Pickup Location, from Jan 2009  June 2015&quot;</span><span class="p">)</span> <span class="o">+</span>
            coord_equal<span class="p">()</span>

png<span class="p">(</span><span class="s">&quot;nyc-taxi-5.png&quot;</span><span class="p">,</span> w<span class="o">=</span><span class="m">950</span><span class="p">,</span> h<span class="o">=</span><span class="m">860</span><span class="p">,</span> res<span class="o">=</span><span class="m">300</span><span class="p">)</span>
plot
dev.off<span class="p">()</span></code></pre></figure>

<p><img src="/img/nyc-ggplot2-howto/nyc-taxi-5.png" alt=""></p>

<p>That wasn&rsquo;t too bad. The gradient shows that <a href="https://www.google.com/maps/place/penn+station+nyc/@40.750568,-73.993519,15z">Penn Station</a> in Manhattan, along with the two airports, are the largest revenue generators.</p>

<p>I <a href="https://www.reddit.com/r/dataisbeautiful/comments/3sjmc0/total_revenue_for_nyc_yellow_taxis_by_pickup/">posted the hex-overlayed map</a> on Reddit to /r/dataisbeautiful as a part of my data visualization beta-testing. Although the chart received just under 200 upvotes, the comments in the Reddit thread were <em>unanimously negative</em>. Reddit user /u/DanHeidel <a href="https://www.reddit.com/r/dataisbeautiful/comments/3sjmc0/total_revenue_for_nyc_yellow_taxis_by_pickup/cwxuy1e">posted a long rant</a> on the problems with the aesthetics of the chart. And for the most part, I agree with his assessment.</p>

<p>Let&rsquo;s try again, and address the claims made in the Reddit comments.</p>

<ul>
<li>Only show hex bins where there is enough valid data, which should remove the mysterious hexes over the water. This can be implemented through a helper aggregate function which does not render the hex if the total revenue of the hex is under some threshold value. (I set it to $100,000)</li>
<li>Scale the total hex revenue logarithmically, and change the color to a Red hue (Alizarin) to make the step values more visible.</li>
<li>Zoom the chart dimensions closer to Manhattan.</li>
<li>Make a few more aesthetic tweaks.</li>
</ul>

<p>Here&rsquo;s take two:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r">total_rev <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">,</span> threshold <span class="o">=</span> <span class="m">10</span><span class="o">^</span><span class="m">5</span><span class="p">)</span> <span class="p">{</span>
    <span class="kr">if</span> <span class="p">(</span><span class="kp">sum</span><span class="p">(</span>x<span class="p">)</span> <span class="o">&lt;</span> threshold<span class="p">)</span> <span class="p">{</span><span class="kr">return</span> <span class="p">(</span><span class="kc">NA</span><span class="p">)}</span>
    <span class="kr">else</span> <span class="p">{</span><span class="kr">return</span> <span class="p">(</span><span class="kp">sum</span><span class="p">(</span>x<span class="p">))}</span>
<span class="p">}</span>

plot <span class="o">&lt;-</span> ggplot<span class="p">(</span>df <span class="o">%&gt;%</span> filter<span class="p">(</span>num_pickups <span class="o">&gt;</span> <span class="m">10</span><span class="p">),</span> aes<span class="p">(</span>x<span class="o">=</span>long<span class="p">,</span> y<span class="o">=</span>lat<span class="p">,</span> z<span class="o">=</span>total_revenue<span class="p">))</span> <span class="o">+</span>
            geom_point<span class="p">(</span>size<span class="o">=</span><span class="m">0.06</span><span class="p">,</span> color<span class="o">=</span><span class="s">&quot;#999999&quot;</span><span class="p">)</span> <span class="o">+</span>
            stat_summary_hex<span class="p">(</span>fun <span class="o">=</span> total_rev<span class="p">,</span> bins<span class="o">=</span><span class="m">100</span><span class="p">,</span> alpha<span class="o">=</span><span class="m">0.5</span><span class="p">)</span> <span class="o">+</span>
            scale_x_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="m">-74.0224</span><span class="p">,</span> <span class="m">-73.8521</span><span class="p">))</span> <span class="o">+</span>
            scale_y_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="m">40.6959</span><span class="p">,</span> <span class="m">40.8348</span><span class="p">))</span> <span class="o">+</span>
            theme_map_dark<span class="p">()</span> <span class="o">+</span>
            scale_fill_gradient<span class="p">(</span>low<span class="o">=</span><span class="s">&quot;#FFFFFF&quot;</span><span class="p">,</span> high<span class="o">=</span><span class="s">&quot;#E74C3C&quot;</span><span class="p">,</span> labels<span class="o">=</span>dollar<span class="p">,</span> trans<span class="o">=</span><span class="s">&quot;log&quot;</span><span class="p">,</span> breaks<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="m">10</span><span class="o">^</span><span class="p">(</span><span class="m">6</span><span class="o">:</span><span class="m">8</span><span class="p">)))</span> <span class="o">+</span>
            labs<span class="p">(</span>title <span class="o">=</span> <span class="s">&quot;Total Revenue for NYC Yellow Taxis by Pickup Location, from Jan 2009  June 2015&quot;</span><span class="p">)</span> <span class="o">+</span>
            coord_equal<span class="p">()</span>

png<span class="p">(</span><span class="s">&quot;nyc-taxi-6.png&quot;</span><span class="p">,</span> w<span class="o">=</span><span class="m">900</span><span class="p">,</span> h<span class="o">=</span><span class="m">900</span><span class="p">,</span> res<span class="o">=</span><span class="m">300</span><span class="p">)</span>
plot
dev.off<span class="p">()</span></code></pre></figure>

<p><img src="/img/nyc-ggplot2-howto/nyc-taxi-6.png" alt=""></p>

<p>A good step forward. Revenue all through Manhattan is mostly the same except for Penn Station. Meanwhile, the hexes in LaGuardia Airport are noticeably more saturated than Penn Station.</p>

<p>Hopefully, this tutorial gave you a good look into a few interesting tricks that can be accomplished with ggplot2, even though the code can be somewhat messy. If you want more orthodox methods of plotting geographic data in ggplot2, you should look into the <a href="https://cran.r-project.org/web/packages/ggmap/index.html">ggmap</a> R package, which I used to plot <a href="http://minimaxir.com/2014/04/san-francisco/">Facebook Checkin data in San Francisco</a>, and look into the <a href="https://cran.r-project.org/web/packages/maps/index.html">maps</a> R package plus shape files, which I used to plot <a href="http://minimaxir.com/2015/01/tree-time/">Instagram photo location data</a>. Unfortunately, the code may not necessarily be less messy.</p>

<hr>

<p><em>If you use the code or data visualization designs contained within this article, it would be greatly appreciated if proper attribution is given back to this article and/or myself. Thanks!</em></p>
]]></content>
  </entry>
  
</feed>
